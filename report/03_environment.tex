\section{Environment}\label{enironment}

This section presents the simulated economic environment that the autonomous pricing agents interact with. I consider an infinitely repeated pricing game with a logit demand as in \textcite{calvano_artificial_2019}. Restricting the analysis to the oligopoly case with $n=2$ agents (where $i = 1,2$), the market comprises \emph{2} differentiated products and an outside option. In every period $t$, both agents simultaneously pick a price $p_i$. Demand for agent $i$ is then determined\footnote{Generalization to a model with \emph{n} agents is straightforward. In fact, the demand formula remains the same. The limitation to 2 agents is merely chosen for computational efficiency and the (intuitive) conjecture that the simulation results generalize to more players provided learning time is sufficiently high.}:

\begin{gather}\label{quantity}
q_{i,t}=\frac{e^{\frac{a_i - p_{i,t}}{\mu}}}{\sum_{j=1}^{n}~ e^{\frac{a_j-p_{j,t}}{\mu}}+e^{\frac{a_0}{\mu}}}
\end{gather}

\textbf{Citations needed}
$\mu$ controls the degree of horizontal differentiation, where $\mu \rightarrow 0$ approximates perfect substitutability. Vertical differentiation is incorporated through the quality parameters $a_i$. $a_0$ reflects the appeal of the outside good. It diminishes as $a_0 \rightarrow -\infty$ \parencite{anderson_logit_1992}. 

Profits of both agents $\pi_i$ are simply calculated as

\begin{gather}\label{profit}
\pi_{i,t} = (p_{i,t} * q_{i,t}) - c_i,
\end{gather}

where $c_i$ is a firm-specific marginal cost. Market entry and exit are not considered. The baseline parametrization emulates \textcite{calvano_artificial_2019}:
$c_i = 1$,
$a_i = 2$,
$a_0 = 0$ and
$\mu = \frac{1}{4}$. These parameters give rise to a static Nash equilibrium with $p_n \approx 1.47$ and $\pi_n \approx 0.23$ per agent. The monopolist solution entails $p_m \approx 1.92$ with $\pi_m \approx 0.34$ for each product. Nevertheless, the following section covers the applied reinforcement learning methods for general parameters.
