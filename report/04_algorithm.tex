\section{Reinforcement Learning with Function Approximation}

if the system were stationary --> convergence guarantee (e.g. Jaakoola et al. 1994). However, non-stationarity induced by multi-agent learning breaks that guarantee. To my knowledge, no guarantees, but empirically strong results

Though both agents repeatedly face the environment as presented in \autoref{enironment}, I will present this section from the vantage point of a single agent. Accordingly, the subscript {i} is dropped when appropriate.

\subsection{value approximation}

The agent learns to approximate the value of an action given the available information. The potential actions reflect the available prices in the current period. It is useful to discretize the action space.\footnote{This discretization usually implies that agents will not charge exactly $p_n$ or $p_m$.} Compared to the baseline specification in \textcite{calvano_artificial_2019}, I consider a wider price range confined by a lower bound $A_i^L$ and an upper bound $A_i^U$:

\begin{gather}
A^{L} = c
\end{gather}

\begin{gather}
A^{U} = p_m + \zeta (p_n - c)
\end{gather}

The lower bound ensures positive margins. It is conceivable that a human manager could implement a sanity restriction like that before conceding pricing authority to an algorithm. The parameter $\zeta$ controls the extent to which the upper bound $A_i^U$ exceeds the monopoly price. With $\zeta = 1$, the difference between $A^{L}$ and $p_n$ is equal to the difference between $A^{U}$ and $p_m$. The available set of prices $\mathcal{A}$ is then evenly spaced out in the the interval $[A^L, A^U]$:

\begin{gather}
	\mathcal{A} = (A^L, A^L + \frac{1(A^U - A^L)}{m-1}, A^L + \frac{2(A^U - A^L)}{m-1}~ , ... , ~ A^L + \frac{m-2(A^U - A^L)}{m-1}, A^U)
\end{gather}

$m$ determines the number of feasible prices. Following \textcite{sutton_reinforcement_2018}, I denote any possible action as $a \in \mathcal{A}$ and the actual realization at time $t$ as $A_t$.

In this simulation, the state set $S_t$  comprises merely the prices of the previous period $t-1$:

\begin{gather}
S_t = \{ p_{i, t-1}, p_{j, t-1} \}
\end{gather}



Lastly, a set of parameters $\boldsymbol{w} = \{w_1, w_2, ..., w_d\}$ maps any combination of $S_t$ and $A_t$ to a value estimate $\hat{q}$.\footnote{In the computer science literature, $\boldsymbol{w}$ is typically referred to as \emph{weights}. I will stick to the economic vocabulary and declare $\boldsymbol{w}$ parameters.} Hence:

\begin{gather}\label{q_estimation}
	\hat{q}(S_t,A_t,\boldsymbol{w}_t) = \hat{q}(p_{i, t-1}, p_{j, t-1}, p_{i, t}, \boldsymbol{w}_t)
\end{gather}

Two elements are required for this system to work successfully. First, the agents must mix between \emph{exploration and exploitation}. Second, the set of parameters $\boldsymbol{w}$ must be continuously optimized.

\paragraph{Exploration and Exploitation} 
In every period, the agent chooses either to \emph{exploit} its current knowledge and pick the supposedly optimal action or to \emph{explore} in order to test the merit of alternative choices that are perceived sub-optimal but may turn out to be superior. As is common, I use a simple $\epsilon$-greedy policy to steer this tradeoff:

\begin{gather}\label{action_selection}
 A_t = \begin{cases} arg ~\underset{a}{max} ~ \hat{q}(S_t,a,\boldsymbol{w}_t) & \quad \text{with probability } 1 - \epsilon\\
\text{randomize over } \mathcal{A} & \quad \text{with probability } \epsilon\\ \end{cases} 
\end{gather}

In words, the agent chooses to play the action that is regarded optimal with probability $1-\epsilon$ and randomizes over all prices with probability $\epsilon$.\footnote{If more than one $a$ maximizes $\hat{q}$, ties are broken randomly.} Subsequently, the quantities and profits are realized in accordance with equations \ref{quantity} \& \ref{profit}. The agents' actions in period $t$ become the state set in $t+1$ and new actions are chosen as dictated by equations \ref{q_estimation} \& \ref{action_selection}.

Irrespective of the \emph{exploit vs explore} decision, the agent proceeds to leverage the observed outcomes to refine $\boldsymbol{w}$.



\paragraph{Update}

After observing the opponent's price and the own profits\footnote{Again, the computer science literature tends to refer to $\pi$ as \emph{reward} which is close to the economic interpretation of \emph{profits}.}, the agent exploits this new information to improve $\boldsymbol{w}$. A good starting point to introduce the utilized update rules is the so called \emph{TD error}, denoted $\delta_t$ (\textbf{finalize footnote}).\footnote{Without function approximation, versions of the \emph{TD error} usually encompass a discount factor $gamma$, such as:
	\begin{center}
		$\delta_t^{SARSA} = \pi_t + \gamma \hat{q}(S_{t+1}, A_{t+1}, \boldsymbol{w}) - \hat{q}(S_t, A_t, \boldsymbol{w})$
		
		$\delta_t^{Q-Learning} = \pi_t + \gamma ~ \underset{a}{max} ~ \hat{q}(S_{t+1}, a, \boldsymbol{w}) - \hat{q}(S_t, A_t, \boldsymbol{w})$
\end{center}
While they come with a meaningful economic interpretation, \textcite{sutton_reinforcement_2018} and \textcite{naik_discounted_2019} show that their use is inappropriate in infinite sequences with function approximation settings. Moreover, a policy maximizing average rewards is equivalent to a policy maximizing the average of discounted future values - irrespective of the particular discount factor.}

\textbf{average setting update}

\begin{gather}
	\delta_t = \pi_t -\widetilde{R}_{t-1} + \hat{q}(S_{t+1}, A_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t) ~~   \text{,}
\end{gather}

where $\widetilde{R}_{t-1}$ is a (weighted) average reward. $\delta_t$ measures the difference between the \emph{ex ante} ascribed value to the selected state-action combination in $t$ and the \emph{ex post} \emph{differential} profit $\pi_t - \widetilde{R}_{t_1}$ in conjunction with the estimated value of the newly arising state-action combination in $t+1$. A positive $\delta_t$ indicates that the actual realization turned out to exceed the original expectation. Likewise, a negative $\delta_t$ suggests that the realization failed short of the expected reward of playing the particular state-action combination. In both instances, $\boldsymbol{w}$ will be adjusted accordingly, such that the state-action combination is valued respectively higher or lower next time. Note that $\delta_t$ can only be calculated after the action in the next period has been taken.\footnote{This is often referred to as \emph{SARSA}, abbreviating a state-action-reward-state-action sequence.}


Surprisingly, this system 
* discount factor can be = 1






\emph{Semi-gradient} methods constitute a basic procedure for such continuous optimization. They serve as a good benchmark before developing more complex algorithms. Essentially, the direction and magnitude of updating parameters is driven by the \emph{TD error} $\delta_t$ and the gradient of $\hat{q}_t(S_t, A_t, \boldsymbol{w})$ with respect to $\boldsymbol{w}$:
$\frac{\Delta \hat{q}}{\Delta \boldsymbol{w}} =
\{ \frac{\Delta \hat{q}}{\Delta w_1},
\frac{\Delta \hat{q}}{\Delta w_2},
...,
\frac{\Delta \hat{q}}{\Delta w_d}  \}$. The update rule is:

\begin{gather}
 \boldsymbol{w}_{t+1} \leftarrow \boldsymbol{w}_t +
 	\alpha \delta_t
 	\frac{\Delta \hat{q}}{\Delta \boldsymbol{w}} ~~ \text{,}
\end{gather}

where $\alpha$ steers the speed of learning. Note that I will only consider approximations that are linear in parameters. Thus, $\frac{\Delta \hat{q}}{\Delta \boldsymbol{w}}$ simplifies to a \emph{feature vector} $\boldsymbol{x} = \{x_1, x_2, ..., x_d\}$ where every element is derived from a state, an action or a combination thereof (\textbf{more in section XYZ}).

\textcite{seijen_true_2014} showed that the performance of that algorithm can be improved by keeping track of an eligibility vector $\boldsymbol{z} = \{z_1, z_2, ..., z_d\}$, called the \emph{Dutch Trace}. Like $\boldsymbol{w}$, this trace vector is updated at every time step. The trick is that the eligibility trace controls the magnitude by which individual parameters are updated, prioritizing those that contributed to producing an estimate of $hat{q}$. The update rule for the eligibility trace is:

\begin{gather}
\boldsymbol{z}_{t} \leftarrow \boldsymbol{z}_{t-1} \gamma \lambda + \boldsymbol{x_t} (1 - \alpha \gamma \lambda  {  \boldsymbol{x_t} \intercal \boldsymbol{z}_{t-1} }) ~~ \text{,}
\end{gather}

where $ \boldsymbol{x}_t \intercal \boldsymbol{z}_t $ denotes the inner product of $\boldsymbol{x}_t$ and $\boldsymbol{z}_t$, i.e. $\sum_{i}^{d} x_i z_i$. $\boldsymbol{z}$ is then used in the refined parameter update:

\begin{gather}
	\boldsymbol{w}_{t+1} \leftarrow
		\boldsymbol{w}_{t} +
		\alpha \delta \boldsymbol{z}_t +
		\alpha ( \boldsymbol{w}_t \intercal \boldsymbol{x}_t  -
				 \boldsymbol{w}_{t-1} \intercal \boldsymbol{x}_t)
				(\boldsymbol{z}_t - \boldsymbol{x}_t)
\end{gather}