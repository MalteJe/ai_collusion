\section{Algorithm}\label{algorithm}

This section describes the way both players approach, interact with and learn from the environment presented in \autoref{enironment}. For the sake of simplicity, I will present that process from the vantage point of a single player. Accordingly, the subscript {i} is dropped when appropriate. The player's objective is to maximize its net present value of discounted future profits:

\begin{gather}\label{maximization}
max \sum_{t = 0}^{\infty} \gamma^t \pi_{t} \text{,}
\end{gather}

where $\gamma \in [0, 1]$ is the discount factor.\footnote{There is a small notational predicament. In the economic literature, $\delta$ usually represents the discount factor. However, reinforcement learning texts reserve $\delta$ for the TD error (see \autoref{parameter_update}). Here, I will follow the latter convention and settle with $\gamma$ to represent the discount factor.} Importantly, the decision maker does not know about the demand function, its opponent's pricing strategy or the benchmark values $p_m$ and $p_n$. Rather, the player can only input a price $p$ and \emph{retrospectively} observe the opponent's action and own profits. This paradigm can be formalized as a \emph{Markov Decision Process} (\gls{mdp}). In a \gls{mdp}, at any point in time $t$, the player, or \emph{agent}, observes the current state of the environment $S_t \in \mathcal{S}$ and proceeds to select an action $A_t \in \mathcal{A}$.\footnote{From the agent's point of view, the \emph{environment} consists of everything that is outside of its control, mainly the demand function \emph{and} the behavior of the other agent.} $\mathcal{S}$ and $\mathcal{A}$, respectively, represent the entirety of possible states and actions. The environment 'returns' a reward $R_t$ to the agent and moves to the next stage $S_{t+1}$.\footnote{Sometimes, the reward due to $A_t$ and $S_t$ is modeled as $R_{t+1}$ instead.} In the case of this study, the action set comprises a finite number of prices  that are inputted into \autoref{quantity} and the state set represents the prices of the previous period (see below).\footnote{The provided description of MDPs is very specific to this study. For a more general treatment of MDPs, consult \textcite{sutton_reinforcement_2018}. \textcite{calvano_algorithmic_2018} and \textcite{hettich} apply the MDP framework to the Bertrand environment in a more rigorous fashion.}

\textbf{paragraph on policies and Bellman equation?}


\subsection{Price grid}

For the trialed algorithms, it is necessary to discretize the action space.\footnote{This discretization usually implies that agents will not be able charge exactly $p_n$ or $p_m$.} Compared to the baseline specification in \textcite{calvano_artificial_2019}, I consider a wider price range confined by a lower bound $A^L$ and an upper bound $A^U$:

\begin{gather}\label{price_grid_formula}
A^{L} = c \\
A^{U} = p_m + \zeta (p_n - c)
\end{gather}

The lower bound ensures positive margins. It is conceivable that a human manager could implement a sanity restriction like that before ceding pricing authority to an algorithm. The parameter $\zeta$ controls the extent to which the upper bound $A^U$ exceeds the monopoly price. With $\zeta = 1$, the difference between $A^{L}$ and $p_n$ is equal to the difference between $A^{U}$ and $p_m$. The available set of prices $\mathcal{A}$ is then evenly spaced out in the interval $[A^L, A^U]$:

\begin{gather}\label{available_prices}
\mathcal{A} = (A^L, A^L + \frac{1(A^U - A^L)}{m-1}, A^L + \frac{2(A^U - A^L)}{m-1}~ , ... , ~ A^L + \frac{(m-2)(A^U - A^L)}{m-1}, A^U)
\end{gather}

$m$ determines the number of feasible prices. Following \textcite{sutton_reinforcement_2018}, I denote any possible action as $a$ and the actual realization at time $t$ as $A_t$.

In this simulation, the state set merely comprises the prices of the previous period $t-1$:

\begin{gather}
S_t = \{ p_{i, t-1}, p_{j, t-1} \}
\end{gather}

Accordingly, for both state variables, the set of possible states is identical to the feasible actions $\mathcal{A}$. However, this is not required with function approximation methods. Theoretically, any state variable could be continuous and unbounded. Similarly to actions, $s$ denotes any possible state set and $S_t$ refers to the actual states at $t$.


\subsection{Value approximation}\label{value_approximation}

As established, the agent chooses an action based on the current state without knowing about the demand function. With the profit maximization objective (\autoref{maximization}) in mind, how does the agent decide on which action to play? It considers all the information that is available, i.e.\ $s$, and estimates the value of playing $a$. With function approximation, a set of parameters $\boldsymbol{w}_t = \{w_{t, 1}, w_{t, 2}, ..., w_{t, D}\}$, where $d \in \{1, 2, ..., D\}$,\footnote{Henceforth, the time subscript is implied for individual components of $\boldsymbol{w}_t$, i.e.\ $w_{t,d} = w_d$.} maps any combination of $S_t$ and $A_t$ to a value estimate $\hat{q}_t$.\footnote{In the computer science literature, $\boldsymbol{w}$ is typically referred to as \emph{weights}. I will stick to the economic vocabulary and declare $\boldsymbol{w}$ parameters.} Hence:

\begin{gather}\label{q_estimation}
	\hat{q}_t = \hat{q}(S_t,A_t,\boldsymbol{w}_t) = \hat{q}(p_{i, t-1}, p_{j, t-1}, p_{i, t}, \boldsymbol{w}_t)
\end{gather}

More specifically, each parameter $w_d$ is associated with a counterpart $x_d$, called a \emph{feature}. The value of every feature is derived  from a state variable, the considered action or a combination thereof:\ $x_d = x_d(S_t, A_t)$. Taken together, the features form a vector $\boldsymbol{x}_t = \boldsymbol{x}(S_t, A_t) = \{x_1(S_t, A_t), x_2(S_t, A_t), ..., x_D(S_t, A_t)\}$. Any state-action combination can be represented by such a \emph{feature vector}.\footnote{Tabular learning methods, such as Q-Learning, contain \emph{exactly one} feature per state-action combination. A single feature evaluates to $1$ if its dedicated state-action combination is played (or considered) and $0$ otherwise. Functional approximation methods employ more complicated mechanisms to extract features. \autoref{feature_extraction} outlines the methods utilized in this study in more detail.} It is important to realize that $\boldsymbol{x}_t$ is determined solely by the combination of $s$ and $a$. The mechanism, by which the state-action combinations are mapped to numerical values remains constant over time. Consequently, there is no need to subscript $x_d$ with respect to $t$. Contrary $\boldsymbol{w}_t$ constitutes the agent's valuation strategy at $t$ which is continuously refined over time.

Note that I will only consider linear functions of $\hat{q}$. In this specific case, \autoref{q_estimation} can be written as the inner product of the feature vector and the set of parameters:

\begin{gather}\label{q_estimation_linear}
\boldsymbol{x}_t \top \boldsymbol{w}_t = \sum_{d=1}^{D} x_d(S_t, A_t) w_d = \sum_{d=1}^{D} x_d(p_{i, t-1}, p_{j, t-1}, p_{i, t}) w_d
\end{gather}

An intuitive policy to achieve profit maximization would be to always play the action $a$ that maximizes the estimated value $\hat{q}$ given $s$. Indeed, this strategy, called a \emph{greedy} policy, would be optimal if the values of state-action combinations were estimated perfectly at all times. \textbf{Bellman optimality?} However, initially, the agent has never interacted with the environment and its valuation of state-action combinations is necessarily wrong. To improve the value estimation, it is necessary to \emph{explore} the merit of various actions. The next section describes how the agent mixes between \emph{exploration and exploitation}. Subsequently, \autoref{parameter_update} describes how the agent continuously learns to improve the set of parameters $\boldsymbol{w}$ from interacting with the environment and, ultimately, refines its value estimation.

\subsection{Exploration and exploitation} 
In every period, the agent chooses either to \emph{exploit} its current knowledge and pick the supposedly optimal action or to \emph{explore} in order to test the merit of alternative choices that are perceived sub-optimal but may turn out to be superior. As is common, I use a simple $\epsilon$-greedy policy to steer this tradeoff:

\begin{gather}\label{action_selection}
 A_t = \begin{cases} arg ~\underset{a}{max} ~ \hat{q}(S_t,a,\boldsymbol{w}_t) & \quad \text{with probability } 1 - \epsilon_t\\
\text{randomize over } \mathcal{A} & \quad \text{with probability } \epsilon_t\\ \end{cases} 
\end{gather}

In words, the agent chooses to play the action that is regarded optimal with probability $1-\epsilon_t$ and randomizes over all prices with probability $\epsilon_t$.\footnote{If more than one $a$ maximizes $\hat{q}$, ties are broken randomly.} The subscript suggests that exploration varies over time. The explicit definition is equivalent to \textcite{calvano_algorithmic_2018}:

\begin{gather}
	\epsilon_t = e^{-\beta t}~ \text{,}
\end{gather}

where $\beta$ controls the speed of decay in exploration. This \emph{time-declining} exploration rate ensures that the agent randomizes actions frequently at the beginning of the simulation and stabilizes its behavior over time. 

After both agents selected an action, the quantities and profits are realized in accordance with equations \ref{quantity} \& \ref{profit}. The agents' actions in period $t$ become the state set in $t+1$ and new actions are chosen again as dictated by equations \ref{q_estimation} \& \ref{action_selection}.

Whether the agent decided to explore or exploit, it proceeds to leverage the observed outcomes to refine $\boldsymbol{w}$.

\subsection{Parameter update}\label{parameter_update}

After observing the opponent's price and own profits, the agent exploits this new information to improve its estimation technique. This study's full system to update the parameter vector $\boldsymbol{w}$ at $t$ comprises the calculation of a \emph{temporal-difference error} $\delta_t$, a vector of eligibility traces $\boldsymbol{z}_t$ tracking the importance of individual coefficients in $\boldsymbol{w}_t$ and the final ultimate update rule to refine $\boldsymbol{w}_t$. The full update system is defined by:

\begin{gather}
\delta_t = R_t + \gamma \bar{V}_t(S_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t) ~~ \text{,} \label{td_error_expected} \\
\boldsymbol{z}_{t} = 
\gamma \lambda \rho_t \boldsymbol{z}_{t-1} + \frac{\Delta \hat{q}}{\Delta \boldsymbol{w}_t} ~~~~~ \text{,} \label{eligibility_trace_update} \\
\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \alpha \delta_t	\boldsymbol{z}_t \label{update_rule}
\end{gather}

I will explain all three of the system's components. Starting with \autoref{td_error_expected}, $\delta_t$ represents the so called \emph{temporal-difference} or \emph{TD error}, i.e.\ the error associated with the estimation of $\hat{q}$.\footnote{The utilized update rule is often referred to as \emph{Expected SARSA}, where \emph{SARSA} abbreviates a state-action-reward-state-action sequence. Q-Learning is similar but evaluates the upcoming state based on the action that is regarded as optimal:
	\begin{center}
		$\delta_t^{Q-Learning} = r_t + \gamma ~ \underset{a}{max} ~ \hat{q}(S_{t+1}, a, \boldsymbol{w}) - \hat{q}(S_t, A_t, \boldsymbol{w})$
	\end{center}
	In \autoref{vary_algorithm} I will consider a successor of Q-Learning suitable for function approximation.} It measures the difference between the \emph{ex ante} ascribed value to the selected state-action combination in $t$ on the right-hand side and the \emph{ex post} actual reward in conjunction with the estimated value of the newly arising state-action combination in $t+1$ on the left-hand side. To elaborate on the latter, the reward $r_t = \pi_t - p_n$ reflects the profits relative to the Nash solution.\footnote{Please note that I explicitly distinguish profits and rewards. Profits, $\pi$, represent the monetary remuneration from operating in the environment and can be interpreted economically. However, profits do not enter the learning algorithm directly. Instead, they serve as a precursor of rewards, $r$. Rewards constitute the signal that agents interpret as direct feedback to refine their algorithms.} $\gamma$ is the discount factor and is applied to the expected value of the upcoming state $\bar{V}_t(S_t)$, i.e.\ the average of all state-action estimates weighted by their probability of being played:

\begin{gather}\label{expected_state_value}
\bar{V}_t(S_{t+1}, \boldsymbol{w}_t) = \sum_{a} Pr(a|S_{t+1}, \boldsymbol{w}_t) ~ \hat{q}(S_{t+1}, a, \boldsymbol{w}_t) ~~   \text{,}
\end{gather}

where $Pr(a|S_{t+1})$ represents the probability of selecting an action conditional on the next state in accordance with the $\epsilon$-greedy policy (\autoref{action_selection}). A positive $\delta_t$ indicates that the reward due to playing $A_t$ turned out to exceed the original expectation. Likewise, a negative $\delta_t$ suggests that the realization failed short of the expected reward. In both instances, $\boldsymbol{w}$ will be adjusted accordingly, such that the state-action combination is valued respectively higher or lower next time.

The second component, the eligibility trace $\boldsymbol{z}_t$ keeps track of the importance of individual coefficients in $\boldsymbol{w}$ when estimating $\hat{q}$ and, ultimately, selecting an action. The idea is that the eligibility trace controls the magnitude by which individual parameters are updated, prioritizing those that contributed to producing an estimate of $hat{q}$. Though there exist various forms, \autoref{eligibility_trace_update} employs the most popular variation, an \emph{accumulating eligibility trace} \textbf{citation needed}.\footnote{\textcite{seijen_true_2014} develop the \emph{Dutch Trace} which has been shown to be a little more powerful. For the sake of simplicity and computational efficiency, it is not used in this study.} Specifically, the update comprises two components: a decay term $\gamma \lambda \rho_t \boldsymbol{z}_{t-1}$ and the gradient of $\hat{q}$. With respect to the former, note that $\boldsymbol{z}_{t-1}$ is multiplied with the known discount factor $\gamma$, another parameter $\lambda \in [0,1]$ (more on which in \autoref{parametrization}) and the \emph{importance sampling ratio} $\rho_t$:

\begin{gather}
	 \rho_t = \frac{\kappa(A_t|S_{t})}{Pr(A_t|S_t)} ~~  \text{,}
\end{gather}

where $Pr(A_t|S_t)$ is the probability of having selected $A_t$ given $S_t$ under the utilized $\epsilon$-greedy policy and $\kappa(A_t|S_t)$ is the probability of selecting $A_t$ given $S_t$ under a hypothetical \emph{target policy} without exploration ($\epsilon = 0$). Accordingly, $\rho_t$ is zero if the agent chooses to explore because under the target policy, the probability of choosing a non-greedy action is null. On the other hand, $\rho_t$ exceeds 1 if a greedy action is selected, because under the target policy the selected action is always greedy. Consequently, the left term resets $\boldsymbol{z}$ to $\boldsymbol{0}$ once a non-greedy action is selected. Otherwise, whether the components of $\boldsymbol{z}$ decay towards zero or accumulate depends on the specific combination of $\gamma$, $\lambda$ and $\beta$ (which affects $Pr(A_t|S_t)$).

The second term is the gradient of $\hat{q}_t(S_t, A_t, \boldsymbol{w}_t)$ with respect to $\boldsymbol{w}_t$:
$\frac{\Delta \hat{q}}{\Delta \boldsymbol{w}_t} =
\{ \frac{\Delta \hat{q}}{\Delta w_1},
\frac{\Delta \hat{q}}{\Delta w_2},
...,
\frac{\Delta \hat{q}}{\Delta w_d}  \}$. As indicated earlier, I will only consider approximations that are linear in parameters. Thus, $\frac{\Delta \hat{q}}{\Delta \boldsymbol{w}}$ simplifies to the \emph{feature vector} $\boldsymbol{x}_t$. This simplification also elucidates the purpose of the eligibility trace. It puts higher weights on coefficients that have been used to decide on $A_t$ and ensures that they can be altered faster and beyond the horizon of a single time period. To illustrate the benefits of eligibility traces consider the following example. An agent selects an action that yields a good immediate reward, but unexpectedly disappointing outcomes in the subsequent period.\footnote{For instance, a deviation from a collusive price level yields high immediate profits but might be followed by punishment prices.} Without eligibility traces, the action yields a positive $\delta$ and coefficients are updated accordingly. There is no direct 'negative credit' for the underwhelming outcomes in episodes in the more distant future. In the long run, the agent might still end up refraining from playing the myopic action by taking into account the low estimated value of the subsequent state, but this might take several revisits to the state-action combination. Eligibility traces can accelerate the learning process. While a similar immediate effect on updating $\boldsymbol{w}$ takes place, the eligibility trace 'remembers' which coefficients were adjusted in the past and enables a retrospective readjustment prompted by underwhelming rewards in future periods. \autoref{baseline} discusses that $\lambda$ controls the degree of hindsight. 

Armed with the TD error $\delta_t$ and the eligibility trace $\boldsymbol{z}_t$, the final piece is an update rule to improve the parameter vector $\boldsymbol{w}$. The update in \autoref{update_rule} comprises $\boldsymbol{w}_t$  (the current knowledge of the agent we hope to improve) and three components, $\alpha$, $\delta_t$ and $\boldsymbol{z}_t$ that dictate the specific shape of the improvement. I will briefly discuss each parameter's role. The TD error $\delta_t$ specifies the direction and magnitude of the update. Recall that it constitutes the 'error' of the original value estimation. For instance, a $\delta_t$ close to 0 suggests the value of the state-action combination was estimated accurately and the update to $\boldsymbol{w}$ will be small. Contrary, a high (or strongly negative) $\delta$ unveils a large divergence between the agent's value estimation and the realized reward $R_t$ and warrants a more significant update to the parameters. The eligibility trace $\boldsymbol{z}_t$ specifies which elements in $\boldsymbol{w}$ are adjusted giving priority to evocative parameters. As discussed, $\boldsymbol{z}$ memorizes which parameters were 'responsible' for the selected actions in the past and ensures a retrospective adjustment beyond a one-episode horizon. Lastly, $\alpha$ steers the speed of learning. In this study, it is constant over time.\footnote{See \autoref{feature_extraction} for a brief discussion on selecting appropriate values of $\alpha$.} The outlined reinforcement learning algorithm in this study, adapted from \textcite{sutton_reinforcement_2018} is summarized in \autoref{expected SARSA}.

\begin{algorithm}
	\caption{\textbf{Gradient Descend} Expected SARSA with eligibility traces.}
	\begin{algorithmic}[testing]
		\label{expected SARSA}
		\small
		\STATE input feasible prices via $m \in \mathbb{N}$ and $\zeta > 0$
		\STATE configure static algorithm parameters $\alpha > 0$, $\beta > 0$, and $\lambda \in [0, 1]$
		\STATE initialize parameter vector and eligibility trace $\boldsymbol{w} = \boldsymbol{z} = \boldsymbol{0}$
		\STATE declare convergence rule (see \autoref{convergence})
		\STATE randomly initialize state S
		\STATE start tracking time: $t = 1$
		\WHILE{convergence is not achieved,}
		\STATE select action $A_t$ according to \autoref{action_selection}
		\STATE observe profit $\pi$, adjust to reward $r$
		\STATE observe next state: $S_{t+1} \leftarrow A_t$
		\STATE calculate TD-error: $\delta \leftarrow R +  \gamma \bar{V}(S_{t+1}) - \hat{q}(S_t, A_t)$ (\autoref{td_error_expected})
		\STATE update eligibility trace: $\boldsymbol{z} \leftarrow \gamma \lambda \rho \boldsymbol{z} + \boldsymbol{x} $ (\autoref{eligibility_trace_update})
		\STATE update parameter vector: $\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha  \delta  \boldsymbol{z}$ (\autoref{update_rule})
		\STATE move to next stage: $S_t \leftarrow S_{t+1}$ and $t \leftarrow t+1$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Baseline parametrization}\label{parametrization}

\begin{center}
	\begin{table}
		\begin{tabular}{|c|c|l|}
			\hline
			\textbf{Parameter}&\textbf{Baseline Value}&\textbf{Variations}\\
			\hline
			$\beta$&$4 * 10^{-5}$&$(1 * 10^{-5}, ~2 * 10^{-5}, ~8 * 10^{-5}, ~1.6 * 10^{-4})$ \\
			\hline
			$\gamma$&$0.95$&$(0, ~0.25, ~0.5, ~0.75, ~0.9, ~1)$ \\
			\hline
			$\lambda$&$0.5$&$(0, ~0.2, ~0.4, ~0.6, ~0.8, ~0.9)$ \\
			\hline
			$\zeta$&$1$&$(0.1, ~0.5, ~2)$ \\
			\hline
			$m$&$19$&$(10, ~39, ~63)$ \\
			\hline
		\end{tabular}
		\caption{Parametrization of learning parameters. Baseline and variations.}
		\label{baseline}
	\end{table}
\end{center}

The presented learning paradigm requires specifying a number of parameters. Optimizing the setting is not a primary concern of this study. Nevertheless, I attempt to choose reasonable values for all parameters. To some degree, this task can be guided by theoretical considerations and previous simulation studies. Still, there remains a high degree of arbitrariness. To make the impact of those choices more transparent, \autoref{robustness} will present results for other specifications. \autoref{baseline} presents the baseline parametrization of the learning algorithm as well as the considered variations. The discussion on appropriate values of the learning rate parameter $\alpha$ is delayed until \autoref{feature_extraction}. I will briefly provide a justification for the specific parametrization and relate it to other studies.

Starting with $\beta$, the parameter controlling the speed of decay in exploration, note that the baseline in this study is about 4 times higher than in \textcite{calvano_algorithmic_2018}.  Unfortunately, feature extraction methods require more computational power than tabular learning and the scope of this study did not allow for a lower value without sacrificing robustness through the execution of multiple runs per parametrization. Consequently, the agents have less time to explore actions. It is hoped that this is partially offset by the introduction of eligibility traces. Also, the variation $\beta = 1*10^{-5}$ falls right into the considered parameter grid in \textcite{calvano_algorithmic_2018}.

The discount factor $\gamma$ is the only parameter with an inherent economic interpretation. $\gamma$ quantifies the time preference of profits today over profits tomorrow. The baseline value $0.95$ is chosen to stay consistent with other simulations in this area (e.g.\ {\textcite{calvano_algorithmic_2018}, \textcite{klein_autonomous_2019} \textbf{other?}). In reinforcement learning, the usage of discounting is often void of any theoretical justification. Rather it is commonly employed as a practical mean to avoid infinite reward sums of future rewards in continuous learning tasks \parencite{schwartz_reinforcement_1993}. However, there are some theoretical arguments questioning the validity of using discounting in continuous learning tasks with function approximation. In \autoref{robustness}, I will discuss results of an alternative setting that attempts to optimize \emph{average rewards}.

Regarding $\lambda$, recall that it appears in \autoref{eligibility_trace_update} to update the eligibility trace $\boldsymbol{z}$. In particular, $\lambda$ controls the \emph{decay rate} of $\boldsymbol{z}$. To understand its purpose, consider first the special case of $\lambda = 0$. This reduces the trace update to $\boldsymbol{z}_t = \frac{\Delta \hat{q}}{\Delta \boldsymbol{w}_t} = \boldsymbol{x}_t$ which is paramount to not using eligibility traces at all.\footnote{In fact, the system of equations \ref{td_error_expected}-\ref{update_rule} could then be reduced conveniently to a single equation:
	\begin{center}
		$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \alpha (r_t + \gamma \bar{V}_t(S_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t)) \boldsymbol{x}_t$
	\end{center}
} Increasing $\lambda$ boosts the degree of hindsight by enlarging the algorithm's memory on which parameters were responsible for past action selections. In turn, those evocative parameters are adjusted beyond a one-period time-frame. However, increasing $\lambda$ comes at the cost of high variance. Persistent traces blow up the number of variables in $\boldsymbol{z}$ such that any outcome is imputed to \emph{too} many parameters that have been activated in the distant past.\footnote{The other extreme, $\lambda = 1$, mimics Monte-Carlo algorithms where learning only takes place after an entire \emph{sequence} of episodes concludes. In some problems, the learning process can be separated into distinct sequences. Such separation is unnatural in the context of demand systems. Accordingly, $\lambda = 1$ is not considered a viable specification for this study.} Empirical results show that intermediate values of $\lambda$ tend to perform best (see e.g.\ \textcite{sutton_1988}, \textcite{rummery_niranjan_1994} and \textcite{sutton_reinforcement_2018}). Accordingly, I choose a baseline value of $\lambda = 0.5$. The variations comprise experiments with $\lambda$ between $0$ and $0.9$.

The parameters $\zeta$ and $m$ both control which prices are feasible. The baseline $\zeta = 1$ ensures a symmetric excess of feasible prices above $p_m$ and below $p_n$. The default value of $m$ is $19$.\footnote{As indicated earlier, both $p_m$ and $p_n$ can not be played exactly. The variations of $m$ were chosen specifically to encompass feasible prices close to both benchmarks for prices close to those benchmarks. For instance, $m = 19$ entails one pricing option at $1.466$ (close to $p_n = 1.473$ and another at $1.932$ (close to $p_m = 1.925$)}. The deliberate choice to depart from the scheme in \textcite{calvano_algorithmic_2018} was made to challenge the algorithms with a less \emph{prefabricated} environment and comes at the cost of impeded comparability.



\subsection{convergence}\label{convergence_considerations}

if the system were stationary --> convergence guarantee (e.g. Jaakoola et al. 1994). However, non-stationarity induced by multi-agent learning breaks that guarantee. To my knowledge, no guarantees, but empirically strong results