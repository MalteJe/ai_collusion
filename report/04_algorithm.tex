\section{Learning algorithm}\label{algorithm}

This section describes the way both players approach, interact with and learn from the environment presented in \autoref{enironment}. For the sake of simplicity, I will present that process from the vantage point of a single player. Accordingly, the subscript {i} is dropped when appropriate. The player's objective is to maximize its net present value of discounted future profits:

\begin{gather}\label{maximization}
max \sum_{t = 0}^{\infty} \gamma^t \pi_{t} \text{,}
\end{gather}

where $\gamma \in [0, 1]$ is the discount factor.\footnote{There is a small notational predicament. In economic papers, $\delta$ usually represents the discount factor. However, reinforcement learning texts reserve $\delta$ for the \emph{temporal difference error} (see \autoref{parameter_update}). Here, I will follow the latter convention and let $\gamma$ denote the discount factor.} Importantly, the decision maker does not know about the demand function, its opponent's pricing strategy or the benchmark values $p_m$ and $p_n$. Rather, the player can only input a price $p$ and \emph{retrospectively} observe the opponent's action and own profits. This paradigm can be formalized as a \emph{Markov Decision Process} (\gls{mdp}). In a \gls{mdp}, at any point in time $t$, the player, or \emph{agent}, observes the current state of the environment $S_t \in \mathcal{S}$ and proceeds to select an action $A_t \in \mathcal{A}$.\footnote{From the agent's point of view, the \emph{environment} consists of everything that is outside of its control, mainly the demand function \emph{and} the behavior of the other agent.} $\mathcal{S}$ and $\mathcal{A}$, respectively, represent the entirety of possible states and actions. The environment 'returns' a reward $R_t$ to the agent and moves to the next stage $S_{t+1}$.\footnote{Sometimes, the reward due to $A_t$ and $S_t$ is modeled as $R_{t+1}$ instead. The provided description of MDPs is very brief and specific to this study. For a more general treatment of MDPs, consult \textcite{sutton_reinforcement_2018}. \textcite{calvano_artificial_2020} and \textcite{hettich_algorithmic_2021} apply the \gls{mdp} framework to the Bertrand environment in a more rigorous fashion} In this study, the action set comprises a finite number of prices that are inputted into \autoref{quantity}. The state set simply represents the actions (i.e.\ prices) of the previous period. The next section details the grid of available prices.

\subsection{Price grid}

For the trialed algorithms, it is necessary to discretize the action space. Compared to the baseline specification in \textcite[p.3274]{calvano_artificial_2020}, I consider a wider price range confined by a lower bound $A^L$ and an upper bound $A^U$:

\begin{gather}\label{price_grid_formula}
A^{L} = c \\
A^{U} = p_m + \zeta (p_n - c)
\end{gather}

The lower bound ensures positive margins. It is conceivable that a human manager could implement a sanity restriction like that before ceding pricing authority to an algorithm.\footnote{\textcite[p.13]{johnson_platform_2020} make the same decision.} The parameter $\zeta$ controls the extent to which the upper bound $A^U$ exceeds the monopoly price. With $\zeta = 1$, the difference between $A^{L}$ and $p_n$ is equal to the difference between $A^{U}$ and $p_m$. The available set of prices $\mathcal{A}$ is then evenly spaced out in the interval $[A^L, A^U]$:

\begin{gather}\label{available_prices}
\mathcal{A} = \{A^L, A^L + \frac{1(A^U - A^L)}{m-1}, A^L + \frac{2(A^U - A^L)}{m-1}~ , ... , ~ A^L + \frac{(m-2)(A^U - A^L)}{m-1}, A^U\} \text{,}
\end{gather}

where $m$ determines the number of feasible prices. Note that the discretization implies that agents will not be able charge exactly $p_n$ or $p_m$. However, by choosing $m$ appropriately, one can get close (see \autoref{parametrization}). Following \textcite{sutton_reinforcement_2018}, I denote any possible action as $a$ and the actual realization at time $t$ as $A_t$.

In this simulation, the state set merely comprises two variables, namely the actions taken at $t-1$:

\begin{gather}
S_t = \{ p_{i, t-1}, p_{j, t-1} \}
\end{gather}

Accordingly, for both state variables, the set of possible states is identical to the feasible actions $\mathcal{A}$. However, this is not required with function approximation methods. Theoretically, any state variable could be continuous and unbounded. Similarly to actions, $s$ denotes any possible state set and $S_t$ refers to the actual states at $t$.


\subsection{Value approximation}\label{value_approximation}

As established, the agent chooses an action based on the current state without knowing about the demand function. With the profit maximization objective (\autoref{maximization}) in mind, how does the agent decide on which action to play? It considers all the information that is available, i.e.\ $s$, and estimates the value of playing $a$. With function approximation, a set of parameters $\boldsymbol{w}_t = \{w_{t, 1}, w_{t, 2}, ..., w_{t, D}\}$\footnote{Henceforth, I will use $d \in \{1, 2, ..., D\}$ to iterate over elements in $\boldsymbol{w}$. Moreover, from now one the time subscript is implied for individual components of $\boldsymbol{w}_t$, i.e.\ $w_{t,d} = w_d$. } maps any combination of $S_t$ and $A_t$ to a value estimate $\hat{q}_t$.\footnote{In the computer science literature, $\boldsymbol{w}$ is typically referred to as \emph{weights}. I will stick to the economic vocabulary and declare $\boldsymbol{w}$ parameters.} Formally:

\begin{gather}\label{q_estimation}
	\hat{q}_t = \hat{q}(S_t,A_t,\boldsymbol{w}_t) = \hat{q}(p_{i, t-1}, p_{j, t-1}, p_{i, t}, \boldsymbol{w}_t)
\end{gather}

More specifically, each parameter $w_d$ is associated with a counterpart $x_d$, called a \emph{feature}. The value of every feature is derived  from a state variable, the considered action or a combination thereof:\ $x_d = x_d(S_t, A_t)$. Taken together, the features form a vector $\boldsymbol{x}_t = \boldsymbol{x}(S_t, A_t) = \{x_1(S_t, A_t), x_2(S_t, A_t), ..., x_D(S_t, A_t)\}$. Any state-action combination can be represented by such a \emph{feature vector}.\footnote{Tabular learning methods, such as Q-Learning, contain \emph{exactly one} feature per state-action combination. A single feature evaluates to $1$ if its dedicated state-action combination is played (or considered) and $0$ otherwise. Function approximation methods employ more complicated mechanisms to extract features. Section \ref{feature_extraction} outlines the methods utilized in this study in more detail.} It is important to realize that $\boldsymbol{x}_t$ is determined solely by the combination of $s$ and $a$. The mechanism, by which the state-action combinations are mapped to numerical values remains constant over time. Consequently, there is no need to subscript $x_d$ with respect to $t$. Contrary $\boldsymbol{w}_t$ constitutes the agent's valuation strategy at $t$ which is continuously refined over time.

Note that I will only consider linear functions of $\hat{q}$. In this specific case, \autoref{q_estimation} can be written as the inner product of the feature vector and the set of parameters:

\begin{gather}\label{q_estimation_linear}
\hat{q}_t = \boldsymbol{x}_t \top \boldsymbol{w}_t = \sum_{d=1}^{D} x_d(S_t, A_t) w_d = \sum_{d=1}^{D} x_d(p_{i, t-1}, p_{j, t-1}, p_{i, t}) w_d
\end{gather}

An intuitive policy to achieve profit maximization would be to always play the action $a$ that maximizes the estimated value $\hat{q}$ given $s$. Indeed, this strategy, called a \emph{greedy} policy, would be optimal if the values of state-action combinations were estimated perfectly at all times. However, initially, the agent has never interacted with the environment and its valuation of state-action combinations is necessarily naive. To improve the value estimation, it is necessary to \emph{explore} the merit of various actions. The next section describes how the agent mixes between \emph{exploration and exploitation}. Subsequently, \autoref{parameter_update} describes how the agent continuously learns to improve the set of parameters $\boldsymbol{w}$ from interacting with the environment and, ultimately, refines its value estimation.

\subsection{Exploration and exploitation} 
In every period, the agent chooses either to \emph{exploit} its current knowledge and pick the supposedly optimal action or to \emph{explore} in order to test the merit of alternative choices that are perceived sub-optimal but may turn out to be superior. As is common, I use a simple $\epsilon$-greedy policy to steer this tradeoff:

\begin{gather}\label{action_selection}
 A_t = \begin{cases} arg ~\underset{a}{max} ~ \hat{q}(S_t,a,\boldsymbol{w}_t) & \quad \text{with probability } 1 - \epsilon_t\\
\text{randomize over } \mathcal{A} & \quad \text{with probability } \epsilon_t\\ \end{cases} 
\end{gather}

In words, the agent chooses to play the action that is regarded optimal with probability $1-\epsilon_t$ and randomizes over all prices with probability $\epsilon_t$.\footnote{If more than one $a$ maximizes $\hat{q}$, ties are broken randomly.} The subscript suggests that exploration varies over time. The explicit definition is equivalent to \textcite[p.3274]{calvano_artificial_2020}:

\begin{gather}
	\epsilon_t = e^{-\beta t}~ \text{,}
\end{gather}

where $\beta$ is a parameter controlling the speed of decay in exploration. This \emph{time-declining} exploration rate ensures that the agent randomizes actions frequently at the beginning of the simulation and stabilizes its behavior over time. 

After both agents selected an action, the quantities and profits are realized in accordance with equations \ref{quantity} \& \ref{profit}. The agents' actions in period $t$ become the state set in $t+1$ and new actions are chosen again as dictated by equations \ref{q_estimation} and \ref{action_selection}.

Whether the agent decided to explore or to exploit, it proceeds to leverage the observed outcomes to refine $\boldsymbol{w}$.

\subsection{Parameter update}\label{parameter_update}

After observing the opponent's price and own profits, the agent exploits this new information to improve its estimation technique. This study's full system to update the parameter vector $\boldsymbol{w}$ at $t$ comprises the calculation of a \emph{temporal-difference error} (\gls{td_error}, denoted $\delta_t$), a vector of eligibility traces $\boldsymbol{z}_t$ tracking the importance of individual coefficients in $\boldsymbol{w}_t$ and the final update rule to refine $\boldsymbol{w}_t$. The formulas are:

\begin{gather}
\delta_t = R_t + \gamma \bar{V}_t(S_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t) ~~ \text{,} \label{td_error_expected} \\
\boldsymbol{z}_{t} = 
\gamma \lambda \rho_t \boldsymbol{z}_{t-1} + \frac{\Delta \hat{q}_t}{\Delta \boldsymbol{w}_t} ~~~~~ \text{,} \label{eligibility_trace_update} \\
\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \alpha \delta_t	\boldsymbol{z}_t \label{update_rule}
\end{gather}

I will explain all three of the system's components. Starting with \autoref{td_error_expected}, $\delta_t$ represents the so called \emph{temporal-difference} or \emph{TD error}, i.e.\ the error associated with the estimation of $\hat{q}_t$.\footnote{The utilized update system is referred to as \emph{Expected SARSA}, where \emph{SARSA} abbreviates a state-action-reward-state-action sequence. Q-Learning behaves similar but evaluates the upcoming state based on the action that is regarded as optimal:
	\begin{center}
		$\delta_t^{Q-Learning} = R_t + \gamma ~ \underset{a}{max} ~ \hat{q}(S_{t+1}, a, \boldsymbol{w}) - \hat{q}(S_t, A_t, \boldsymbol{w})$
	\end{center}
	In \autoref{vary_algorithm} I will consider a successor of Q-Learning suitable for function approximation.} It measures the difference between the \emph{ex ante} ascribed value to the selected state-action combination in $t$ on the right-hand side and the \emph{ex post} actual reward in conjunction with the estimated value of the newly arising state-action combination in $t+1$ on the left-hand side. To elaborate on the latter, the reward $R_t = \pi_t - p_n$ reflects the profits relative to the Nash solution.\footnote{Please note that I explicitly distinguish profits and rewards. Profits, $\pi$, represent the monetary remuneration from operating in the environment and can be interpreted economically. However, profits do not enter the learning algorithm directly. Instead, they serve as a precursor of rewards, $R_t$. Rewards constitute the signal that agents interpret as direct feedback to refine their algorithms.} $\gamma$ is the discount factor and is applied to the expected value of the upcoming state $\bar{V}_t(S_t)$, i.e.\ the average of all state-action estimates weighted by their probability of being played. Formally:

\begin{gather}\label{expected_state_value}
\bar{V}_t(S_{t+1}, \boldsymbol{w}_t) = \sum_{a} Pr(a|S_{t+1}, \boldsymbol{w}_t) ~ \hat{q}(S_{t+1}, a, \boldsymbol{w}_t) ~~   \text{,}
\end{gather}

where $Pr(a|S_{t+1})$ represents the probability of selecting action $a$ conditional on $S_{t+1}$ in accordance with the $\epsilon$-greedy policy (\autoref{action_selection}). A positive $\delta_t$ indicates that the value of playing $A_t$ turned out to exceed the original expectation. Likewise, a negative $\delta_t$ suggests that the realization failed short of the estimated value. In both instances, $\boldsymbol{w}$ will be adjusted accordingly, such that the state-action combination is valued respectively higher or lower next time.

The second component $\gamma \lambda \rho_t \boldsymbol{z}_{t-1} + \frac{\Delta \hat{q}_t}{\Delta \boldsymbol{w}_t}$, labeled the eligibility trace $\boldsymbol{z}_t$, keeps track of the importance of individual coefficients in $\boldsymbol{w}$ when estimating $\hat{q}_t$ and, ultimately, selecting an action. The idea is that the eligibility trace controls the magnitude by which individual parameters are updated, prioritizing those that contributed to producing an estimate of $\hat{q}_t$. Though there exist various forms, \autoref{eligibility_trace_update} employs the most popular variation, called an \emph{accumulating eligibility trace} \parencite{sutton_learning_1988}.\footnote{\textcite{seijen_true_2014} develop the \emph{Dutch Trace} which has been shown to be a little more powerful. For the sake of simplicity and computational efficiency, it is not used in this study.} Specifically, the update comprises two components: a decay term $\gamma \lambda \rho_t \boldsymbol{z}_{t-1}$ and the gradient of $\hat{q}_t$. With respect to the former, note that $\boldsymbol{z}_{t-1}$ is multiplied with the known discount factor $\gamma$, another parameter $\lambda \in [0,1]$ (more on this in \autoref{parametrization}) and the \emph{importance sampling ratio} $\rho_t$. The latter indicates whether the played action coincides with the strategy currently considered \emph{optimal}. More specifically:

\begin{gather}
	 \rho_t = \frac{\kappa(A_t|S_{t})}{Pr(A_t|S_t)} ~~  \text{,}
\end{gather}

where $Pr(A_t|S_t)$ is the probability of having selected $A_t$ given $S_t$ under the utilized $\epsilon$-greedy policy and $\kappa(A_t|S_t)$ is the probability of selecting $A_t$ given $S_t$ under a hypothetical \emph{target policy} without exploration ($\epsilon = 0$). Accordingly, $\rho_t$ is zero if the agent chooses to explore a non-optimal action because, under the target policy, the probability of choosing a non-greedy action is null. On the other hand, $\rho_t$ exceeds 1 if a greedy action is selected, because under the target policy the selected action is always greedy. In summary, the left term resets $\boldsymbol{z}$ to $\boldsymbol{0}$ once a non-greedy action is selected. Otherwise  the specific combination of $\gamma$, $\lambda$ and $\beta$ (which affects $Pr(A_t|S_t)$) determines whether the components of $\boldsymbol{z}$ decay towards zero or accumulate.

The second term is the gradient of $\hat{q}$ with respect to $\boldsymbol{w}_t$:

\begin{gather}\label{gradient}
	\frac{\Delta \hat{q}_t}{\Delta \boldsymbol{w}_t} =
	\{ \frac{\Delta \hat{q}_t}{\Delta w_1},
	\frac{\Delta \hat{q}_t}{\Delta w_2},
	...,
	\frac{\Delta \hat{q}_t}{\Delta w_d}  \} =
	\frac{\Delta (\boldsymbol{x}_t \top \boldsymbol{w}_t)}{\Delta \boldsymbol{w}_t} = \boldsymbol{x}_t
\end{gather}

The final simplification is possible because I only consider approximations that are linear in parameters. Equation \ref{gradient} also elucidates the purpose of the eligibility trace. It puts higher weights on coefficients that have been used to decide on $A_t$ and ensures that they can be altered faster and beyond the horizon of a single time period. To illustrate the benefits of eligibility traces consider the following example. An agent selects an action that yields a good immediate reward, but unexpectedly disappointing outcomes in the subsequent period.\footnote{For instance, a deviation from a collusive price level yields high immediate profits but might be followed by punishment prices.} Without eligibility traces, the action yields a positive $\delta_t$ and the responsible coefficients in $\boldsymbol{w}$ are updated such that the action will be estimated to be higher next time. However, the method can not assign 'negative credit' to elements in $\boldsymbol{w}$ for the underwhelming outcomes in more distant future periods. In the long run, the agent might still end up refraining from playing the myopic action by taking into account the low estimated value of the subsequent state, but this might take several revisits to the state-action combination.

Eligibility traces can accelerate the learning process. While a similar immediate effect on updating $\boldsymbol{w}$ takes place, the eligibility trace 'remembers' which coefficients were adjusted in the past and enables a retrospective readjustment prompted by underwhelming rewards in future periods. Section \ref{parametrization} discusses that $\lambda$ controls the degree of hindsight. 

Armed with the TD error $\delta_t$ and the eligibility trace $\boldsymbol{z}_t$, the final piece, \autoref{update_rule}, is an update rule to improve the parameter vector: $\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \alpha \delta_t	\boldsymbol{z}_t$. It comprises $\boldsymbol{w}_t$  (the current knowledge of the agent we hope to improve) and three components, $\alpha$, $\delta_t$ and $\boldsymbol{z}_t$ that dictate the specific shape of the update. I will briefly discuss each parameter's role. The TD error $\delta_t$ specifies the direction and magnitude of the update. Recall that it estimates the error associated with the value estimation through $\hat{q}_t$. For instance, a $\delta_t$ close to 0 suggests the value of the state-action combination was estimated accurately and the update to $\boldsymbol{w}$ will be small. Contrary, a high (or strongly negative) $\delta$ unveils a large divergence between the agent's value estimation and the realized reward $R_t$ and warrants a more significant update to the parameters. The eligibility trace $\boldsymbol{z}_t$ specifies which elements in $\boldsymbol{w}$ are adjusted giving priority to \emph{evocative} parameters. As discussed, $\boldsymbol{z}$ memorizes which parameters were 'responsible' for the selected actions in the past and ensures a retrospective adjustment beyond a one-period horizon. Lastly, $\alpha$ steers the speed of learning. In this study, it is constant over time.\footnote{See \autoref{feature_extraction} for a brief discussion on selecting appropriate values of $\alpha$.} The outlined reinforcement learning algorithm in this study, adapted from \textcite[p.287-312]{sutton_reinforcement_2018} is summarized as \autoref{expected SARSA}. Note that the execution stops once \emph{convergence} is achieved. I describe the employed convergence rules in \autoref{convergence}.

\begin{algorithm}
	\caption{Expected SARSA with eligibility traces.}
	\begin{algorithmic}[testing]
		\label{expected SARSA}
		\small
		\STATE input feasible prices via $m \in \mathbb{N}$ and $\zeta \ge 0$
		\STATE configure static algorithm parameters $\alpha > 0$, $\beta > 0$, and $\lambda \in [0, 1]$
		\STATE initialize parameter vector and eligibility trace $\boldsymbol{w}_0 = \boldsymbol{z}_0 = \boldsymbol{0}$
		\STATE declare convergence rule (see \autoref{convergence})
		\STATE randomly initialize state S
		\STATE start tracking time: $t = 1$
		\WHILE{convergence is not achieved,}
		\STATE select action $A$ according to \autoref{action_selection}
		\STATE observe profit $\pi$, adjust to reward $R$
		\STATE observe next state: $S_{t+1} \leftarrow A$
		\STATE calculate TD-error: $\delta \leftarrow R +  \gamma \bar{V}(S_{t+1}) - \hat{q}(S_t, A)$ (\autoref{td_error_expected})
		\STATE update eligibility trace: $\boldsymbol{z} \leftarrow \gamma \lambda \rho \boldsymbol{z} + \boldsymbol{x} $ (\autoref{eligibility_trace_update})
		\STATE update parameter vector: $\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha  \delta  \boldsymbol{z}$ (\autoref{update_rule})
		\STATE move to next stage: $S_t \leftarrow S_{t+1}$ and $t \leftarrow t+1$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Baseline parametrization}\label{parametrization}



The presented learning paradigm requires specifying a number of parameters. Optimizing the setting is not a primary concern of this study. Nevertheless, I attempt to choose reasonable values for all parameters. To some degree, this task can be guided by theoretical considerations and previous simulation studies. Still, there remains a high degree of arbitrariness. To make the impact of those choices more transparent, \autoref{robustness} will present results for other specifications. \autoref{baseline} presents the baseline parametrization of the learning algorithm as well as the considered variations. The discussion on appropriate values of the learning rate parameter $\alpha$ is delayed until \autoref{feature_extraction}. I will briefly provide a justification for the specific parametrization and, if possible, relate it to other studies.

Starting with $\beta$, the parameter controlling the speed of decay in exploration, note that the baseline in this study is about 4 times higher than in \textcite[p.3275-3276]{calvano_artificial_2020}.\footnote{Unfortunately, feature extraction methods require more computational power than tabular learning and the scope of this study did not allow for a lower value of $\beta$ without sacrificing robustness through the execution of multiple runs per parametrization.} Consequently, the agents have less time to explore actions and learn from the obtained rewards. It is conjectured that this is partially offset by the introduction of eligibility traces. Also, the variation $\beta = 1*10^{-5}$ falls right into the considered parameter grid in \textcite{calvano_artificial_2020}.

The discount factor $\gamma$ is the only parameter with an inherent economic interpretation. $\gamma$ quantifies the time preference of profits today over profits tomorrow. The baseline value $0.95$ is chosen to stay consistent with other simulations similar to this study (e.g.\ {\textcite{calvano_artificial_2020}, \textcite{klein_autonomous_2019} and \textcite{hettich_algorithmic_2021}). In reinforcement learning, the usage of discounting is often void of any theoretical justification. Rather it is commonly employed as a practical mean to avoid infinite reward sums in continuous learning tasks \parencite[p.3]{schwartz_reinforcement_1993}. However, there are some theoretical arguments questioning the validity of discounting in continuous learning tasks with function approximation. In \autoref{differential}, I will discuss results of an alternative setting that attempts to optimize \emph{average rewards}.

	\begin{table}
		\centering
		\begin{tabular}{|c|c|l|}
			\hline
			\textbf{Parameter}&\textbf{Baseline Value}&\textbf{Variations}\\
			\hline
			$\beta$&$4 * 10^{-5}$&$\{1 * 10^{-5}, ~2 * 10^{-5}, ~8 * 10^{-5}, ~1.6 * 10^{-4}\}$ \\
			\hline
			$\gamma$&$0.95$&$\{0, ~0.25, ~0.5, ~0.75, ~0.8, ~0.85, ~0.9, ~0.99\}$ \\
			\hline
			$\lambda$&$0.5$&$\{0, ~0.2, ~0.4, ~0.6, ~0.8, ~0.9\}$ \\
			\hline
			$\zeta$&$1$&$\{0.1, ~0.5, ~1.5\}$ \\
			\hline
			$m$&$19$&$\{10, ~39, ~63\}$ \\
			\hline
		\end{tabular}
		\caption[Baseline parametrization and variations]{Baseline parametrization and variations.}
		\label{baseline}
	\end{table}

Regarding $\lambda$, recall that it appears in \autoref{eligibility_trace_update} to update the eligibility trace $\boldsymbol{z}$. In particular, $\lambda$ controls the \emph{decay rate} of $\boldsymbol{z}$. To understand its purpose, consider first the special case of $\lambda = 0$. This reduces the trace update to $\boldsymbol{z}_t = \frac{\Delta \hat{q}}{\Delta \boldsymbol{w}_t} = \boldsymbol{x}_t$ which is paramount to not using eligibility traces at all.\footnote{In fact, the system of equations \ref{td_error_expected}-\ref{update_rule} could then be conveniently reduced to a single equation:
	\begin{center}
		$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \alpha (r_t + \gamma \bar{V}_t(S_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t)) \boldsymbol{x}_t$
	\end{center}
} Increasing $\lambda$ boosts the degree of hindsight by enlarging the algorithm's memory on which parameters were responsible for past action selections. In turn, those evocative parameters are adjusted beyond a one-period horizon. However, increasing $\lambda$ comes at the cost of high variance. Persistent traces blow up the number of variables in $\boldsymbol{z}$ such that any outcome is imputed to \emph{too} many parameters that have been activated in the distant past.\footnote{The other extreme, $\lambda = 1$, mimics Monte-Carlo algorithms where learning only takes place after an entire \emph{sequence} of time steps concludes. In some problems, the learning process can be separated into distinct sequences. Such separation is unnatural in the context of demand systems. Accordingly, $\lambda = 1$ is not considered a viable specification for this study.} Empirical results show that intermediate values of $\lambda$ tend to perform best (see e.g.\ \textcite{sutton_learning_1988}, \textcite{rummery_-line_1994} and \textcite{sutton_reinforcement_2018}). Accordingly, I choose a baseline value of $\lambda = 0.5$. The variations comprise experiments with $\lambda$ between $0$ and $0.9$.

The parameters $\zeta$ and $m$ both control which prices are feasible. The baseline $\zeta = 1$ ensures a symmetric excess of feasible prices above $p_m$ and below $p_n$. The default value of $m$ is $19$.\footnote{As indicated earlier, both $p_m$ and $p_n$ can not be played exactly. The variations of $m$ were chosen specifically to encompass feasible prices close to both benchmarks for prices close to those benchmarks. For instance, $m = 19$ entails one pricing option at $1.466$ (close to $p_n = 1.473$) and another at $1.932$ (close to $p_m = 1.925$)} The deliberate choice to depart from the scheme in \textcite{calvano_artificial_2020} was made to challenge the algorithms with a less \emph{prefabricated} environment. It comes at the cost of impeded comparability.



\subsection{Convergence considerations}\label{convergence_considerations}

Many reinforcement learning algorithms come with convergence guarantees.\footnote{\textcite[p.1189-1191]{jaakkola_convergence_1994} prove that Q-Learning is guaranteed to converge to an optimal strategy under mild conditions in \emph{stationary} environments. \textcite{tsitsiklis_analysis_1997} discuss convergence with linear function approximation.} To my knowledge, there exist none for the simulations in this study. There are two inhibitors. First, the combination of function approximation, off-policy learning and bootstrapping (i.e.\ estimating the value of future states) is known to pose a threat of instability \parencite[p.264-265]{sutton_reinforcement_2018}. Second, the environment is non-stationary because both agents dynamically change their strategies over time. This causes a \emph{moving target} problem. The optimal strategy might change over time depending on the other player's learning process \parencite[p.42]{tuyls_multiagent_2012}. Notwithstanding the absence of a guarantee, convergence is nevertheless possible.