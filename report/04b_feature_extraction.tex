
\subsection{Feature Extraction}\label{feature_extraction}

\textbf{TBD: introduction}

\textbf{Tabular}

A natural way to represent the state-action space is to preserve a distinct feature (and coefficient) for every unique state-action combination. Features are binary, i.e.\ any feature is $1$ if the associated state-action combination is selected and $0$ otherwise. The respective coefficient tracks the performance over time and directly represents the \emph{value} of that state-action combination. This approach is called \emph{tabular} because it is easy to imagine a table where every cell represents a unique state-action combination. Tabular methods have been used extensively in the simulations on algorithmic collusion that provided empirical evidence of collusive outcomes being possible in simple environments (\textbf(citations)). Their widespread application is justified by their conceptual simplicity and their historic usage in autonomous pricing of airline fares and electricity markets \parencite{ittoo_algorithmic_2017}. Moreover, tabular methods give rise to a family of robust learning algorithms with well-understood convergence guarantees (\textbf{citation}) \footnote{Q-Learning being just one particular application.}.

However, tabular methods are not necessarily the best or fastest way to learn an optimal policy. In real life markets, two salient factors may impede its effectiveness. First, prices are (quasi-) continuous - a fact essentially ignored by tabular methods. Second, most economic markets are complex and change over time. I will take a closer look at each of these factors and the potential repercussions when utilizing tabular methods.

Obviously, any decision maker is only restricted by the currency's smallest feasible increment and can set more than just a handful of prices. It is certainly conceivable, maybe even desirable, that a decision maker reduces the number of considered prices and gets arbitrarily close to an optimal solution without exactly achieving it. However, strictly speaking, when considering the price of other players
	

disadvantages:
prices are continuous.
	- more serious, in reality one agent can not limit the action space of other agents (or even additional environment variables that capture demand) and it is not entirely clear how to handle that. possible response is to approximate continuous prices by reducing price intervals to arbitrary length
		-> increases complexity and thus, learning time. (as shown in appendix of Calvano et al. appendix?)
	- neglects similarity of prices


tabular case as baseline --> special case of tile coding

As outlined in \autoref{value_approximation}, the state-action space contains just 3 variables. Assigning a single coefficient to each variable certainly fails to do justice to the complexity of the optimization problem. In particular, a \emph{reward-punishment} theme requires that actions are chosen conditional on past prices (i.e.\ the state space). Hence, it is imperative to consider interactions and non-linearities. Therefore, I utilize various methods to extract features form the state-action space.

In reinforcement learning, a common approach is to store a distinct set of coefficients for every feasible action.\footnote{In this case, the vector of coefficients contains \emph{m} times features components} This is a sensible approach with qualitative action spaces. However, very much like tabular learning, a separate set of coefficients neglects the (quasi-) continuous nature of prices. Therefore two issues arise. First, discretizing the action space doesn't scale well if the number of feasible prices increases. Consequently, learning requires relatively many periods with large $m$. Second, observing a particular reward may not only constitute an informative feedback for the particular action undertaken, but also for 'similar' prices. Using and updating coefficients valid for (a subset of) all feasible prices exploits this.

For this simulation, I use \emph{polynomials}, \emph{polynomial splines} and \emph{tile coding} to extract features from the state-action space.

\subsubsection{Polynomials}

\emph{Polynomial approximation} of order $k$ maps states and action to a set of features, where a single feature corresponds to:



\begin{gather}
x_i^{Poly} = p_{1, t-1}^{\kappa_1} ~ p_{2, t-1}^{\kappa_2} ~ p_{1, t}^{\kappa_3}
\end{gather}


Every combination of exponents that adheres to the restrictions

\begin{itemize}
	\item $0 < \kappa_1 + \kappa_2 + \kappa_3 \leq k$ and
	\item $\kappa_1, \kappa_2, \kappa_3 \in \{0, 1, ..., k\}$
\end{itemize}

constitutes one feature. Using polynomial approximation, the feature vector $\boldsymbol{x}$ contains ${k + 3\choose3}  - 1$ elements.

\subsubsection{Normalized Polynomials}

\textbf{TBD}

\subsubsection{Polynomial Splines}

\textbf{TBD}

\subsubsection{Tile Coding}

In reinforcement learning, \emph{Tile Coding} is a common way to extract linear, in fact binary, features from a state-action space.\footnote{for an extensive introduction with instructive illustrations refer to \textcite{sutton_reinforcement_2018}} The idea is that several \emph{tilings} superimpose the state-action space. The $\mathcalligra{T}$ \ tilings are offset but each tiling covers the entire state-action space:

\begin{gather}
	 \mathcal{T}^L \leq A^L  ~ \& ~ \mathcal{T}^U \geq A^U    \text{for } \mathcal{T} \in \{1, 2, ..., \mathcalligra{T} ~ \}
\end{gather}

Each tiling is itself composed of uniformly spaced out \emph{tiles}.\footnote{With 2 dimensions, a tiling simply corresponds to a grid. In our case, the state-action space is 3-dimensional, so it may prove more intuitive to think of cubes instead of tilings and tiles.} Every tile is uniquely demarcated by a lower and an upper threshold for every dimension. Consequently, the number of tiles per tiling is controlled by the number of thresholds. For this simulation, it suffices to define a single set of thresholds per tiling that applies to all 3 dimensions. More specifically, the thresholds are spaced out evenly in the tiling-specific interval $[\mathcal{T}^L, \mathcal{T}^U]$:

\begin{gather}
\mathcal{T} = (
\mathcal{T}^L,
\mathcal{T}^L + \frac{1(\mathcal{T}^U - \mathcal{T}^L)}{\tau},
\mathcal{T}^L + \frac{2(\mathcal{T}^U - \mathcal{T}^L)}{\tau}~ , ... , ~
\mathcal{T}^L + \frac{(\tau-1)(\mathcal{T}^U - \mathcal{T}^L)}{\tau},
\mathcal{T}^U)
\end{gather}

This gives rise to $\tau^3$ tiles per tiling. Tiles are binary, i.e.\ if a state-action observation falls into a particular demarcation, the corresponding tile is \emph{activated}:

\begin{gather}\label{tile_activation}
x_i^{Tiling} = \begin{cases}
1 & \quad \text{if } \{p_{1, t-1}, p_{2, t-1}, p_{1, t}\} \text{~in tile demaraction}_i  \\
0 & \quad \text{if } \{p_{1, t-1}, p_{2, t-1}, p_{1, t}\} \text{~not in tile demarcation}_i \\ \end{cases} 
\end{gather}

Since tiles within a tiling are non-overlapping, any state-action combination activates exactly $\mathcal{T}$ tiles, one per tiling. The total number of features is simply $\mathcalligra{T}~\tau^3$. Note that the tabular case can be recovered as a special case by setting $\mathcalligra{T}~ = 1$ and $\tau \leq m$. In this case, every tile is activated by at most one feasible state-action combination which is equivalent to storing a dedicated coefficient for every state-action combination.\footnote{If $\tau > m$, some tiles would never be activated. But again, every table entry would correspond to a unique tile.}

\textbf{TBD: This feature exhibits the advantage of function approximation in large state spaces.  dimensionality problem when increasing the exponent, or $\tau$ not so much when increasing the number of tilings}
