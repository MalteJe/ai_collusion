\section{Results}\label{results}
This section reports on the simulation outcomes of the baseline specification. To foreshadow the results, profits mostly exceed Nash-predictions, but remain below monopoly profits. While agents learn to charge supra-competitive prices, they fail to incorporate \emph{reward-punishment} schemes consistently. Overall, the results crucially hinge on the combination of \gls{fem} and selected parameters. Only tabular learning exhibits a clear tendency to punish deviations with lower prices in subsequent periods. Before I detail those points, it is helpful to create a common vocabulary.

I report results for various specifications and will refer to every unique combination of \gls{fem} and parameters as an \emph{experiment}. Every experiment consists of 48 \emph{runs}, i.e. repeated simulations with the exact same set of starting conditions. Lastly, within the scope of a particular \emph{run}, time steps are called \emph{periods}.\footnote{The simulations are run in \emph{R}. The code is available on github (\url{https://github.com/MalteJe/ai_collusion}). Another technical note: The program seed does not vary between experiments, i.e.\ for every run there is a \emph{sibling run} with an equivalent initialization of the random number generator in every other experiment. The seed determines the initial state and the decision in which periods the agents decide to explore.}

\subsection{Convergence}\label{convergence}

As outlined in \autoref{convergence_considerations}, convergence is not guaranteed in a non-stationary environment. Notwithstanding the lack of a theoretical convergence guarantee, prior experiments have shown that simulation runs tend to approach a stable equilibrium in practice (see \autoref{literature review}). Beware that I use a descriptive notion of equilibrium that is detached from economic theory. It is characterized by the simple observations that the same set of prices continuously recur over a longer time interval. It \emph{is} a stable equilibrium because both agents play what they \emph{estimate} is their best action given the state set. But I do not require that their belief in the best course of action is sound, let alone optimal.\footnote{Also, I do not check rigorously whether off-path strategies are stable. Though I show the reaction to forced deviations in \autoref{deviations}}. In fact, at times the observed outcomes in this study starkly contradict predictions from game theory. For instance, despite symmetric profit functions, the converged outcomes may display asymmetric prices. Moreover, price cycles, i.e.\ a recurring sequence of price combinations, occur frequently.\footnote{The model from \autoref{quantity} predicts symmetric outcomes without cycles. This is typical for simultaneous pricing games, but not universal across economic models. For instance, collusive outcomes in quantity competition (i.e.\ Cournot) may exhibit price asymmetries. \textcite{maskin_theory_1988} pioneer a sequential pricing game that predicts \emph{Edgeworth price cycles} where agents successively undercut each other until one firm prefers to reset the cycle and increases its price. Based on their model, \textcite{klein_autonomous_2019} shows that \emph{Q-Learning} agents are indeed capable of learning those dynamic strategies.}

\begin{figure}
	\includegraphics[width=\linewidth]{plots/converged.png}
	\caption[Converged runs by \gls{fem} and $\alpha$]{Number of runs per experiments that (i) achieved convergence, (ii) did not converge or (iii) failed to complete as a function of \gls{fem} and $\alpha$.}
	\label{converged}
\end{figure}

The following, arbitrary, but practical convergence rule was employed. If a price cycle recurred for 10,000 consecutive periods, the algorithm is considered \emph{converged} and the simulation concludes. A price cycle requires both agents' adherence.\footnote{Of course it is possible that the cycle length differs between agents. For instance, one agent may continuously play the same price while the opponent keeps alternating between two prices. In this case, the cycle length is $1*2=2$.} For efficiency reasons, price cycles up to a length of 10 are considered and a check for convergence is undertaken only every 2,000 periods. If no convergence is achieved until 500,000 periods, the simulation terminates and the run is deemed \emph{not converged}. Furthermore, there are a number of runs that \emph{failed to complete} as a consequence of the program running into an error. Unfortunately, the program code does not allow to examine the exact cause of such occurrences in retrospect. However, the failed runs only occurred with unsuitable specifications (see below for a detailed discussion).

In accordance with the outlined convergence criteria above, \autoref{converged} displays the share of runs that, respectively, converged successfully, did not converge until the end of the simulation or failed to complete. Two main conclusions emerge. First, failed runs are only prevalent in the polynomial tile experiment with $\alpha = 0.001$. Second, the tiling methods are more likely to converge. Both points deserve some further elucidation.

Regarding the failed runs, recall from \autoref{learning_speed_considerations} that features of polynomial extraction are not binary and warrant cautious adjustments of the coefficient vector. I suspect that with unreasonably large values of $\alpha$, the estimates of $\boldsymbol{w}$ overshoot early in the simulation, diverge and at some point exceed the software's numerical limits.\footnote{Controlled runs where I could carefully monitor the development of the coefficient vector $\boldsymbol{w}$ seem to confirm the hypothesis.} While important to acknowledge, the failed runs are largely an artifact of an unreasonable specification and I will not account for them for the remainder of this text.

Out of the completed runs without program failure, 95.8\% did converge. Interestingly, there are subtle differences between \gls{fem}s. With only one exception, both tiling methods converged consistently for various $\alpha$. With only 89.3\% of runs converging, separate polynomials constitute the other extreme. The figure also indicates that convergence becomes less likely for low values of $\alpha$. With tabular learning, 92.1\% of runs converged without clear relation to different values of $\alpha$.

\autoref{convergence_at} displays a frequency polygon of the runs that achieved convergence within 500,000 periods. Clearly, the distribution is fairly uniform across \gls{fem}s. Most runs converged between 200,000 and 300,000 runs. This is an artifact of the decay in exploration as dictated by $\beta$. Before the focal point of 200,000 is reached, agents probabilistically experiment too frequently to observe 10,000 consecutive periods without any deviation from the learned strategies. Thereafter, it becomes increasingly likely that both agents keep \emph{exploiting} their current knowledge and continuously play the same strategy for a sufficiently long time to trigger the convergence criteria. Note that the low quantity of runs converging between 300,000 and 500,000 suggests that increasing the maximum of allowed periods would not necessarily produce a significantly higher share of converged runs.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/convergence_at.png}
	\caption[Timing of convergence by \gls{fem}]{Timing of convergence by \gls{fem}. Only includes converged runs. Width of bins: 8,000.}
	\label{convergence_at}
\end{figure}

\autoref{cycle_length} visualizes the distribution of cycle length and offers some interesting insights. Unsurprisingly, a first glance suggests that the frequency of runs decreases with cycle length. Not accounting for differences between selection methods, the bars appear similar to a geometric distribution with the largest bar corresponding to a 'cycle length of 1' (i.e.\ no cycle at all). Moving towards the right, the frequency of observed runs decreases with cycle length, though at a decreasing pace. In fact, there are even 6 runs with the largest considered cycle length of 10.

This time, the differences between \gls{fem}s are substantial. Polynomial tiles largely follows the described decaying pattern. Similarly, tile coding rarely converges in long cycles, though the most prevalent cycle length is 2 (with a total of 220 runs). Interestingly, the frequency of cycle length of converged tabular runs is distributed almost uniformly. This observation also suggests that the employed convergence rule may well have misclassified some of the runs in the top left panel of \autoref{converged} as \emph{not converged} where in reality the convergence cycle length simply exceeded the threshold arbitrarily set at 10. Finally, all runs of the separate polynomials \gls{fem} converged in a static price. This is in sharp contrast to the other methods. I will revisit the observation in \autoref{deviations} 

\begin{figure}
	\includegraphics[width=\linewidth]{plots/cycle_length.png}
	\caption[Count of converged runs by \gls{fem} and cycle length]{Count of converged runs by \gls{fem} and cycle length.}
	\label{cycle_length}
\end{figure}


It is not obvious why there exist such differences between \gls{fem}s, especially since there is no economic justification for price cycles in a simultaneous pricing environment in the first place. Appendix \ref{prices} unveils that the cycle length is also positively related to the range of prices agents charge upon convergence. Next, I proceed by examining profits.

\subsection{Profits}\label{profits}

In order to benchmark the simulation profits, I normalize profits as in \textcite[p.3277]{calvano_artificial_2020} and \textcite[p.3]{hettich_algorithmic_2021}:

\begin{gather}
\Delta = \frac{\bar{\pi} - \pi_n}{\pi_m - \pi_n} ~~ \text{,}
\end{gather}

where $\bar{\pi}$ represents profits averaged over the final 100 time steps upon convergence and over both agents in a single run.\footnote{Naturally, in the case of non-converged runs I use the final 100 episodes before termination.} The normalization implies that $\Delta = 0$ and $\Delta = 1$ respectively reference the Nash and monopoly solution.\footnote{I will denote these special cases as $\Delta_n$ and $\Delta_m$.} Note that it is possible to obtain a $\Delta$ below $0$ (e.g. if both agents charge prices equal to marginal costs), but not above $1$.\footnote{Strictly speaking, exactly 1 is not attainable either. Recall that $m$ was chosen to allow for prices very close, but not equal to both benchmark prices. With $m = 19$, the highest feasible $\Delta$ is 0.9997.}

\begin{figure}
	\includegraphics[width=\linewidth]{plots/alpha.png}
	\caption[Average $\Delta$ by \gls{fem} and $\alpha$]{Average $\Delta$ by \gls{fem} and $\alpha$. Includes converged and non-converged runs. Three experiment (polynomial tiles with $\alpha = 0.0001$ and separated polynomials with $\alpha \in \{0.001, 0.0001\}$) are excluded for better presentability. An exemplary experiment from \textcite{calvano_artificial_2020} is provided for comparison purposes. Beware the logarithmic x-scale.}
	\label{alpha}
\end{figure}

\autoref{alpha} displays the convergence profits as a function of \gls{fem} and $\alpha$.\footnote{For comparison purposes, I also show the closest available experiment from \textcite{calvano_artificial_2020} ($\beta = 2*10^{-5}$, $m = 15$). The disparity to tabular learning runs in this study can be explained by differences in parameter choices and experiment setup (e.g.\ initialization of Q-Matrix).} Every data point represents one experiment, more specifically the mean of $\Delta$ across all runs making up the experiment. First of all, note that average profits consistently remain between both benchmarks $p_m$ and $p_n$ across specifications.\footnote{There are three exceptions that are hidden in the plot to preserve reasonable y axis limits. More specifically, for the \gls{fem} polynomial tiles with $\alpha = 0.0001$, the average $\Delta$ is $-1.72$. With separated polynomials, the average $\Delta$ with, respectively, $\alpha = 0.0001$ and $\alpha = 0.001$ is $-1.86$ and $-0.282$. This refines the observation from \autoref{convergence}. It appears that high values of $\alpha$ converge in equilibrium strategies void of any reasonableness. In the case of polynomial tiles, the program even crashes due to diverging parameters.} As with prior results, the plot unveils salient differences between \gls{fem}s. On average, polynomial tiles runs yield the highest profits. The average $\Delta$ peaks at 0.84 for $\alpha = 10^{-8}$. Higher values of $\alpha$ tend to progressively decrease profits. Moving downwards on the y-axis, both tabular learning and tile coding yield similar average values of $\Delta$. Furthermore, the level of $\alpha$ does not seem to impact $\Delta$ much. For both methods $\alpha = 10^{-4}$ induces the highest average $\Delta$ at 0.48 and 0.47 respectively. Similarly for separate polynomials, $\Delta$ does not seem to respond to variations in $\alpha$. The maximum $\Delta$ is 0.37.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/alpha_violin.png}
	\caption[Distribution of $\Delta$ by \gls{fem} and $\alpha$]{Distribution of $\Delta$ by \gls{fem} and $\alpha$. Includes converged and non-converged runs. Violin widths are scaled to maximize width of individual violins, comparisons of widths between violins are not meaningful. Violins are trimmed at smallest and largest observation respectively. Three experiment (polynomial tiles with $\alpha = 0.0001$ and separated polynomials with $\alpha \in \{0.001, 0.0001\}$) and a single run with $\Delta < -0.5$ are excluded for better presentability. Horizontal lines represent the median.}
	\label{alpha_violin}
\end{figure}

Naturally, averaging $\Delta$ over all runs of an experiment, as done to create \autoref{alpha},  has the potential to hide subtleties in the distribution of $\Delta$. Therefore, \autoref{alpha_violin} displays a violin plot illustrating the distribution of $\Delta$ per experiment. The distribution largely confirms the conclusion that most runs converge between $\Delta_m$ and $\Delta_n$. The only \gls{fem} that generated a significant quantity of runs with profits below the Nash benchmark are separate polynomials where 39.3\% of runs converged with profits below the Nash equilibrium (though this is heavily biased through the experiments with high $\alpha$ where the share of negative average $\Delta$ is 100\%). While the other \gls{fem}s tend to elicit runs within the benchmarks, the variability remains quite high. This indicates a degree of path dependence and suggests that the algorithms are prone to stick to early explored strategies that are \emph{above average}, but \emph{sub-optimal}. Polynomial tiles exhibit the narrowest range of $\Delta$, in particular for low $\alpha$.

These insights in conjunction with \autoref{convergence} and \autoref{alpha} establish that, what constitutes a sensible value of $\alpha$ clearly depends on the \gls{fem}. Therefore, for the remainder of this chapter, I will select an 'optimal' $\alpha$ for every \gls{fem} and present further results only for these combinations. In determining \emph{optimality} of $\alpha$, I don't rely on a single hard criteria, rather I consider a number of factors including the percentage of converged runs, comparability with previous studies and prefer to select experiments with high average $\Delta$ as they are most central to the purpose of this study. \autoref{justifications} provides a justification for every experiments setting deemed \emph{optimal}. To get a sense of the variability of runs within the optimized experiments and the price trajectory over time, Appendix \ref{trajectory_prices_section} contains further visualizations of the development of prices and profits of all runs with optimized $\alpha$.


	\begin{table}
		\centering
		\begin{tabular}{|l|c|l|}
			\hline
			\textbf{\gls{fem}}&$\boldsymbol{\alpha}$&\textbf{Justification} \\
			\hline
			Tabular&0.1&- comparability with previous simulation studies \\
			&&- most pronounced response to price deviations \\
			&& \ \ (see \autoref{deviations}) \\
			\hline
			Tile Coding&0.001&- high $\Delta$ \\
			&&- most pronounced response to price deviations \\
			&&\ \ (see \autoref{deviations}) \\
			\hline
			Separate Polynomials&$10^{-6}$&- high percentage of converged runs \\
			\hline
			Polynomial Tiles&$10^{-8}$&- high $\Delta$ \\
			\hline
		\end{tabular}
		\caption{\emph{Optimized} values of $\alpha$ by \gls{fem}}
		\label{justifications}
	\end{table}



\subsection{Deviations}\label{deviations}

This section examines whether the learned strategies are stable in the face of deviations from the learned behavior. There are at least two explanations for the existence of supra-competitive outcomes. First, agents simply fail to learn how to compete effectively and miss out on opportunities to undercut their opponent. Second, agents avoid deviating from the stable strategy because they fear retaliation and lower (discounted) profits in the long run. Importantly, only the latter cause, supra-competitive prices underpinned by some form of a \emph{reward-punishment scheme}, allows to label the outcomes as \emph{collusive} and warrants attention from competition policy \parencite[p.37]{assad_algorithmic_2020}. 
Therefore, I conducted an artificial \emph{deviation experiment} to scrutinize whether agents learn to actually retaliate in the wake of a deviation. The bottom line of that exercise is that, only tabular learning evokes a conspicuous punishment from the non deviating agent. Before the results are discussed in detail, I follow with a short portrayal of the deviation experiment.

Denote the period in which convergence was detected as $\tau = 0$. At this point, both agents played for 10,000 periods an equilibrium strategy they mutually regard as optimal. At $\tau = 1$, I force one agent to deviate from her learned strategy and play instead the short-term best response that mathematically maximizes immediate profits. Subsequently, she reverts to the learned strategy. In order to verify whether the non deviating agent proceeds to punish the cheater, he sticks to his learned behavior throughout the deviation experiment. In total, the deviation episode lasts 10 periods. Learning and exploration are disabled (i.e.\ $\alpha = \epsilon = 0$).\footnote{See \autoref{prolonged_deviations} for prolonged deviations and continued learning \emph{after} detected convergence.} In order to evaluate the deviation, it appears useful to define a \emph{counterfactual} situation where both agents stick to their learned strategies for another 10 periods. Comparing (discounted) profits between the experiment and the counterfactual allows to assess the profitability of the deviation.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/average_intervention.png}
	\caption[Average price trajectory around deviation by \gls{fem}]{Average price trajectory around deviation by \gls{fem}. Points represent the average price over all runs of an experiment. Dashed horizontal lines represent the fully collusive price $p_m$ and the static Nash solution $p_n$. Dotted vertical line reflects time of convergence, i.e.\ the period immediately before the forced deviation.}
	\label{average_intervention}
\end{figure}

As the responses to one agent's deviation vastly differ across \gls{fem}s, it is natural to discuss them separately at first and contrast differences only thereafter. It is difficult to summarize all information in a single graph or table, so I will consult \autoref{average_intervention}, \autoref{intervention_boxplot} and \autoref{share_deviation_profitability} simultaneously to describe the deviation and response patterns. Before that, a brief description of these plots is in line. \autoref{average_intervention} displays the price trajectory around the forced deviations averaged over all runs of an experiment.\footnote{To reiterate the result from previously, the plot reinforces that the price variation between periods is non-negligible \emph{before} the deviation even takes place - despite averaging over all runs of the optimal experiments.}  Since the average price trajectory might veil important differences between runs, \autoref{intervention_boxplot} illustrates the range of deviation and punishment prices compared to the counterfactual price that would have materialized if no deviation had taken place and agents kept following their learned strategies. Note that in the presence of price cycles, part of the variation can be explained by \emph{cycle shifting}, a phenomenon where the agents return to the learned cycle but the intervals are not aligned with the counterfactual path. These differences should even out over all runs of an experiment and therefore, not systematically bias the boxes in either direction. Similarly, the average price response in \autoref{average_intervention} is largely unaffected by this phenomenon. Finally, \autoref{share_deviation_profitability} reports the share of deviations that turned out to be profitable compared to the counterfactual.\footnote{Appendix \ref{deviations_appendix} contains further visualizations of the deviation experiments.}

\begin{figure}
	\includegraphics[width=\linewidth]{plots/intervention_boxplot.png}
	\caption[Distribution of price differences around deviation by \gls{fem}]{Distribution of price differences around deviation relative to counterfactual path \emph{without} forced deviation, i.e.\ the difference to the price had no deviation taken place, by \gls{fem}. Only includes converged runs because a clear counterfactual exists. Boxes demarcate 15th and 85th percentiles. They are extended by whiskers that mark the entire range of price differences. Horizontal lines represent the group median.}
	\label{intervention_boxplot}
\end{figure}

As indicated earlier, the non-deviating agents in tabular learning runs learned to punish deviations. \autoref{average_intervention} shows that the non deviating agent, on average, undercuts the deviation price at $\tau = 2$.  Simultaneously, the deviating agent already begins reverting to pre-deviation prices. However, this result's general validity is qualified. \autoref{intervention_boxplot} unveils that the non deviating agent does not always reduce prices compared to the counterfactual. Despite the existence of punishment prices in some runs, agents are fairly quick to return to the price levels observed before the deviation was forced upon them. As early as $\tau = 3$ there is no visible difference between average pre- and post-deviation price levels.\footnote{Previous studies showcase a strong deviation is usually followed by a more gradual reversion to pre-deviation behavior (around 5-10 periods), see in particular Figure 4 in \textcite{calvano_artificial_2020} and Figure 3 in \textcite{klein_autonomous_2019}.} This might partly follow from prices being relatively close to the Nash equilibrium in the first place. The punishments ensure that deviating is (strictly) profitable in only 24\% of runs. This suggests that, upon convergence, agents stick to a stable equilibrium, from which deviations tend to be unprofitable due to the cheated agent retaliating. These findings confirm the results from previous studies (see \autoref{literature review}).


	\begin{table}
		\centering
		\input{tables/share_deviation_profitability.tex}
		\caption[Share of profitable deviations by \gls{fem} and agent]{Share of profitable and non-profitable deviations by \gls{fem} and agent. Deviations are deemed \emph{profitable} if the discounted profits until $\tau = 10$ due to the deviation exceed cash flows from a counterfactual without deviation. Only includes converged runs because a clear counterfactual exists. Discounting is equivalent to $\gamma$ in \autoref{td_error_expected}, i.e.\ 0.95. A significant number of 'deviations' are neither profitable nor unprofitable. In those runs, the learned strategy of the deviating agent is actually the best response at $\tau = 1$ and both agents keep following their respective price cycle.}
		\label{share_deviation_profitability}
	\end{table}

When examining the outcomes of the other \gls{fem}s, different conclusions emerge. Recall from \autoref{alpha} that tile coding yielded convergence profits very similar to tabular learning. Yet, \autoref{average_intervention} and \autoref{intervention_boxplot} only hint at slight punishments in some runs. In fact, the median of the cheated agent's price at $\tau = 2$ is exactly 0, which amounts to a complete absence of a response. This lack of punishment renders 56\% of the cheater's deviations profitable. In light of that, it is surprising that the cheating agent tends to return to pre-intervention price levels instead of continuing to exploit her opponent's failure to punish deviations.

This is even more obvious for the separate polynomials \gls{fem}. The deviation responses are easy to summarize. In all runs, after the forced intervention at $\tau = 1$, both agents immediately return to the pre-deviation equilibrium. This is remarkable for two reasons. First, the non deviating agent completely fails to punish the cheater's behavior and does not respond to the price cut whatsoever. Consequently, 69\% of the deviations are profitable.\footnote{The remaining 31\% comprise runs where the deviating agent was already playing the short-term best response. Remember that the separate polynomials \gls{fem} tends to converge with prices at or close to the Nash equilibrium.} This leads to the second point. Despite the obvious advantage of cheating, the deviating agent returns to the pre-deviation price without exception, thus failing to exploit her opponent's weakness. To put this in the right context, remember that the initial price levels are fairly close to the Nash equilibrium and the deviation's profitability is relatively small compared to the potential gains realizable in other experiments (see also Appendix \ref{deviations_appendix}). Still, it appears puzzling that such a simple strategy improvement remains consistently untapped. A potential explanation is this. Recall from \autoref{separate_polys} that \emph{separate polynomials} preserve a distinct set of parameters for every feasible action $a$.\footnote{As opposed to the two other function approximation \gls{fem}s that treat the action space as a continuous variable.} It appears that the set for the preferred action is constructed in a way that yields high estimated values for that action \emph{irrespective of the state}. In its effect, this is paramount to a simplistic valuation technique involving just \emph{one} parameter per action.\footnote{With $m=19$, that hypothetical \gls{fem} entails just 19 parameters.} With that approach, the agent completely disregards the state set and simply plays the preferred action at all times. A generous interpretation is that this is similar to a static Nash equilibrium. However, many runs converge in price levels way above that. To conclude, the agents' failure to play economically sound strategies casts doubts on the viability of the \gls{fem} in reality.

Finally, turn your attention to the experiment with polynomial tiles. Recall that this experiment generated prices closest to the monopoly benchmark. Despite that, the deviation experiment for polynomial tiles leads to similar conclusions to the ones with separate polynomials. Though there are some variations between runs that warrant detailed examination. Consider first the non deviating agent. Again, the majority of runs exhibits a failure to respond to the price cut. However, selected runs show a \emph{matching} strategy where the cheated agent meets the price cut with a similar price. Notably, in those circumstances, agents \emph{do not return} to the previously learned path but quickly establish a new equilibrium. Moreover, note that Figures \ref{average_intervention} and \ref{intervention_boxplot} display a slight bias downwards over all periods. This is indicative of \emph{continued cheating} of the deviating agent. After being forced to undercut the price, she proceeds to set prices below pre-deviation levels without getting punished. This, too, results in a new equilibrium.\footnote{\autoref{intervention_poly_tiling} in \autoref{appendix} illustrates both phenomena (price matching and continued cheating) through the exact price sequence of exemplary runs.} In light of high pre-deviation prices and the lack of retaliatory prices, it is unsurprising that 85\% of deviations are profitable.

To summarize the deviation experiments, tabular learning agents learned to collude and tend to support the  supra-competitive equilibria with a reward-punishment scheme. On the other hands, baring a few exceptions, the non deviating agents in experiments with the \gls{fem}s utilizing function approximation \emph{fail to respond to price cuts} and are easy to exploit. Moreover, the deviating agents tend to leave that weakness unexploited. Overall the deviation exercise suggests that while algorithmic agents manage to sustain high prices when playing each other, their strategies are incomplete and easy to exploit.

Evidently, under the regime of this study's simulations, tabular learning is better in producing stable supra-competitive outcomes than the function approximation \gls{fem}s. To illustrate the notion of \emph{stability} in this context, consider the following thought experiment of a \emph{superagent}. Upon convergence, a rational player with perfect information about the economic environment and the learned policies of both agents enters the game and takes over pricing authority from one of them.\footnote{For the sake of the argument it is irrelevant whether this superagent is human or not.} Importantly, the superagent could anticipate the opponent's price in the next period and calculate the short-term maximizing response as well as the opponent's reaction to the deviation and so on. When playing against a tabular learning agent, the superagent would deliberately stick to the convergence pricing scheme as cheating is sure to evoke a retaliation rendering a deviation unprofitable (see \autoref{share_deviation_profitability}). Contrary, when facing an opponent who learned its strategy through a function approximation \gls{fem}, the superagent could easily cheat on the opponent to increase short-term profits without being punished in subsequent periods.

The evidence also suggests that function approximation creates hesitation in the agents to change best responses. Probabilistically, \emph{exploration} ensures that both agents will undercut the price of their opponent and realize excess profits similar to those in the forced deviation experiment. However, it appears that agents fail to learn (enough) from such \emph{explored cheating}. As evidenced by the undertaken deviation experiment, they typically return to the pre-deviation price (cycle) immediately. This rigidity in adjusting strategies potentially points to a problem with the specific algorithm or the tuning of its parameters. For instance, a higher $\alpha$ could enable the cheater to learn faster that an unpunished deviation is more profitable than adhering to the learned strategy. 

Recall that learning and exploration were turned off for the deviation experiment. This gives rise to an objection to the presented results. The non deviating agent, stripped of the ability to adjust his strategy, might only be exploitable for a finite number of periods until he adjusts his strategy. In fact, a generosity to condone isolated price cuts might be conducive to establishing high price levels early in the simulation runs. However, the next section demonstrates that the lack of punishment in response to a deviation remains ubiquitous in prolonged deviation experiments with enabled learning ($\alpha > 0$). Moreover, I show that the findings are robust to variations in parameters and changes to \autoref{expected SARSA}.
