\section{Robustness and variations}\label{robustness}

To show that the presented findings are not an artifact of specific experiment design choices, this section performs are variety of robustness checks and reports results on slightly altered learning schemes. In all additional experiments I hold $\alpha$ fixed at its optimal values (see \autoref{justifications}). I will show that the considered variations do not impact the simulation results much. Rather, they reinforce the conclusions from the last section. First, I consider an alternative deviation experiment. In sections \ref{vary_parameter} and \ref{price_grid_section} I vary the parameters controlling the learning process and the price grid that is available to both agents. Variations to the discount factor are presented in \autoref{discounting}. Finally, I consider slight alterations to, respectively, the algorithm and the reward setting in sections \ref{vary_algorithm} and \ref{differential}.

\subsection{Prolonged deviations}\label{prolonged_deviations}

To extend the mixed results from the deviation experiments in \autoref{deviations}, I conducted another \emph{prolonged deviation} experiment with continued learning. As I will show, the previously drawn conclusions remain intact. However, first I briefly explain why a continued intervention could theoretically be different from a one-time deviation. The critical components are continued learning and the eligibility trace vector $\boldsymbol{z}$ that make conceivable an agent tolerating isolated deviations but punishing longer price cuts.

Without eligibility traces and the ability to learn, a one-time deviation suffices to assess retaliatory behavior because the memory is too short to \emph{remember} that the opponent cheated for longer than a single period. Likewise, stripping the non deviating agent of his ability to update $\boldsymbol{w}$ renders him unable to learn that tolerating deviations is exploitable and can culminate in continuous low rewards. Consequently, if he failed to punish a deviation at $\tau = 2$, he will not react at $\tau = 3$ either. On the contrary, with the ability to learn enabled, both agents can readjust the parameter updates. For instance, after discovering that tolerating a one-time deviation yields a low reward, the non deviating agent might adjust $\boldsymbol{w}$ and decide to play a different action next time he is cheated (e.g.\ match the price cut). This is augmented by the length of deviation episodes and the existence of eligibility traces. If the deviating agent \emph{continues} to cheat, the opponent should continue to decrease the valuation of the \emph{tolerating} strategy and could ultimately fall back to the next best action (which might be a price cut).\footnote{Furthermore, remember that the deviation experiment is conducted right after convergence was detected. Consequently, the algorithm was \emph{on-path} for a large number of periods and the eligibility traces have not been reset recently (see \autoref{eligibility_trace_update}). Therefore, updates are large in magnitude and after the deviation experiment concludes, the convergence equilibrium might not be feasible anymore because its valuation by the agents changed.}

The \emph{prolonged deviation} experiment lasts 20 periods in total. It was set up as follows. The deviating agent anticipates the price of her opponent perfectly and continuously plays the best response for a total of 10 periods of cheating. This imposes the assumption that she is capable of perfectly predicting her opponent's response to the initial deviation. Exploration remains disabled ($\epsilon = 0$) but both agents continue learning from their actions.\footnote{I also prescribe that the forced deviation is considered \emph{on-policy}. Since $\epsilon = 0$, this is most natural to incorporate.} After I stop forcing the deviating agent to play the best response, both agents play another 10 periods adhering to their learned strategies.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/average_prolonged_intervention.png}
	\caption[Average price trajectory around prolonged deviation by \gls{fem}]{Average price trajectory around prolonged deviation by \gls{fem}. Points represent the average price over all runs of an experiment. Dashed horizontal lines represent the fully collusive price $p_m$ and the static Nash solution $p_n$. Dotted vertical line reflects time of convergence, i.e.\ the period immediately before the forced deviation.}
	\label{average_prolonged_intervention}
\end{figure}

The additional deviation experiments were conducted with the optimal values of $\alpha$ in accordance with \autoref{justifications}. \autoref{average_prolonged_intervention} displays the average price trajectory around the prolonged deviation and confirms the previous observations. Only with tabular learning does the non deviating agent match the price cuts systematically. The top left panel shows both agents hover around the Nash benchmark for the entire duration of the deviation episode. Clearly, this is unprofitable for both agents but the punishment is necessary to sustain supra-competitive prices in the first place. Note also the quick return to pre-deviation levels as soon as the deviating agent returns to her learned behavior. Since the opponent follows immediately, it appears that the return must be initiated be the original cheater. The experiment illustrates that the supra-competitive outcomes remain sustainable in the face of persistent interruptions.

With regard to the three function approximation methods, the deviating agent appears to systematically exploit her opponent who fails to punish the price cut. The subtle differences between \gls{fem}s extend to the prolonged deviation. Separate polynomials evoke no response from the non deviating agent. Both tiling methods show a small \emph{average} price cut over the duration of the prolonged deviation, but this response falls short of a reliable mechanism that consistently deters deviation across runs. Indeed, Appendix \ref{prolonged_deviations_appendix} shows that only isolated runs exhibit the non deviating agent cutting the pre-deviation price levels. Most runs show no reaction which is veiled by averaging over all runs of the experiment. This absence of a retaliation opens up the opportunity for continuous exploitation by the deviating agent. Despite that, the latter tends to return to pre-deviation price levels. Therefore, both agents act far from optimal (in the economic sense of the word) and fail to learn (enough) from the prolonged deviation experiment. Lastly, note the difference between pre- and post-deviation price levels at the bottom right panel, representing polynomial tiles. As noted previously, this suggests that the agents proceed to play a different, less profitable equilibrium after the deviation. This easy switch to a new strategy further challenges the viability of the pre-deviation equilibrium in the first place.

It is conceivable, maybe even likely, that the non deviating agent does alter its strategy after a time frame much longer than 10 periods. However, this is not important for this study because the agent's strategy is easy to exploit in the short term and the deviations are clearly profitable (refer back to \autoref{share_deviation_profitability}).


\subsection{Learning parameters}\label{vary_parameter}

Besides the learning rate $\alpha$, the exploration strategy is arguably the most important steering choice in reinforcement learning. As discussed, $\beta$ controls the decay in exploration over time. To assess its impact on the sensitivity of outcomes, I run a number of experiments varying $\beta$ while keeping the manually optimized values of $\alpha$ constant (see \autoref{justifications}).\footnote{Note that these values are not necessarily 'optimized' for alternative $\beta$. Ideally, exploration rate and learning speed should not be considered in isolation. Indeed, \textcite{calvano_artificial_2020} show that lower values of $\alpha$ perform better if exploration is extensive. However, the scope of this study does not allow to systematically search over a 2-dimensional grid of $\alpha$ and $\beta$.} Naturally, I adjust  proportionately the number of maximal periods before a run is forced to terminate.\footnote{With the lowest and highest values of $\beta$ ($0.00016$ and $10^-{5}$), the maximum number of periods is adjusted to $125,000$ and $2,000,000$ respectively.}

\begin{figure}
	\includegraphics[width=\linewidth]{plots/beta.png}
	\caption[Average $\Delta$ by \gls{fem} and $\beta$]{Average $\Delta$ by \gls{fem} and $\beta$. Includes converged and non-converged runs.}
	\label{beta}
\end{figure}

\autoref{beta} displays that the impact of exploration on average $\Delta$ is relatively small across \gls{fem}s. Interestingly, applying the deviation routine described in \autoref{deviations} uncovers that extended exploration supports the stability of the convergence equilibrium only in the case of tabular learning. \autoref{share_deviation_profitability_beta} clearly shows that cheating becomes less profitable when the non-deviating agent utilizing tabular learning had more opportunities to explore reactions to a deviation. The share of profitable deviations ranges from 39\% at $\beta = 0.00016$ to 6\% at $\beta= 2*10^{-5}$. Strangely, the separate polynomials \gls{fem} shows the opposite pattern. Less exploration makes deviations less attractive. My interpretation is that the overall lower prices levels associated with less exploration indicate that the deviating agent already plays the best response in many runs. Lastly, the value of $\beta$ does not seem to have a large impact on either of the tiling \gls{fem}s. Their convergence equilibria are unstable for all trialed values of $\beta$.

\begin{table}
	\centering
	\input{tables/share_deviation_profitability_beta.tex}
	\caption[Share of profitable deviations by \gls{fem}, agent and $\beta$]{Share of profitable deviations by \gls{fem}, agent and $\beta$. Annotations from \autoref{share_deviation_profitability} apply.}
	\label{share_deviation_profitability_beta}
\end{table}

Appendix \ref{beta_appendix} supplements the observation of this section by showing that punishment severity and length also increase with extended exploration. Moreover, in Appendix \ref{lambda_appendix} I briefly discuss that the choice of $\lambda$ does have little impact on deviation behavior. But the variance in the distribution of profits between runs seems to increase with high values of $\lambda$.

\subsection{Price grid}\label{price_grid_section}

\autoref{feature_extraction_summary} emphasized that the length of the parameter vector $\boldsymbol{w}$ with tabular learning increases disproportionately with $m$. Likewise, the optimization problem is likely to become more complex. On the other hand, the feature extraction mechanisms of tile coding and polynomial tiles are largely unaffected by $m$. To gauge the effect on outcomes, I executed experiments with additional variations, specifically $m=10$, $m = 39$ and $m = 63$.\footnote{As before, the odd numbers are chosen to enable prices close to $p_m$ and $p_n$} Due to computational restrictions, these experiments  only comprise 16 runs. Accordingly, inference should be treated with care.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/m.png}
	\caption[Average $\Delta$ by \gls{fem} and $m$]{Average $\Delta$ by \gls{fem} and $m$. Includes converged and non-converged runs.}
	\label{m}
\end{figure}

Unsurprisingly, convergence becomes less likely when $m$ increases. While all runs with $m=10$ converged, the percentage for $m=39$ and $m=63$ is only  67.2\% and 57.8\% respectively.\footnote{ This is driven mainly by less converged runs with the \gls{fem}s tabular learning and tile coding. See Appendix \ref{price_grid_appendix}.} Despite that, \autoref{m} indicates that varying $m$ does not seem to have much of an impact on the average $\Delta$. On first glance, this seems confusing. As the complexity of the problem increases, one would expect agents to struggle with optimizing their strategy. However, the puzzle is partly solved by taking into account the stability of the equilibrium. \autoref{share_deviation_profitability_m} suggests that the share of profitable deviations increases with $m$ for tabular learning and the polynomial tiles \gls{fem}. Most notably, the share of profitable punishments in runs with tabular learning increases from only 12\% when $m=10$ to 67\% when $m=63$. Interestingly, the share of profitable deviation in runs with polynomial tiles is also significantly smaller when $m=10$. Recall that the \gls{fem} evoked a weak punishment and gave rise to new equilibria in \emph{some} runs. This tendency seems to be reinforced when $m$ is low.

To summarize, increasing the environment's complexity through $m$ makes supra-competitive outcomes not less likely, but less stable in the face of deviations. Appendix \ref{price_grid_appendix} supports this hypothesis by showing that punishments seem to be strongest with $m = 10$.

\begin{table}
	\centering
	\input{tables/share_deviation_profitability_m.tex}
	\caption[Share of profitable deviations by \gls{fem}, agent and $m$]{Share of profitable deviations by \gls{fem}, agent and $m$. Annotations from \autoref{share_deviation_profitability} apply. Empty cells are a consequence of no converged runs for the particular experiment.}
	\label{share_deviation_profitability_m}
\end{table}


Next, I will consider variations in $\zeta$. Recall from \autoref{price_grid_formula} that $\zeta$ controls the available excess range above the fully collusive price $p_m$. These prices are inferior to $p_m$ in almost any situation and the simulations confirm that few runs converge in \emph{supra-monopoly} prices. So how may a change in $\zeta$ affect the learning behavior? Most importantly, large values of $\zeta$ increase the share of available prices above $p_m$ and decreases the share of \emph{viable} prices within the range of $p_m$ and $p_n$. Consequently, the agents may quickly discard a larger share of actions engendering low (or negative) rewards and \emph{narrow down} the range of reasonable actions between $p_n$ and $p_m$. Then, with fewer available actions, the optimization within that range might be facilitated. This might be particularly important with the separate polynomials \gls{fem} because agents could learn that certain polynomials associated with actions above $p_m$ (or below $p_n$) consistently yield low rewards - irrespective of the preceding state set, refrain from playing them early in the simulation and focus on refining the polynomials of actions within the range of $p_n$ and $p_m$.

There is an additional effect on both tiling methods. The thresholds of all tiles derive from the size of the action space. Therefore, tiles are resized and relocated. The natural consequence is that some state-actions combinations will be associated with different tiles. \emph{A priori}, the effect on outcomes is hard to predict.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/zeta.png}
	\caption[Average $\Delta$ by \gls{fem} and $\zeta$]{Average $\Delta$ by \gls{fem} and $\zeta$. Includes converged and non-converged runs.}
	\label{zeta}
\end{figure}

I conducted three additional experiments with $\zeta \in \{0.1, 0.5, 1.5\}$ to assess the impact of varying $\zeta$ while keeping $m$ constant at $19$ to ensure comparability between experiments.\footnote{Note however, other $\zeta$ may prohibit playing actions very close to $p_n$ or $p_m$. For instance, with $\zeta = 1.5$, the price closest to $p_n = 1.473$ ($p_m = 1.925$) is $1.454$ ($1.908$). These gaps are quite a bit higher than in the default specification.} \autoref{zeta} illustrates that $\zeta$ significantly influences profits upon convergence. Across \gls{fem}s, the average $\Delta$ increases with $\zeta$. This trend is most pronounced with polynomial tiles. To reiterate, prices close to the collusive solution are not necessarily evidence of a stable equilibrium with a \emph{reward-punishment} scheme. If anything, the simulation runs in this study have suggested the opposite and it turns out that despite the differences in $\Delta$, the stability of the learned strategies is not heavily influenced by $\zeta$. \autoref{share_deviation_profitability_zeta} does not show prominent trends in the share of profitable deviations. As further evidence, Appendix \ref{price_grid_appendix} shows that tabular learning agents tend to punish deviations with retaliatory prices in all considered variations of $\zeta$. Similarly, the absence of retaliatory prices in experiments with function approximation \gls{fem}s does not seem to hinge on $\zeta$.

\begin{table}
	\centering
	\input{tables/share_deviation_profitability_zeta.tex}
	\caption[Share of profitable deviations by \gls{fem}, agent and $\zeta$]{Share of profitable deviations by \gls{fem}, agent and $\zeta$. Annotations from \autoref{share_deviation_profitability} apply.}
	\label{share_deviation_profitability_zeta}
\end{table}

To summarize, the grid of available prices does have some impact on the outcomes. Perhaps unsurprisingly, by increasing the complexity of the problem through the number of available prices $m$, tabular learning agents are less likely to support their convergence equilibria with punishment strategies. Similarly, an increase in the share of \emph{viable} prices in the range of $p_m$ and $p_n$ seems to lower convergence profits. Despite these subtleties, the qualitative conclusions from \autoref{results} remain intact.

\subsection{Discount factor}\label{discounting}

In dynamic oligopolies, theory ascribes great importance to the discount factor $\gamma$. Typically, there exists a critical value below which the weight on future profits becomes too low to sustain any collusive behavior. Likewise, if $\gamma$ is sufficiently high, rational actors with full information will collude on the monopoly solution. In reality, there are various reasons why decision makers may end up charging prices between both extremes. For instance, they might not be fully aware of what exactly the benchmark prices are and might struggle to communicate and agree on a joint action (explicitly or tacitly). Similarly, in reinforcement learning, it is unlikely that there exists a strict dichotomy between fully collusive and perfectly competitive agents. Indeed, the results so far suggests that many intermediate levels are realistic. Nevertheless, with lower values of $\gamma$, less weight is put on the (expected) value of the future state in \autoref{td_error_expected} and the immediate reward $R_t$ gains relative importance. Accordingly, one would expect the agents to gradually approach the Nash benchmark as $\gamma$ decreases.

To gauge the actual effect of $ßgamma$ on outcomes, I conducted a series of experiments ranging from perfectly myopic ($\gamma =0$) to almost infinitely patient ($\gamma = 0.99$) agents.\footnote{While $\gamma = 1$ is usually easy to model in economics, it is highly problematic in continuing learning tasks due to its infinite sum property (this is the main reason why discounting is commonly utilized in reinforcement learning in the first place, see e.g.\ \textcite[p.3]{schwartz_reinforcement_1993}). Consider the following example. An agent with no time preference ($\gamma = 1$) in a continuous task explores  early that a particular action consistently yields positive rewards. When \emph{exploiting}, the agent keeps playing that action and the value estimate accumulates to infinity. This results in a significant bias towards actions that have been explored early and at some point becomes computationally infeasible. Through similar reasoning, values marginally below $1$ are known to be unstable \parencite{naik_discounted_2019}.} \autoref{gamma} summarizes the variation in average $\Delta$. Though the relationship is not as clear as anticipated, the curves of tabular learning and tile coding confirm the hypothesized pattern. With $\gamma = 0$, the average profits are much closer to the Nash benchmark.\footnote{The relationship is more pronounced with prices, see Appendix \ref{discounting_appendix}.} Another interesting revelation is that the \emph{average} profits for tabular learning are highest at $\gamma = 0.85$ (average $\Delta = 0.48$). This suggests the agents struggle with high variance due to large values of $\gamma$ \parencite[p.6]{naik_discounted_2019}.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/gamma.png}
	\caption[Average $\Delta$ by \gls{fem} and $\gamma$]{Average $\Delta$ by \gls{fem} and $\gamma$. Includes converged and non-converged runs.}
	\label{gamma}
\end{figure}

With regard to the polynomial \gls{fem}s, the figure serves as further evidence of their ineptness for the considered learning task. Even without discounting ($\gamma = 0$), the outcomes remain high. In fact, they are even higher with separate polynomials. This clearly hints at a failure to learn how to compete when \emph{only} the immediate reward should matter.\footnote{I interpret this similar to the results in \textcite{waltman_q-learning_2008} where \emph{memoryless} agents without the ability to assert whether the opponent cheated still learn to charge supra-competitive prices.}

\subsection{Alternative algorithms}\label{vary_algorithm}

Of course, the specific algorithm described in \autoref{expected SARSA} is only one of many ways to use function approximation in learning tasks. I will consider two variations: \emph{Tree backup} and \emph{on-policy SARSA}.

\subsubsection{Tree backup}\label{tree_backup}

\textcite{precup_eligibility_2000} suggest the \emph{tree backup} algorithm as a successor to Q-Learning. Compared to the \emph{expected SARSA} algorithm, the update in \autoref{eligibility_trace_update} is replaced by:

\begin{gather}\label{eligibility_traces_tree_backup}
\boldsymbol{z}_t = \gamma \lambda \kappa(A_t | S_t) \boldsymbol{z}_{t-1} + \frac{\Delta \hat{q}}{\Delta \boldsymbol{w}_t}
\end{gather}

Recall that $\kappa(A_t | S_t)$ represents the probability of choosing $A_t$ if the agent were to follow a hypothetical target policy with $\epsilon= 0$. As with the eligibility trace in expected SARSA, the idea is that $\boldsymbol{z}$ resets to $\boldsymbol{0}$ as soon as a non-greedy action is played. Unsurprisingly, applying the tree backup algorithm with optimized values of $\alpha$ to the environment yields not very different results. \autoref{tb_violin} displays the distribution of $\Delta$ which is reminiscent of the violins for optimized values of $\alpha$ in \autoref{alpha_violin}. Appendix \ref{vary_algorithm_appendix} contains visualizations illustrating that the deviation experiments do not reveal new insights either.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/tb_violin.png}
	\caption[Distribution of $\Delta$ by \gls{fem} with \emph{tree backup} algorithm]{Distribution of $\Delta$ by \gls{fem} with \emph{tree backup} algorithm. Includes converged and non-converged runs. Violin widths are scaled to maximize width of individual violins, comparisons of widths between violins are not meaningful. Violins are trimmed at smallest and largest observation respectively. Horizontal lines represent the median.}
	\label{tb_violin}
\end{figure}

\subsubsection{On-policy SARSA}
\emph{Q-Learning}, \emph{tree backup} and \emph{expected SARSA} all belong to the family of \emph{off-policy} learning algorithms. This stems from the simple fact that the (discounted) value estimation of the state-action combination at $t+1$ is not always based on the actually chosen action $A_{t+1}$ (see \autoref{td_error_expected}).\footnote{The only exception is $\epsilon = 0$.}  So, it is \emph{off-path} of the actually pursued policy. Off-policy methods tend to exhaust the entire range of state-action combination well, but convergence guarantees for them are generally weaker than for \emph{on-policy} algorithms \parencite[p.257-265]{sutton_reinforcement_2018}.\footnote{The main reason why I haven't put much consideration into this is that due to the \emph{moving target problem} described in \autoref{convergence_considerations}, convergence is not guaranteed anyway. Moreover, \textcite{hettich_algorithmic_2021} shows that off-policy methods can work well with function approximation.} As their name suggests, \emph{on-policy} algorithms wait until the state-action combination at $t+1$ is actually known and only then estimate the TD error $\delta_t$. A straightforward adaption is:\footnote{The full algorithm is documented in Appendix \ref{vary_algorithm_appendix}}

\begin{gather}\label{td_error_on_policy}
\delta_t^{SARSA} = r_t + \gamma \hat{q}(S_{t+1}, A_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t) ~~ \text{,}
\end{gather}

Note that learning is delayed in the sense that $\delta_t^{SARSA}$ can only be calculated after the action in the next period has been taken. Using the optimized values of $\alpha$, I conducted one experiment per \gls{fem} with optimized $\alpha$. \autoref{op_violin} illustrates that the distribution of outcomes per experiment resembles the two \emph{off-policy} algorithms. Overall, the conclusions drawn in the previous section also apply to the \emph{on-policy} algorithm.

\begin{figure}
	\includegraphics[width=\linewidth]{plots/op_violin.png}
	\caption[Distribution of $\Delta$ by \gls{fem} with \emph{on-policy} algorithm ]{Distribution of $\Delta$ by \gls{fem} with \emph{on-policy} algorithm . Includes converged and non-converged runs. Violin widths are scaled to maximize width of individual violins, comparisons of widths between violins are not meaningful. Violins are trimmed at smallest and largest observation respectively. Horizontal lines represent the median.}
	\label{op_violin}
\end{figure}

		
\subsection{Differential reward setting}\label{differential}

In reinforcement learning, discounting is commonly used to avoid infinite value accumulation, but rarely has a practical interpretation \parencite{schwartz_reinforcement_1993}. Therefore, the blend with an economic task seems natural. However, despite wide usage, \textcite{naik_discounted_2019} argue that discounting in combination with function approximation is fundamentally incompatible in infinite sequences. They suggest an alternative \emph{differential reward} setting, where \autoref{td_error_expected} is replaced by:\footnote{See chapter 10 in \textcite[pp.249-252]{sutton_reinforcement_2018} for a rigorous treatment formulation. \textcite{hettich_algorithmic_2021} shows that the differential reward setting works well with agents in a Bertrand environment. He also compares both settings in a static environment and finds a tendency for oscillating behavior when discounting is used.}


\begin{gather}\label{differential_reward}
\delta_t^{differential} = R_t - \widetilde{R}_{t} + \hat{q}(S_{t+1}, A_{t+1}, \boldsymbol{w}_t) - \hat{q}(S_t, A_t, \boldsymbol{w}_t) ~~  \text{,}
\end{gather}

where $\widetilde{R}_{t}$ is a (weighted) average reward periodically updated according to

\begin{gather}
	\widetilde{R}_{t+1} = \widetilde{R}_t + \upsilon r_t ~~\text{,}
\end{gather}

where $\upsilon$ is a parameter controlling the speed of adjustment. The formulation ensures that recent rewards are weighted higher. The rest of \autoref{expected SARSA} remains untouched. Note that the differential reward setting does not involve any discounting. At first glance, this clashes with the economic understanding of time preferences.\footnote{This is the main reason why I have not utilized the differential setting in the main part of this study.} However, there are two arguments why the differential reward setting might still be well suited. First, \textcite[pp.253-254]{sutton_reinforcement_2018} proof that, due to the infinite nature of the Bertrand environment, the ordering of policies in the discounted value setting and the setting with average rewards are equivalent (irrespective of $\gamma$). Second, pricing algorithms tend to be used in markets with frequent price changes where it is less important whether a profit is realized immediately or in the next period.


\begin{figure}
	\includegraphics[width=\linewidth]{plots/converged_upsilon.png}
	\caption[Converged runs by \gls{fem} and $\upsilon$]{Number of runs per experiments that (i) achieved convergence, (ii) did not converge or (iii) failed to complete by \gls{fem} and $\upsilon$.}
	\label{converged_upsilon}
\end{figure}

I conducted a series of experiments varying over the following values of $\upsilon$: $0.001$, $0.005$, $0.01$, $0.025$, $0.05$ and $0.1$. As with the other variations, $\alpha$ is fixed at values deemed optimal. \autoref{converged_upsilon} shows the share of converged runs as a function of $\upsilon$ and the \gls{fem}. Disregarding two runs that failed to complete, convergence is consistently achieved for tabular learning, tile coding and separate polynomials. Contrary, only 74.2\% of polynomial tiles runs converged. This starkly contrasts the observation made in the experiments using the discounted reward setting. There, all runs with polynomial tiles converged for various values of $\alpha$.\footnote{The statements disregards runs that failed to complete. Refer back to the bottom right panel in \autoref{converged}. Appendix \ref{differential_appendix} shows that polynomial tiles in the differential reward setting also tend to converge later than the other \gls{fem}s.} Moreover, the plot suggests that low values of $\upsilon$ impede convergence for this \gls{fem}.


 \begin{figure}
	\includegraphics[width=\linewidth]{plots/upsilon.png}
	\caption[average $\Delta$ by \gls{fem} and $\upsilon$]{average $\Delta$ by \gls{fem} and $\upsilon$. Includes converged and non-converged runs. Beware the logarithmic x-scale.}
	\label{upsilon}
\end{figure}


\autoref{upsilon} displays how the average profits relative to $p_n$ and $p_m$ change with $\upsilon$. The overall impact is small. However, tabular learning and, to a lesser extent, tile coding seem to converge at higher profits when $\upsilon$ is very low.\footnote{Appendix \ref{differential_appendix} reveals that the variability of average $\Delta$ is higher than in the discounted setting.} With respect to punishment of price cuts, the results are similar to the discounted setting. Irrespective of $\upsilon$, the majority of deviations in experiments with separate polynomials and polynomial tiles is profitable and evokes no retaliation. With tabular learning, the share of profitable deviations is 24.7\% over all runs. There is slight evidence that the hint at some sort of punishment in tile coding is more pronounced in the differential reward setting. Only 48.8\% of deviations are strictly profitable.\footnote{The percentage drops to only 45.8\% if only runs with $\upsilon = 0.1$ or $\upsilon = 0.005$ are considered.} Appendix \ref{differential_appendix} shows that non deviating agents retaliate with a price cut at $\tau = 2$ in some runs.