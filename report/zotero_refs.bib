
@techreport{calvano_artificial_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Artificial {Intelligence}, {Algorithmic} {Pricing} and {Collusion}},
	url = {https://papers.ssrn.com/abstract=3304991},
	abstract = {Pricing algorithms are increasingly replacing human decision making in real marketplaces. To inform the competition policy debate on possible consequences, we run experiments with pricing algorithms powered by Artificial Intelligence in controlled environments (computer simulations).In particular, we study the interaction among a number of Q-learning algorithms in the context of a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. We show that the algorithms consistently learn to charge supra-competitive prices, without communicating with each other. The high prices are sustained by classical collusive strategies with a finite punishment phase followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.},
	language = {en},
	number = {ID 3304991},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
	month = apr,
	year = {2019},
	doi = {10.2139/ssrn.3304991},
	keywords = {Artificial Intelligence, Collusion, Pricing-Algorithms, Q-Learning., Reinforcement Learning},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\BCZC26X8\\papers.html:text/html;calvano_et_al2020_supplementary.pdf:C\:\\Users\\psymo\\Zotero\\storage\\E3CWZ5ZJ\\calvano_et_al2020_supplementary.pdf:application/pdf;calvano_et_al2019_20.pdf:C\:\\Users\\psymo\\Zotero\\storage\\YW4I8JRH\\calvano_et_al2019_20.pdf:application/pdf;summary.html:C\:\\Users\\psymo\\Zotero\\storage\\8LA7B44I\\summary.html:text/html},
}

@article{tesauro_pricing_2002,
	title = {Pricing in {Agent} {Economies} {Using} {Multi}-{Agent} {Q}-{Learning}},
	volume = {5},
	issn = {1573-7454},
	url = {https://doi.org/10.1023/A:1015504423309},
	doi = {10.1023/A:1015504423309},
	abstract = {This paper investigates how adaptive software agents may utilize reinforcement learning algorithms such as Q-learning to make economic decisions such as setting prices in a competitive marketplace. For a single adaptive agent facing fixed-strategy opponents, ordinary Q-learning is guaranteed to find the optimal policy. However, for a population of agents each trying to adapt in the presence of other adaptive agents, the problem becomes non-stationary and history dependent, and it is not known whether any global convergence will be obtained, and if so, whether such solutions will be optimal. In this paper, we study simultaneous Q-learning by two competing seller agents in three moderately realistic economic models. This is the simplest case in which interesting multi-agent phenomena can occur, and the state space is small enough so that lookup tables can be used to represent the Q-functions. We find that, despite the lack of theoretical guarantees, simultaneous convergence to self-consistent optimal solutions is obtained in each model, at least for small values of the discount parameter. In some cases, exact or approximate convergence is also found even at large discount parameters. We show how the Q-derived policies increase profitability and damp out or eliminate cyclic price “wars” compared to simpler policies based on zero lookahead or short-term lookahead. In one of the models (the “Shopbot” model) where the sellers' profit functions are symmetric, we find that Q-learning can produce either symmetric or broken-symmetry policies, depending on the discount parameter and on initial conditions.},
	language = {en},
	number = {3},
	urldate = {2021-01-25},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Tesauro, Gerald and Kephart, Jeffrey O.},
	month = sep,
	year = {2002},
	pages = {289--304},
	annote = {Summary
 
economic model



infinitively repeated
sequential price competition (Maskin Tirole 1988???)
three benchmarks

vertical product differentiation
horizontal product competition
no differentiation





Algorithm: Dynamic Programming
Results

throughout specifications

Algorithms generally learn to increase prices and profits compared to myopic agents (with perfect information)


},
	file = {Springer Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\FG8HVHGR\\Tesauro and Kephart - 2002 - Pricing in Agent Economies Using Multi-Agent Q-Lea.pdf:application/pdf},
}

@article{waltman_q-learning_2008,
	title = {Q-learning agents in a {Cournot} oligopoly model},
	volume = {32},
	issn = {0165-1889},
	url = {https://econpapers.repec.org/article/eeedyncon/v_3a32_3ay_3a2008_3ai_3a10_3ap_3a3275-3293.htm},
	number = {10},
	urldate = {2021-01-25},
	journal = {Journal of Economic Dynamics and Control},
	author = {Waltman, Ludo and Kaymak, Uzay},
	year = {2008},
	note = {Publisher: Elsevier},
	pages = {3275--3293},
	annote = {Summary
focus on collusive outcomes (not strategies)
economic model:



infinitely repeated Cournot (quantity) competition
no product differentiation



Algorithm: Q-learning



time-declining exploration rate
Boltzman exploration strategy



Results



quantities and profits between One-Shot Nash and competition
surprisingly supra-competitive outcomes also for memoryless agents (Q-learning without states, just actions)


},
	file = {RePEc Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\PLR46KCF\\v_3a32_3ay_3a2008_3ai_3a10_3ap_3a3275-3293.html:text/html;Waltman and Kaymak - 2008 - Q-learning agents in a Cournot oligopoly model.pdf:C\:\\Users\\psymo\\Zotero\\storage\\9TVZX8S5\\Waltman and Kaymak - 2008 - Q-learning agents in a Cournot oligopoly model.pdf:application/pdf},
}

@article{kimbrough_learning_2009,
	title = {Learning to {Collude} {Tacitly} on {Production} {Levels} by {Oligopolistic} {Agents}},
	volume = {33},
	doi = {10.1007/s10614-008-9150-6},
	abstract = {Classical oligopoly theory has strong analytical foundations but is weak in capturing the operating environment of oligopolists
and the available knowledge they have for making decisions, areas in which the management literature is relevant. We use agent-based
models to simulate the impact on firm profitability of policies that oligopolists can pursue when setting production levels.
We develop an approach to analyzing simulation results that makes use of nonparametric statistical tests, taking advantage
of the large amounts of data generated by simulations, and avoiding the assumption of normality that does not necessarily
hold. Our results show that in a quantity game, a simple exploration rule, which we call Probe and Adjust, can find either the Cournot equilibrium or the monopoly solution depending on the measure of success chosen by the firms.
These results shed light on how tacit collusion can develop within an oligopoly.},
	journal = {Computational Economics},
	author = {Kimbrough, Steven and Murphy, Frederic},
	month = feb,
	year = {2009},
	pages = {47--78},
	annote = {Summary
focus on collusive outcomes (not strategies)
economic model:



infinitely repeated Cournot (quantity) competition
no product differentiation



Algorithm: Probe and adjust



inspired my management literature

models human decision makers rather than algorithms


allows for continuous action space



Results

depend on programmed reward:

firm profits: (One Shot) Cournot equilibrium
supra-competitive outcomes require that rewards include industry profitsin some form


},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\REP7QJL4\\Kimbrough and Murphy - 2009 - Learning to Collude Tacitly on Production Levels b.pdf:application/pdf},
}

@article{siallagan_aspiration-based_2013,
	title = {Aspiration-{Based} {Learning} in a {Cournot} {Duopoly} {Model}},
	volume = {10},
	issn = {2188-2096},
	url = {https://doi.org/10.14441/eier.A2013015},
	doi = {10.14441/eier.A2013015},
	abstract = {This paper explores the implication of aspiration-based learning in a simple Cournot duopoly model. When the firms know the average industry-wide profit and perceive it as aspiration level, then the market leads to collusive outcome or collusive equilibrium. In this sense, all firms have the same reference point, i.e., the average industry-wide profit as their aspiration level. However, the firms may have their own aspiration level (e.g., a goal of profit) and will choose their strategy accordingly. Therefore, the firms will try to reach their own aspiration level. This aspiration level is not static and the firms will adjust their aspiration level. In this research we consider a market that consists of several firms with their own aspiration level. We propose an aspiration-based learning shaped by an information searching mechanism to examine the behavior of the firms in the market. A firm updates its aspiration level by searching the information of the other firms’ aspiration level and then compares this information with its current aspiration level. Based on its aspiration level, the firm will choose the best strategy through learning. Simulation results show that the learning model and the information searching mechanism lead the market to competitive outcome, i.e., Nash equilibrium, if the firms have many strategies even if their initial aspiration level is low. However, if the firms have fewer strategies and start with high initial aspiration level, then collusive behavior will occur.},
	language = {en},
	number = {2},
	urldate = {2021-01-25},
	journal = {Evolutionary and Institutional Economics Review},
	author = {Siallagan, Manahan and Deguchi, Hiroshi and Ichikawa, Manabu},
	month = dec,
	year = {2013},
	pages = {295--314},
	annote = {Summary
focus on collusive outcomes (not strategies)
economic model:



infinitely repeated Cournot (quantity) competition

each episode, all players face each other 1v1 in a duopoly game





aspiration-based learning



discrete action space
models human decision makers rather than algorithms
aspiration level is adjusted by

signal of other firms' aspiration levels
differences between aspiration levels and realized profits





Results:



convergence to collusion only attainable with action space of 3


},
	file = {Springer Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\7F4RRA5Z\\Siallagan et al. - 2013 - Aspiration-Based Learning in a Cournot Duopoly Mod.pdf:application/pdf},
}

@techreport{klein_assessing_2018,
	type = {Working {Paper}},
	title = {Assessing {Autonomous} {Algorithmic} {Collusion}: {Q}-{Learning} {Under} {Short}-{Run} {Price} {Commitments}},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	shorttitle = {Assessing {Autonomous} {Algorithmic} {Collusion}},
	url = {https://www.econstor.eu/handle/10419/185575},
	abstract = {A novel debate within competition policy and regulation circles is whether autonomous machine learning algorithms may learn to collude on prices. We show that when firms face short-run price commitments, independent Q-learning (a simple but well-established self-learning algorithm) learns to profitably coordinate on either a fixed price or on asymmetric price cycles -- although convergence to rational and Pareto-optimal collusive behavior is not guaranteed. The general framework used can guide future research into the capacity of more advanced algorithms to collude, also in environments that are less stylized or more case-specific.},
	language = {eng},
	number = {TI 2018-056/VII},
	urldate = {2021-01-25},
	institution = {Tinbergen Institute Discussion Paper},
	author = {Klein, Timo},
	year = {2018},
	annote = {Summary
essentially as in Klein (2019)},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\PGNMH26J\\Klein - 2018 - Assessing Autonomous Algorithmic Collusion Q-Lear.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\6XRFYLTP\\185575.html:text/html},
}

@techreport{klein_autonomous_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Autonomous {Algorithmic} {Collusion}: {Q}-{Learning} {Under} {Sequential} {Pricing}},
	shorttitle = {Autonomous {Algorithmic} {Collusion}},
	url = {https://papers.ssrn.com/abstract=3195812},
	abstract = {Prices are increasingly set by algorithms. One concern is that intelligent algorithms may learn to collude on higher prices even in absence of the kind of communication or agreement necessary to establish an antitrust infringement. However, exactly how this may happen is an open question. I show in a simulated environment of sequential competition that competing reinforcement learning algorithms can indeed learn to converge to collusive equilibria. When the set of discrete prices increases, the algorithm considered increasingly converges to supra-competitive asymmetric cycles. I show that results are robust to various extensions and discuss practical limitations and policy implications.},
	language = {en},
	number = {ID 3195812},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Klein, Timo},
	month = jul,
	year = {2019},
	doi = {10.2139/ssrn.3195812},
	keywords = {algorithmic collusion, artificial intelligence, machine learning, pricing algorithms, Q-learning, reinforcement learning, sequential pricing},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\NLKCWL3C\\papers.html:text/html;Klein - 2019 - Autonomous Algorithmic Collusion Q-Learning Under.pdf:C\:\\Users\\psymo\\Zotero\\storage\\4CVWNUVC\\Klein - 2019 - Autonomous Algorithmic Collusion Q-Learning Under.pdf:application/pdf;summary.html:C\:\\Users\\psymo\\Zotero\\storage\\H2NTHC5F\\summary.html:text/html},
}

@article{noel_edgeworth_2008,
	title = {Edgeworth {Price} {Cycles} and {Focal} {Prices}: {Computational} {Dynamic} {Markov} {Equilibria}},
	volume = {17},
	copyright = {© 2008, The Author(s) Journal Compilation © 2008 Blackwell Publishing},
	issn = {1530-9134},
	shorttitle = {Edgeworth {Price} {Cycles} and {Focal} {Prices}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1530-9134.2008.00181.x},
	doi = {https://doi.org/10.1111/j.1530-9134.2008.00181.x},
	abstract = {Motivated by the apparent discovery of Edgeworth Cycles in many retail gasoline markets, this article extends the theory of Edgeworth Cycles along several key dimensions, including models of fluctuating marginal costs, differentiation, capacity constraints and triopoly. A computational approach to search for Markov perfect equilibria is taken. Edgeworth Cycles are found in equilibrium in many situations, and the shape of the cycles are found to carry information about underlying competitive intensity. Cycles in triopoly exhibit interesting coordination problems such as delayed starts and false starts.},
	language = {en},
	number = {2},
	urldate = {2021-01-25},
	journal = {Journal of Economics \& Management Strategy},
	author = {Noel, Michael D.},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1530-9134.2008.00181.x},
	pages = {345--377},
	annote = {Summary
economic model



infinitively repeated
sequential price competition (Maskin Tirole (1988))
extensions:

differentiated goods
fluctuating costs
2-3 firms





Algorithm: Dynamic Programming



algorithms know probabilities of next state?
algorithms evaluate best responses to make a choice?



Results



focal points and edgeworth price cycles
delayed and false starts with 3 players (cycles are a more difficult to maintain)


},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\KMSPC8H4\\j.1530-9134.2008.00181.html:text/html;Noel - 2008 - Edgeworth Price Cycles and Focal Prices Computati.pdf:C\:\\Users\\psymo\\Zotero\\storage\\95BZM5H8\\Noel - 2008 - Edgeworth Price Cycles and Focal Prices Computati.pdf:application/pdf},
}

@techreport{abada_artificial_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Artificial {Intelligence}: {Can} {Seemingly} {Collusive} {Outcomes} {Be} {Avoided}?},
	shorttitle = {Artificial {Intelligence}},
	url = {https://papers.ssrn.com/abstract=3559308},
	abstract = {Strategic decisions are increasingly delegated to algorithms. We extend the results of Waltman and Kaymak [2008] and Calvano et al. [2020b] to the context of dynamic optimization with imperfect monitoring by analyzing a setting where a limited number of agents use simple and independent machine-learning algorithms to buy and sell a storable good on behalf of a large number of consumers. No specific instruction is given to them, only that their objective is to maximize profits based solely on past market prices and payoffs. With an original application to battery operations, we observe that the algorithms learn quickly to exert market power at seemingly collusive levels, despite the absence of any formal communication between them. Contrary to the findings reported in the existing literature, we show that seeming collusion may originate in imperfect exploration, rather than excessive algorithmic sophistication. We then show that a regulator may succeed in disciplining the market to produce socially desirable outcomes by enforcing decentralized learning or with adequate intervention during the learning process.},
	language = {en},
	number = {ID 3559308},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Abada, Ibrahim and Lambin, Xavier},
	month = feb,
	year = {2020},
	doi = {10.2139/ssrn.3559308},
	keywords = {algorithmic decision-making, batteries, decentralized power systems, delegated decisions, Machine learning, multi-agent reinforcement learning, tacit collusion, virtual power plants},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\J3JEJ85Y\\papers.html:text/html;Abada and Lambin - 2020 - Artificial Intelligence Can Seemingly Collusive O.pdf:C\:\\Users\\psymo\\Zotero\\storage\\PP5M5RZE\\Abada and Lambin - 2020 - Artificial Intelligence Can Seemingly Collusive O.pdf:application/pdf},
}

@techreport{johnson_platform_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Platform {Design} {When} {Sellers} {Use} {Pricing} {Algorithms}},
	url = {https://papers.ssrn.com/abstract=3691621},
	abstract = {Using both economic theory and  Artificial Intelligence (AI) pricing algorithms,  we investigate the ability of a platform to design its marketplace to promote competition, improve consumer surplus, and even raise its own profits. We allow sellers to use Q-learning algorithms (a common reinforcement-learning technique from the computer-science literature) to devise pricing strategies in a setting with repeated interactions, and consider the effect of  platform rules that reward firms that cut prices with additional exposure to consumers. Overall, the evidence from our experiments suggests that platform design decisions can meaningfully benefit consumers even when algorithmic collusion might otherwise emerge but that achieving these gains may require more than the simplest steering policies when algorithms value the future highly. We also find that policies that raise consumer surplus can raise the profits of the platform, depending on the platform's revenue model. Finally, we  document several learning challenges faced by the algorithms.},
	language = {en},
	number = {ID 3691621},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Johnson, Justin and Rhodes, Andrew and Wildenbeest, Matthijs R.},
	month = dec,
	year = {2020},
	doi = {10.2139/ssrn.3691621},
	keywords = {artificial intelligence, Algorithms, collusion, platform design},
	annote = {Summary
focus on platform design to prevent algorithmic collusion
economic model



infinitively repeated
price competition
horizontal differentiation
platform only displays subset of suppliers (with low prices) to some consumers



Algorithm: Q-Learning



time declining exploration rate, i.e.

initially: mostly exploration
over time: mostly exploitation




},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\6Q3S5953\\Johnson et al. - 2020 - Platform Design When Sellers Use Pricing Algorithm.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\WEE8BXUU\\papers.html:text/html},
}

@article{xie_studies_2004,
	title = {Studies on horizontal competition among homogenous retailers through agent-based simulation},
	volume = {13},
	issn = {1861-9576},
	url = {https://doi.org/10.1007/s11518-006-0178-7},
	doi = {10.1007/s11518-006-0178-7},
	abstract = {This paper adopts agent-based simulation to study the horizontal competition among homogenous price-setting retailers in a one-to-many supply chain (a supply chain consists of one supplier and multiple retailers). We model the supplier and retailers as agents, and design their behavioral rules respectively. The results show that although the agents learn individually based on their own experiences, the system converges asymptotically to near Nash equilibrium steady states. When analyzing the results, we first discuss the properties of these steady states. Then based on these properties, we analyze the effects of the retailers’ horizontal competition on the retail prices, retailers’ profits and supplier’s revenue.},
	language = {en},
	number = {4},
	urldate = {2021-01-25},
	journal = {Journal of Systems Science and Systems Engineering},
	author = {Xie, Ming and Chen, Jian},
	month = dec,
	year = {2004},
	pages = {490--505},
	file = {Springer Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\NXPLZBCP\\Xie and Chen - 2004 - Studies on horizontal competition among homogenous.pdf:application/pdf},
}

@article{dogan_reinforcement_2015,
	title = {A reinforcement learning approach to competitive ordering and pricing problem},
	volume = {32},
	copyright = {© 2013 Wiley Publishing Ltd},
	issn = {1468-0394},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12054},
	doi = {https://doi.org/10.1111/exsy.12054},
	abstract = {This study analyses simultaneous ordering and pricing decisions for retailers working in a multi-retailer competitive environment for an infinite horizon. Retailers compete for the same market where the market demand is uncertain. The customer selects the winning agent (retailer) in each term on the basis of random utility maximization, which depends primarily on retailer price and random error. The complexity of the problem is increased by competitiveness, necessity for simultaneous decisions and uncertainty in the nature of increases, and is not conducive to examination using standard analytical methods. Therefore, we model the problem using reinforcement learning (RL), which is founded on stochastic dynamic programming and agent-based simulations. We analyse the effects of competitiveness and performance of RL on three different scenarios: a monopolistic case where one retailer employing a RL agent maximizes its profit, a duopolistic case where one retailer employs RL and another utilizes adaptive pricing and ordering policies, and a duopolistic case where both retailers employ RL.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Expert Systems},
	author = {Dogan, Ibrahim and Güner, Ali R.},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12054},
	keywords = {reinforcement learning, agent-based simulation, pricing, supply chain},
	pages = {39--48},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\7R32FEUB\\exsy.html:text/html},
}

@misc{noauthor_organization_nodate,
	title = {Organization, learning and cooperation},
	url = {https://ideas.repec.org/a/eee/jeborg/v70y2009i1-2p39-53.html},
	urldate = {2021-01-25},
	file = {Organization, learning and cooperation:C\:\\Users\\psymo\\Zotero\\storage\\WF56XWSP\\v70y2009i1-2p39-53.html:text/html},
}

@article{barr_modeling_2005,
	title = {Modeling the {Firm} as an {Artificial} {Neural} {Network}},
	abstract = {The purpose of this chapter is two-fold: (1) to make the case that a standard backward propagation artificial neural network (ANN) can be used as a general model of the information processing activities of the firm, and (2) to present a synthesis of Barr and Saraceno (BS) (2002, 2004, 2005), who offer various models of the firm as an artificial neural network.},
	author = {Barr, Jason and Saraceno, Francesco},
	month = nov,
	year = {2005},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\Y9KABZF2\\Barr and Saraceno - 2005 - Modeling the Firm as an Artificial Neural Network.pdf:application/pdf},
}

@article{barr_cournot_2005,
	series = {Computing in economics and finance},
	title = {Cournot competition, organization and learning},
	volume = {29},
	issn = {0165-1889},
	url = {http://www.sciencedirect.com/science/article/pii/S0165188904000156},
	doi = {10.1016/j.jedc.2003.07.003},
	abstract = {We model firms’ output decisions in a repeated duopoly framework, focusing on three interrelated issues: (1) the role of learning in the adjustment process toward equilibrium, (2) the role of organizational structure in the firm's decision making, and (3) the role of changing environmental conditions on learning and output decisions. We characterize the firm as a type of artificial neural network, which must estimate its optimal output decision based on signals it receives from the economic environment (which influences the demand function). Via simulation analysis we show: (1) how organizations learn to estimate the optimal output over time as a function of the environmental dynamics, (2) which networks are optimal for each level of environmental complexity, and (3) the equilibrium industry structure.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Journal of Economic Dynamics and Control},
	author = {Barr, Jason and Saraceno, Francesco},
	month = jan,
	year = {2005},
	keywords = {Cournot competition, Firm learning, Neural networks},
	pages = {277--295},
	annote = {Summary

successor of Barr/Saracenco 2002

economic model



Cournot game
2 firms
no product differentiation



Algorithm: artificial neural network



supposed to model organizational decision process rather than deployed algorithm
focuses on learning the economic environment
state only comprises contemporaneous variables (not memory of past prices --{\textgreater} collusion not attainable/sustainable)
opponent's price not part of inputs but enters into re-evaluation of network's performance by backward propagation reinforcement learning algorithm



Result



artificial networks learn to play (close to) one-shot equilibrium settings throughout specifications


},
	file = {ScienceDirect Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\MITND4XP\\S0165188904000156.html:text/html;Barr and Saraceno - 2005 - Cournot competition, organization and learning.pdf:C\:\\Users\\psymo\\Zotero\\storage\\SF5QN4M8\\Barr and Saraceno - 2005 - Cournot competition, organization and learning.pdf:application/pdf},
}

@inproceedings{sandholm_multiagent_1996,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On multiagent {Q}-learning in a semi-competitive domain},
	isbn = {978-3-540-49726-4},
	doi = {10.1007/3-540-60923-7_28},
	abstract = {Q-learning is a recent reinforcement learning (RL) algorithm that does not need a model of its environment and can be used online. Therefore it is well-suited for use in repeated games against an unknown opponent. Most RL research has been confined to single agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zerosum games). This paper is an empirical study of reinforcement learning in the iterated prisoner's dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-for-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-for-Tat. Playing against another learner was more difficult because the adaptation of the other learner creates a nonstationary environment in ways that are detailed in the paper. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks (RNNs) that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games.},
	language = {en},
	booktitle = {Adaption and {Learning} in {Multi}-{Agent} {Systems}},
	publisher = {Springer},
	author = {Sandholm, Tuomas W. and Crites, Robert H.},
	editor = {Weiß, Gerhard and Sen, Sandip},
	year = {1996},
	keywords = {Reinforcement Learning, Annealing Schedule, Forward Pass, Lookup Table, Recurrent Neural Network},
	pages = {191--205},
	file = {Springer Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\NQIICD9P\\Sandholm and Crites - 1996 - On multiagent Q-learning in a semi-competitive dom.pdf:application/pdf},
}

@article{sandholm_multiagent_1996-1,
	title = {Multiagent reinforcement learning in the {Iterated} {Prisoner}'s {Dilemma}},
	volume = {37},
	issn = {0303-2647},
	url = {http://www.sciencedirect.com/science/article/pii/0303264795015515},
	doi = {10.1016/0303-2647(95)01551-5},
	abstract = {Reinforcement learning (RL) is based on the idea that the tendency to produce an action should be strengthened (reinforced) if it produces favorable results, and weakened if it produces unfavorable results. Q-learning is a recent RL algorithm that does not need a model of its environment and can be used on-line. Therefore, it is well suited for use in repeated games against an unknown opponent. Most RL research has been confined to single-agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the Iterated Prisoner's Dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-For-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-For-Tat. Playing against another learner was more difficult because the adaptation of the other learner created a non-stationary environment, and because the other learner was not endowed with any a priori knowledge about the IPD game such as a policy designed to encourage cooperation. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Biosystems},
	author = {Sandholm, Tuomas W. and Crites, Robert H.},
	month = jan,
	year = {1996},
	keywords = {Machine learning, Exploration, Multiagent learning, Prisoner's Dilemma, Recurrent neural network, Reinforcement learning},
	pages = {147--166},
	file = {ScienceDirect Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\TBYLIHXV\\0303264795015515.html:text/html},
}

@misc{noauthor_algorithms_nodate,
	title = {Algorithms and collusion: {Competition} policy in the digital age - {OECD}},
	shorttitle = {Algorithms and collusion},
	url = {http://www.oecd.org/competition/algorithms-collusion-competition-policy-in-the-digital-age.htm},
	abstract = {Research and policy advice on competition including monopolisation, cartels, mergers, liberalisation, intervention, competition enforcement and regulatory reform., The combination of big data with technologically advanced tools is changing the competitive landscape in many markets and sectors. While this is producing benefits and efficiencies, it is also raising concerns of possible anti-competitive behaviour. This paper looks at whether algorithms can make tacit collusion easier and discusses some of the challenges they present for both competition law enforcement and market regulation.},
	urldate = {2021-01-25},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\978K757K\\algorithms-collusion-competition-policy-in-the-digital-age.html:text/html},
}

@misc{oecd_algorithms_nodate,
	title = {Algorithms and collusion: {Competition} policy in the digital age},
	shorttitle = {Algorithms and collusion},
	url = {http://www.oecd.org/competition/algorithms-collusion-competition-policy-in-the-digital-age.htm},
	abstract = {Research and policy advice on competition including monopolisation, cartels, mergers, liberalisation, intervention, competition enforcement and regulatory reform., The combination of big data with technologically advanced tools is changing the competitive landscape in many markets and sectors. While this is producing benefits and efficiencies, it is also raising concerns of possible anti-competitive behaviour. This paper looks at whether algorithms can make tacit collusion easier and discusses some of the challenges they present for both competition law enforcement and market regulation.},
	urldate = {2021-01-25},
	author = {OECD},
	annote = {Summary - EU response
1.Background

autonomous, self-learning algorithms' decisions hard to comprehend for humans
personalization due to algorithms may be dangerous with respect to information (filter bubble)
personalized pricing
pricing algorithms (central to response)

2. Algorithmic Pricing
2 functions:

monitoring of competitor prices
pricing

3. Algorithms in RPM

enhance monitoring by upstream manufacturer

e.g. detection of deviation from fixed price/recommendations



 
4. Algorithms in horizontal context
4.1. nice delineation of agreements, concerted practices and unilateral intelligent adaption/tacit collusion
4.4 potential of algorithms themselves engaging in explicit collusion
4.5 tacit collusion through algorithms

potential solution: consumer algorithms (e.g. price comparison websites, automatic purchase delay...)

can't be relied upon solely


algorithms 'decoding' each other may be interpreted as communication

probably not in scope of TFEU 101


potentially reconsider legality of tacit collusion

5. Conclusions
firms cannot escape liability of their algorithms behavior (similar to an employee or consultant)},
	annote = {Summary - Original Statement
Machine Learning: "machine learning gives computers the ability to learn without being explicitly programmed" (Samuel 1959)
Deep Learning: Subfield of machine Learning
in addition to traditional tools of competition authorities (ch. 5.4), the OECD consider two "novel" ideas to tackle emerging threat due to algorithmic pricing:

consider algorithmic collusion as unfair compeition (which is illegal in some jurisdictions) (ch. 5.2)
liability of individuals responsible/using/benefiting from algorithmic collusion (ch. 5.3)
},
	annote = {Summary Ezrachi \& Stucke Reply
1: Potential probems of algorithmic tacit collusion, hub-and spoke and behavioral discirmination
1.1. tacit collusion likely in markets with

concentrated markets
homogeneous products
monitoring of prices feasible (e.g. online pricing)

software may detect deviation


timely deterrent mechanism
low buyer power
high market entry barriers

algorithms are better at tacit collusion because they don't exhibit human biases
algorithms may establish tacit collusion in markets where conscious parallelism wasn't realistic before [14]
1.2 Case Studies petrol station:

price transparency in Chile, Germany \& Australia facilitated colllusion over time
algorithms may tap that potential much faster

 
1.3 Hub and spoke (in the context of algorithms):

competitors rely on same third party-software vendor
same/similar algorithm
same/similar input data

1.4. individual prices

may destabilize (tacit) collusive outcome
may enable first degree price discrimination

1.5 Hybrid Collusion AND discrimination

Use Case: Vegas Casinos
collude on list price for "low-value" and loyal customers
behaviorally discriminate "high-value" customers

 
2: Current Enforcement Toolbox
2.1. Liability:

option to deem algorithmic collusion as unfair practice/market manipulation
declare algorithms as excessive transparency (possibly with anticompetitive intent)
market investigations with potential remedies

2.2. Detection:

difficult from market observation due to a lack of counterfactual
auditing algorithms

e.g. deploy algorithm in sandbox
devise guidelines for how algorithms should be programmed
to some extent futile in case of deep reinforcement learning neural nets



2.3 AI Law \& Policy

humans liable for algorithms?

 
3: Counter-Measures
Authors promote incubator sandbox to observe the effects of e.g. the below suggestions

make interactions and price changes less frequent
reduce transparency artificially
stricter merger control to inhibit/obstruct tacit collusion
encourage deviation through

facilitating entry via lower market entry barriers
inciting maverick behavior


pooling buyers to increase their leverage
state/consumer-protection-sponsored algorithms
mediate prices (like Uber)
},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\H2FZUZRG\\algorithms-collusion-competition-policy-in-the-digital-age.html:text/html;OECD - Algorithms and collusion Competition policy in th.pdf:C\:\\Users\\psymo\\Zotero\\storage\\Z2DZ9TPI\\OECD - Algorithms and collusion Competition policy in th.pdf:application/pdf;OECD_ezrachi_stucke_2017.pdf:C\:\\Users\\psymo\\Zotero\\storage\\K2CITMBB\\OECD_ezrachi_stucke_2017.pdf:application/pdf;OECD_response_from_EU.pdf:C\:\\Users\\psymo\\Zotero\\storage\\CNG27SS9\\OECD_response_from_EU.pdf:application/pdf},
}

@misc{bundeskartellamt_bundeskartellamt_nodate,
	title = {Bundeskartellamt - {Homepage} - {Working} {Paper} - {Algorithms} and {Competition}},
	url = {https://www.bundeskartellamt.de/SharedDocs/Publikation/EN/Berichte/Algorithms_and_Competition_Working-Paper.html;jsessionid=839547DE0AC4D5F41DECA09740F7A4E9.2_cid362?nn=3591568},
	urldate = {2021-01-25},
	author = {Bundeskartellamt},
	file = {Bundeskartellamt - Homepage - Working Paper - Algorithms and Competition:C\:\\Users\\psymo\\Zotero\\storage\\BRRVQNGX\\Algorithms_and_Competition_Working-Paper.html\;jsessionid=839547DE0AC4D5F41DECA09740F7A4E9.html:text/html;Bundeskartellamt - Bundeskartellamt - Homepage - Working Paper - Algo.pdf:C\:\\Users\\psymo\\Zotero\\storage\\QH9733VQ\\Bundeskartellamt - Bundeskartellamt - Homepage - Working Paper - Algo.pdf:application/pdf;summary.html:C\:\\Users\\psymo\\Zotero\\storage\\9ITY7MKR\\summary.html:text/html},
}

@article{nitsche_matchmakers_nodate,
	title = {Matchmakers in the {Digital} {Economy}, {Sessions} 19 \& 20 \& 2},
	language = {en},
	author = {Nitsche, Rainer},
	pages = {94},
	file = {Rainer_Nitsche_slides.pdf:C\:\\Users\\psymo\\OneDrive\\Studium\\a_projects\\ai_collusion\\literature\\high_level\\Rainer_Nitsche_slides.pdf:application/pdf},
}

@article{reisinger_discussion_nodate,
	title = {Discussion on   ‘‘{Digital} {Pricing} and {Algorithms}’’ by {Mike} {Walker}   {Competition} and {Markets} {Authority}},
	language = {en},
	author = {Reisinger, Markus},
	pages = {11},
	file = {markus-reisinger_discussion-on-digital-pricing-algorithms-1.pdf:C\:\\Users\\psymo\\OneDrive\\Studium\\a_projects\\ai_collusion\\literature\\high_level\\markus-reisinger_discussion-on-digital-pricing-algorithms-1.pdf:application/pdf},
}

@article{cma_pricing_nodate,
	title = {Pricing algorithms},
	language = {en},
	author = {CMA},
	pages = {63},
	file = {cma_2018.pdf:C\:\\Users\\psymo\\OneDrive\\Studium\\a_projects\\ai_collusion\\literature\\high_level\\cma_2018.pdf:application/pdf},
}

@article{cma_pricing_nodate-1,
	title = {Pricing algorithms},
	language = {en},
	author = {CMA},
	pages = {63},
	file = {Algorithms_econ_report CMA 2018.pdf:C\:\\Users\\psymo\\OneDrive\\Studium\\a_projects\\ai_collusion\\literature\\high_level\\Algorithms_econ_report CMA 2018.pdf:application/pdf},
}

@misc{eu_antitrust_nodate,
	type = {Text},
	title = {Antitrust: {Final} report on e-commerce sector inquiry},
	shorttitle = {Antitrust},
	url = {https://ec.europa.eu/commission/presscorner/detail/en/IP_17_1261},
	abstract = {Antitrust: Commission publishes final report on e-commerce sector inquiry},
	language = {en},
	urldate = {2021-01-25},
	journal = {European Commission - European Commission},
	author = {EU},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\AHUHKKPT\\IP_17_1261.html:text/html;EU - Antitrust Final report on e-commerce sector inqui.pdf:C\:\\Users\\psymo\\Zotero\\storage\\KQ3S77HE\\EU - Antitrust Final report on e-commerce sector inqui.pdf:application/pdf},
}

@misc{ftc_ftc_nodate,
	title = {{FTC} {Hearing} \#7: {The} {Competition} and {Consumer} {Protection} {Issues} of {Algorithms}, {Artificial} {Intelligence}, and {Predictive} {Analytics} {\textbar} {Federal} {Trade} {Commission}},
	url = {https://www.ftc.gov/news-events/events-calendar/ftc-hearing-7-competition-consumer-protection-21st-century},
	urldate = {2021-01-25},
	author = {FTC},
	file = {FTC Hearing #7\: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics | Federal Trade Commission:C\:\\Users\\psymo\\Zotero\\storage\\H3P78C83\\ftc-hearing-7-competition-consumer-protection-21st-century.html:text/html},
}

@article{den_boer_dynamic_2015,
	title = {Dynamic pricing and learning: {Historical} origins, current research, and new directions},
	volume = {20},
	issn = {1876-7354},
	shorttitle = {Dynamic pricing and learning},
	url = {http://www.sciencedirect.com/science/article/pii/S1876735415000021},
	doi = {10.1016/j.sorms.2015.03.001},
	abstract = {The topic of dynamic pricing and learning has received a considerable amount of attention in recent years, from different scientific communities. We survey these literature streams: we provide a brief introduction to the historical origins of quantitative research on pricing and demand estimation, point to different subfields in the area of dynamic pricing, and provide an in-depth overview of the available literature on dynamic pricing and learning. Our focus is on the operations research and management science literature, but we also discuss relevant contributions from marketing, economics, econometrics, and computer science. We discuss relations with methodologically related research areas, and identify directions for future research.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Surveys in Operations Research and Management Science},
	author = {den Boer, Arnoud V.},
	month = jun,
	year = {2015},
	pages = {1--18},
	file = {ScienceDirect Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\KK7EIH65\\S1876735415000021.html:text/html},
}

@misc{noauthor_sutton_nodate,
	title = {Sutton \& {Barto} {Book}: {Reinforcement} {Learning}: {An} {Introduction}},
	url = {http://incompleteideas.net/book/the-book.html},
	urldate = {2021-01-25},
	file = {Sutton & Barto Book\: Reinforcement Learning\: An Introduction:C\:\\Users\\psymo\\Zotero\\storage\\TJDWQQ5V\\the-book.html:text/html},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: uWV0DwAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics},
	file = {Sutton and Barto - 2018 - Reinforcement Learning, second edition An Introdu.pdf:C\:\\Users\\psymo\\Zotero\\storage\\HM4HPA84\\Sutton and Barto - 2018 - Reinforcement Learning, second edition An Introdu.pdf:application/pdf},
}

@incollection{busoniu_multi-agent_2010,
	address = {Berlin, Heidelberg},
	series = {Studies in {Computational} {Intelligence}},
	title = {Multi-agent {Reinforcement} {Learning}: {An} {Overview}},
	isbn = {978-3-642-14435-6},
	shorttitle = {Multi-agent {Reinforcement} {Learning}},
	url = {https://doi.org/10.1007/978-3-642-14435-6_7},
	abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
	language = {en},
	urldate = {2021-01-25},
	booktitle = {Innovations in {Multi}-{Agent} {Systems} and {Applications} - 1},
	publisher = {Springer},
	author = {Buşoniu, Lucian and Babuška, Robert and De Schutter, Bart},
	editor = {Srinivasan, Dipti and Jain, Lakhmi C.},
	year = {2010},
	doi = {10.1007/978-3-642-14435-6_7},
	keywords = {Reinforcement Learning, Markov Decision Process, Multiagent System, Nash Equilibrium, Reward Function},
	pages = {183--221},
}

@article{bloembergen_evolutionary_2015,
	title = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}},
	volume = {53},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10952},
	doi = {10.1613/jair.4818},
	language = {en},
	urldate = {2021-01-25},
	journal = {Journal of Artificial Intelligence Research},
	author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
	month = aug,
	year = {2015},
	pages = {659--697},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\WGJEW6AZ\\Bloembergen et al. - 2015 - Evolutionary Dynamics of Multi-Agent Learning A S.pdf:application/pdf},
}

@inproceedings{leibo_multi-agent_2017,
	address = {Richland, SC},
	series = {{AAMAS} '17},
	title = {Multi-agent {Reinforcement} {Learning} in {Sequential} {Social} {Dilemmas}},
	abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
	urldate = {2021-01-25},
	booktitle = {Proceedings of the 16th {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
	month = may,
	year = {2017},
	keywords = {agent-based social simulation, cooperation, markov games, non-cooperative games, social dilemmas},
	pages = {464--473},
	file = {Leibo et al. - 2017 - Multi-agent Reinforcement Learning in Sequential S.pdf:C\:\\Users\\psymo\\Zotero\\storage\\P3PWN2JA\\Leibo et al. - 2017 - Multi-agent Reinforcement Learning in Sequential S.pdf:application/pdf},
}

@article{crandall_cooperating_2018,
	title = {Cooperating with machines},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-02597-8},
	doi = {10.1038/s41467-017-02597-8},
	abstract = {Since Alan Turing envisioned artificial intelligence, technical progress has often been measured by the ability to defeat humans in zero-sum encounters (e.g., Chess, Poker, or Go). Less attention has been given to scenarios in which human–machine cooperation is beneficial but non-trivial, such as scenarios in which human and machine preferences are neither fully aligned nor fully in conflict. Cooperation does not require sheer computational power, but instead is facilitated by intuition, cultural norms, emotions, signals, and pre-evolved dispositions. Here, we develop an algorithm that combines a state-of-the-art reinforcement-learning algorithm with mechanisms for signaling. We show that this algorithm can cooperate with people and other algorithms at levels that rival human cooperation in a variety of two-player repeated stochastic games. These results indicate that general human–machine cooperation is achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Nature Communications},
	author = {Crandall, Jacob W. and Oudah, Mayada and Tennom and Ishowo-Oloko, Fatimah and Abdallah, Sherief and Bonnefon, Jean-François and Cebrian, Manuel and Shariff, Azim and Goodrich, Michael A. and Rahwan, Iyad},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {233},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\B263SFL9\\Crandall et al. - 2018 - Cooperating with machines.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\NH3WPPWG\\s41467-017-02597-8.html:text/html},
}

@article{crandall_towards_2014,
	title = {Towards {Minimizing} {Disappointment} in {Repeated} {Games}},
	volume = {49},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10860},
	doi = {10.1613/jair.4202},
	language = {en},
	urldate = {2021-01-25},
	journal = {Journal of Artificial Intelligence Research},
	author = {Crandall, J. W.},
	month = feb,
	year = {2014},
	pages = {111--142},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\YZM4AMI2\\Crandall - 2014 - Towards Minimizing Disappointment in Repeated Game.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\J8JK7LKZ\\10860.html:text/html},
}

@article{wang_towards_2018,
	title = {Towards {Cooperation} in {Sequential} {Prisoner}'s {Dilemmas}: a {Deep} {Multiagent} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Towards {Cooperation} in {Sequential} {Prisoner}'s {Dilemmas}},
	url = {http://arxiv.org/abs/1803.00162},
	abstract = {The Iterated Prisoner's Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner's dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.},
	urldate = {2021-01-25},
	journal = {arXiv:1803.00162 [cs]},
	author = {Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.00162},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	annote = {Comment: 13 pages, 21 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\DK6UFNF8\\Wang et al. - 2018 - Towards Cooperation in Sequential Prisoner's Dilem.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\FS2TIBDP\\1803.html:text/html},
}

@article{lerer_maintaining_2018,
	title = {Maintaining cooperation in complex social dilemmas using deep reinforcement learning},
	url = {http://arxiv.org/abs/1707.01068},
	abstract = {Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.},
	urldate = {2021-01-25},
	journal = {arXiv:1707.01068 [cs]},
	author = {Lerer, Adam and Peysakhovich, Alexander},
	month = mar,
	year = {2018},
	note = {arXiv: 1707.01068},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\2TMK7V4R\\Lerer and Peysakhovich - 2018 - Maintaining cooperation in complex social dilemmas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\A2Z9IM3B\\1707.html:text/html},
}

@techreport{romero_model_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Model} of {Adaptive} {Reinforcement} {Learning}},
	url = {https://papers.ssrn.com/abstract=3350711},
	abstract = {We develop a model of learning that extends the classic models of reinforcement learning to a continuous, multidimensional strategy space. The model takes advantage of the recent approximation methods to tackle the curse of dimensionality inherent to a traditional discretization approach. Crucially, the model endogenously partitions strategies into sets of similar strategies, and allows agents to learn over these sets which speeds up the learning process. We provide an application of our model to predict which memory-1 mixed strategies will be played in the indenitely repeated Prisoner's Dilemma game. We show that despite allowing the mixed strategies, strategies close to the pure strategies always defect, grim trigger, and tit-for-tat emerge -- a result that qualitatively matches recent strategy choice experiments with human subjects.},
	language = {en},
	number = {ID 3350711},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Romero, Julian and Rosokha, Yaroslav},
	month = mar,
	year = {2019},
	doi = {10.2139/ssrn.3350711},
	keywords = {Reinforcement Learning, Agent-based Models, Markov Strategies, Mixed Strategies, Repeated Prisoner's Dilemma, Repeated-game Strategies},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\JUMJ5L6M\\papers.html:text/html},
}

@article{li_deep_2018,
	title = {Deep {Reinforcement} {Learning}: {An} {Overview}},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1701.07274},
	abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
	urldate = {2021-01-25},
	journal = {arXiv:1701.07274 [cs]},
	author = {Li, Yuxi},
	month = nov,
	year = {2018},
	note = {arXiv: 1701.07274},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\MK3N2MPJ\\Li - 2018 - Deep Reinforcement Learning An Overview.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\VY995YP4\\1701.html:text/html},
}

@article{poker_application_artificial_nodate,
	title = {Artificial {Intelligence} {Is} {About} to {Conquer} {Poker}—{But} {Not} {Without} {Human} {Help}},
	issn = {1059-1028},
	url = {https://www.wired.com/2017/01/ai-conquer-poker-not-without-human-help/},
	abstract = {No machine has ever beaten the top players at no-limit Texas Hold 'Em. But this time it's different.},
	language = {en-us},
	urldate = {2021-01-25},
	journal = {Wired},
	author = {Poker Application},
	keywords = {artificial intelligence, business, deep learning, enterprise, web},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\TEHA9J4S\\ai-conquer-poker-not-without-human-help.html:text/html},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-01-25},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\J232N2QE\\nature14236.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2021-01-25},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	pages = {484--489},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\9TICCKPB\\nature16961.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-01-25},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\ZELQ4KDQ\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\9QAUJI3M\\nature24270.html:text/html},
}

@article{silver_general_2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	copyright = {Copyright © 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/362/6419/1140},
	doi = {10.1126/science.aar6404},
	abstract = {One program to rule them all
Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.
Science, this issue p. 1140; see also pp. 1087 and 1118
The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.},
	language = {en},
	number = {6419},
	urldate = {2021-01-25},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2018},
	pmid = {30523106},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	pages = {1140--1144},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\H3HT57JP\\Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf},
}

@misc{ai_blog_learning_2017,
	title = {Learning to {Cooperate}, {Compete}, and {Communicate}},
	url = {https://openai.com/blog/learning-to-cooperate-compete-and-communicate/},
	abstract = {Multiagent environments where agents compete for resources are stepping stones on the path to AGI. Multiagent environments have two useful properties: first, there is a natural curriculum—the difficulty of the environment is determined by the skill of your competitors (and if you're competing against clones of yourself, the environment},
	language = {en},
	urldate = {2021-01-25},
	journal = {OpenAI},
	author = {AI Blog},
	month = jun,
	year = {2017},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\B7T5HBDI\\learning-to-cooperate-compete-and-communicate.html:text/html},
}

@techreport{ittoo_algorithmic_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithmic {Pricing} {Agents} and {Tacit} {Collusion}: {A} {Technological} {Perspective}},
	shorttitle = {Algorithmic {Pricing} {Agents} and {Tacit} {Collusion}},
	url = {https://papers.ssrn.com/abstract=3046405},
	abstract = {Amongst the wealth of concerns raised by Artificial Intelligence (“AI”), one is the risk that the deployment of algorithmic pricing agents on markets will increase occurrences of tacit collusion by orders of magnitude, and well beyond the oligopoly setting where such markets failures have been traditionally observed. This concern has already generated policy interest, and regulatory options are now commonly discussed at academic, commercial and official conferences. At the same time, however, we remain in lack of understanding of whether current AI technology holds the capabilities that entitle algorithmic pricing agents to autonomously enter into tacitly collusive strategies without human intervention. In this paper, we look at three plain-vanilla Reinforcement Learning (“RL”) technologies, and attempt to understand whether their introduction at scale on markets can lead to tacit collusion. While we do not deny the fact that smart pricing agents can enter into tacit collusion and that regulators may be right to be vigilant, we find that there are several technological challenges in the general realm of RL that mitigate this risk.Our paper proceeds in five steps. We first discuss the algorithmic tacit collusion conjecture (I). We then provide a non technical overview of reinforcement learning technologies (II). We then move on to discuss how naïve single agent Q-learning (III) and multi-agent Q-learning (IV) interact as market players. We close with a discussion of how technological challenges fragilize the algorithmic tacit collusion conjecture (V).},
	language = {en},
	number = {ID 3046405},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Ittoo, Ashwin and Petit, Nicolas},
	month = oct,
	year = {2017},
	doi = {10.2139/ssrn.3046405},
	keywords = {Artificial Intelligence, Collusion, Reinforcement Learning, Algorithms, Antitrust, Markets, Pricing},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\QXJE9JS7\\papers.html:text/html;Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\26VPYVQC\\Ittoo and Petit - 2017 - Algorithmic Pricing Agents and Tacit Collusion A .pdf:application/pdf},
}

@article{hernandez-leal_survey_2017,
	title = {A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}},
	shorttitle = {A {Survey} of {Learning} in {Multiagent} {Environments}},
	abstract = {The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.},
	author = {Hernandez-Leal, Pablo and Kaisers, Michael and Baarslag, Tim and Munoz de Cote, Enrique},
	month = jul,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\CUYI4DL8\\Hernandez-Leal et al. - 2017 - A Survey of Learning in Multiagent Environments D.pdf:application/pdf},
}

@techreport{abrantes-metz_can_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Can {Machine} {Learning} {Aide} in {Cartel} {Detection}?},
	url = {https://papers.ssrn.com/abstract=3291633},
	abstract = {Recent research has focused on complex antitrust issues of stemming from corporate uses of “Big Data” and “Machine Learning” pricing algorithms. For instance, Whether could the pricing algorithms of two different companies ever could be said to be colluding with each other is just one of the many important questions being raised in this context.? In this short note, we want to explore the other side of the coin, and ask whether Big Data and Machine Learning could be used in the detection (and therefore in the defense) of cartels or other collusive, anti-competitive practices, and what, if anything, would be the role of the economist in such applications.However, one wants to label the application of sophisticated pattern-matching algorithms to large data sets – “data mining,” Big Data, “artificial intelligence,” “machine learning” – it is often considered to be a field of expertise separate and distinct from traditional economics or even econometrics. We will not spend time developing a taxonomy over these different concepts (that itself being an interesting and nuanced exercise) but will simply refer to these collective practices as “machine learning,” a field (or maybe set of fields) often considered the domain of data scientists or computer scientists much more so than economists. Does this field have a home in cartel detection, and can (or should) the economist be excluded?},
	language = {en},
	number = {ID 3291633},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Abrantes-Metz, Rosa M. and Metz, Albert},
	month = jul,
	year = {2018},
	keywords = {Artificial Intelligence, Collusion, Detection, Screens},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\RHP6BSHP\\papers.html:text/html},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} with {Screens} for {Detecting} {Bid}-{Rigging} {Cartels} - {Competition} {Policy} {International}},
	url = {https://www.competitionpolicyinternational.com/machine-learning-with-screens-for-detecting-bid-rigging-cartels/},
	urldate = {2021-01-25},
	file = {Machine Learning with Screens for Detecting Bid-Rigging Cartels - Competition Policy International:C\:\\Users\\psymo\\Zotero\\storage\\MDUAZ8VD\\machine-learning-with-screens-for-detecting-bid-rigging-cartels.html:text/html},
}

@article{samek_explainable_2017,
	title = {Explainable {Artificial} {Intelligence}: {Understanding}, {Visualizing} and {Interpreting} {Deep} {Learning} {Models}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1708.08296},
	abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
	urldate = {2021-01-25},
	journal = {arXiv:1708.08296 [cs, stat]},
	author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.08296},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: 8 pages, 2 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\KFG59XEP\\Samek et al. - 2017 - Explainable Artificial Intelligence Understanding.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\KAL5ALPU\\1708.html:text/html},
}

@article{gilpin_explaining_2019,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/1806.00069},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	urldate = {2021-01-25},
	journal = {arXiv:1806.00069 [cs, stat]},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.00069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The 5th IEEE International Conference on Data Science and Advanced Analytics (DSAA 2018). [Research Track]},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\87RBPWJA\\Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\UMR2AX2N\\1806.html:text/html},
}

@book{whish_competition_2012,
	title = {Competition {Law}},
	isbn = {978-0-19-958655-4},
	abstract = {Table of treaties and conventions p. xiii Table of EU legislation p. xiv Table of statutes p. xviii Table of statutory instruments p. xxii Table of competition commission reports p. xxv Table of OFT reports, decisions and publications p. xxvii Table of cases p. xxviii List of abbreviations p. lxix 1 Competition policy and economics p. 1 1 Introduction p. 1 2 Overview of the Practices Controlled by Competition Law p. 2 3 The Theory of Competition p. 3 4 The Function of Competition Law p. 19 5 Market Definition and Market Power p. 25 2 Overview of EU and UK competition law p. 49 1 Introduction p. 49 2 EU Law p. 49 3 UK Law p. 58 4 The Relationship Between EU Competition Law and National Competition Laws p. 75 5 The Institutional Structure of EU and UK Competition Law p. 79 3 Article 101(1) p. 82 1 Introduction p. 82 2 Undertakings and Associations of Undertakings p. 83 3 Agreements, Decisions and Concerted Practices p. 99 4 The Object or Effect of Preventing, Restricting or Distorting Competition p. 115 5 The De Minimis Doctrine p. 140 6 The Effect on Trade Between Member States p. 144 7 Checklist of Agreements That Fall Outside Article 101(1) p. 149 4 Article 101(3) p. 151 1 Introduction p. 151 2 The Article 101(3) Criteria p. 155 3 Regulation 1/2003 p. 166 4 Block Exemptions p. 168 5 Article 102 p. 173 1 Introduction p. 173 2 The Commission's Guidance on Article 102 Enforcement Priorities p. 174 3 Undertakings p. 177 4 The Effect on Inter-State Trade p. 178 5 Dominant Position p. 179 6 A Substantial Part of the Internal Market p. 189 7 Small Firms and Narrow Markets p. 190 8 Abuse p. 192 9 Defences p. 210 10 The Consequences of Infringing Article 102 p. 214 6 The obligations of Member States under the EU competition rules p. 215 1 Introduction p. 215 2 Article 4(3) TEU-Duty of Sincere Cooperation p. 216 3 Article 106 TFEU-Compliance with the Treaties p. 222 4 Article 37 TFEU -State Monopolies of a Commercial Character p. 245 5 Articles 107 to 109 TFEU-State Aids p. 246 7 Articles 101 and 102: public enforcement by the European Commission and national competition authorities under Regulation 1/2003 p. 248 1 Overview of Regulation 1/2003 p. 250 2 The Commission's Enforcement Powers under Regulation 1/2003 p. 251 3 Regulation 1/2003 in Practice p. 288 4 Judicial Review p. 290 8 Articles 101 and 102: private enforcement in the courts of Member States p. 295 1 Introduction p. 295 2 Actions for an Injunction and/or Damages p. 297 3 Damages Actions in the UK Courts p. 306 4 Competition Law as a Defence p. 319 5 Arbitration p. 325 6 Proposals for Reform p. 327 9 Competition Act 1998-substantive provisions p. 330 1 Introduction p. 330 2 The Competition Act 1998-Overview p. 331 3 The Chapter I Prohibition p. 333 4 The Chapter II Prohibition p. 360 5 'Governing Principles Clause': Section 60 of the Competition Act1998 p. 369 6 The Competition Act 1998 in Practice p. 374 10 Competition Act 1998 and the cartel offence: public enforcement and procedure p. 393 1 Introduction p. 393 2 Inquiries and Investigations p. 394 3 Complaints and Super-Complaints p. 402 4 Opinions and Informal Advice p. 403 5 Enforcement p. 404 6 The Cartel Offence and Company Director Disqualification p. 424 7 Concurrency p. 437 8 Appeals p. 439 9 Article 267 References p. 449 11 Enterprise Act 2002: market studies and market investigations p. 451 1 Introduction p. 451 2 Overview of the Provisions on Market Investigation References p. 452 3 Super-Complaints p. 454 4 OFT Market Studies p. 458 5 Market Investigation References p. 466 6 Public Interest Cases p. 474 7 Enforcement p. 474 8 Supplementary Provisions p. 477 9 The Market Investigation Provisions in Practice p. 479 10 Orders and Undertakings Under the Fair Trading Act 1973 p. 486 12 The international dimension of competition law p. 487 1 Introduction p. 487 2 Extraterritoriality: Theory p. 488 3 The Extraterritorial Application of US Competition Law p. 491 4 The Extraterritorial Application of EU Competition Law p. 495 5 The Extraterritorial Application of UK Competition Law p. 501 6 Resistance to Extraterritorial Application of Competition Law p. 504 7 The Internationalisation of Competition Law p. 506 13 Horizontal agreements (1)-cartels p. 512 1 The Hardening Attitude of Competition Authorities Worldwide Towards Cartels p. 513 2 The European Commission's Approach to Cartels p. 517 3 Horizontal Price Fixing p. 522 4 Horizontal Market Sharing p. 530 5 Quotas and Other Restrictions on Production p. 533 6 Collusive Tendering p. 536 7 Agreements Relating to Terms and Conditions p. 538 8 Exchanges of Information p. 539 9 Advertising Restrictions p. 547 10 Anti-Competitive Horizontal Restraints p. 550 11 UK Law p. 552 14 Horizontal agreements (2)-oligopoly, tacit collusion and collective dominance p. 559 1 Introduction p. 559 2 The Theory of Oligopolistic Interdependence p. 560 3 Article 101 p. 567 4 Article 102 and Collective Dominance p. 571 5 UK Law p. 582 15 Horizontal agreements (3)-cooperation agreements p. 585 1 Introduction p. 585 2 Full-Function Joint Ventures p. 585 3 The Application of Article 101 to Horizontal Cooperation Agreements and the Commission's Guidelines on Horizontal Cooperation Agreements p. 586 4 Information Agreements p. 592 5 Research and Development Agreements p. 592 6 Production Agreements p. 599 7 Purchasing Agreements p. 603 8 Commercialisation Agreements p. 605 9 Standardisation Agreements p. 607 10 Other Cases of Permissible Horizontal Cooperation p. 611 11 The Application of the Chapter I Prohibition in the UK Competition Act 1998 to Horizontal Cooperation Agreements p. 615 16 Vertical agreements p. 617 1 Introduction p. 617 2 The Distribution Chain p. 618 3 Vertical Integration p. 619 4 Commercial Agents p. 621 5 Vertical Agreements: Competition Policy Considerations p. 623 6 Vertical Agreements: Article 101(1) p. 628 7 Vertical Agreements: Regulation 330/2010 p. 649 8 The Application of Article 101(3) to Agreements that do not Satisfy the Block Exemption p. 672 9 Regulation 461/2010 on Motor Vehicle Distribution p. 674 10 Sub-Contracting Agreements p. 676 11 UK Law p. 677 17 Abuse of dominance (1): non-pricing practices p. 681 1 Introduction p. 681 2 Exclusive Dealing Agreements p. 682 3 Tying p. 688 4 Refusal to Supply p. 697 5 Non-Pricing Abuses that are Harmful to the Internal Market p. 711 6 Miscellaneous Other Non-Pricing Abuses p. 712 18 Abuse of dominance (2): pricing practices p. 715 1 Introduction p. 715 2 Cost Concepts p. 716 3 Exploitative Pricing Practices p. 718 4 Rebates that have Effects Similar to Exclusive Dealing Agreements p. 728 5 Bundling p. 737 6 Predatory Pricing p. 739 7 Margin Squeezing p. 754 8 Price Discrimination p. 759 9 Pricing Practices that are Harmful to the Single Market p. 764 19 The relationship between intellectual property rights and competition law p. 767 1 Introduction p. 767 2 Licences of Intellectual Property Rights: Article 101 p. 770 3 Technology Transfer Agreements: Regulation 772/2004 p.},
	language = {en},
	publisher = {OUP Oxford},
	author = {Whish, Richard and Bailey, David},
	month = jan,
	year = {2012},
	keywords = {Law / Antitrust, Law / Business \& Financial, Law / International},
}

@inproceedings{chen_empirical_2016,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '16},
	title = {An {Empirical} {Analysis} of {Algorithmic} {Pricing} on {Amazon} {Marketplace}},
	isbn = {978-1-4503-4143-1},
	url = {https://doi.org/10.1145/2872427.2883089},
	doi = {10.1145/2872427.2883089},
	abstract = {The rise of e-commerce has unlocked practical applications for algorithmic pricing (also called dynamic pricing algorithms), where sellers set prices using computer algorithms. Travel websites and large, well known e-retailers have already adopted algorithmic pricing strategies, but the tools and techniques are now available to small-scale sellers as well. While algorithmic pricing can make merchants more competitive, it also creates new challenges. Examples have emerged of cases where competing pieces of algorithmic pricing software interacted in unexpected ways and produced unpredictable prices, as well as cases where algorithms were intentionally designed to implement price fixing. Unfortunately, the public currently lack comprehensive knowledge about the prevalence and behavior of algorithmic pricing algorithms in-the-wild. In this study, we develop a methodology for detecting algorithmic pricing, and use it empirically to analyze their prevalence and behavior on Amazon Marketplace. We gather four months of data covering all merchants selling any of 1,641 best-seller products. Using this dataset, we are able to uncover the algorithmic pricing strategies adopted by over 500 sellers. We explore the characteristics of these sellers and characterize the impact of these strategies on the dynamics of the marketplace.},
	urldate = {2021-01-25},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Chen, Le and Mislove, Alan and Wilson, Christo},
	month = apr,
	year = {2016},
	keywords = {algorithmic pricing, dynamic pricing algorithms, e-commerce},
	pages = {1339--1349},
}

@techreport{assad_algorithmic_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithmic {Pricing} and {Competition}: {Empirical} {Evidence} from the {German} {Retail} {Gasoline} {Market}},
	shorttitle = {Algorithmic {Pricing} and {Competition}},
	url = {https://papers.ssrn.com/abstract=3682021},
	abstract = {Economic theory provides ambiguous and conflicting predictions about the association between algorithmic pricing and competition. In this paper we provide the first empirical analysis of this relationship. We study Germany’s retail gasoline market where algorithmic-pricing software became widely available by mid-2017, and for which we have access to comprehensive, highfrequency price data. Because adoption dates are unknown, we identify gas stations that adopt algorithmic-pricing software by testing for structural breaks in markers associated with algorithmic pricing. We find a large number of station-level structural breaks around the suspected time of large-scale adoption. Using this information we investigate the impact of adoption on outcomes linked to competition. Because station-level adoption is endogenous, we use brand headquarter-level adoption decisions as instruments. Our IV results show that adoption increases margins by 9\%, but only in non-monopoly markets. Restricting attention to duopoly markets, we find that market-level margins do not change when only one of the two stations adopts, but increase by 28\% in markets where both do. These results suggest that AI adoption has a significant effect on competition.},
	language = {en},
	number = {ID 3682021},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Assad, Stephanie and Clark, Robert and Ershov, Daniel and Xu, Lei},
	year = {2020},
	keywords = {artificial intelligence, collusion, pricing-algorithms, retail gasoline},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\U7XNKCDV\\papers.html:text/html},
}

@techreport{calvano_algorithmic_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithmic {Pricing}: {What} {Implications} for {Competition} {Policy}?},
	shorttitle = {Algorithmic {Pricing}},
	url = {https://papers.ssrn.com/abstract=3209781},
	abstract = {Pricing decisions are increasingly in the “hands” of artificial algorithms. Scholars and competition authorities have voiced concerns that those algorithms are capable of sustaining collusive outcomes more effectively than human decision makers. If this is so, then our traditional policy tools for fighting collusion may have to be reconsidered. We discuss these issues by critically surveying the relevant law, economics and computer science literatures.},
	language = {en},
	number = {ID 3209781},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
	month = jul,
	year = {2018},
	doi = {10.2139/ssrn.3209781},
	keywords = {Collusion, Algorithmic Pricing, Artifical Intellicence, Competition Policy, Machine Learning},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\2I2BUY77\\papers.html:text/html},
}

@techreport{ezrachi_two_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Two {Artificial} {Neural} {Networks} {Meet} in an {Online} {Hub} and {Change} the {Future} ({Of} {Competition}, {Market} {Dynamics} and {Society})},
	url = {https://papers.ssrn.com/abstract=2949434},
	abstract = {In the future, one may imagine a new breed of antitrust humor. Jokes might start along the following lines: “Two Artificial Neural Network and one Nash equilibrium meet in an online (pub) hub. After a few milliseconds, a unique silent friendship is formed…” Back to the present; we are not sure how this joke might end. Nor can we estimate how funny future consumers would find it. We can, however, explain, at present, how technological advancements have changed, and will continue to change, the dynamics of competition and subsequently the distribution of wealth in society. How algorithms may be used in stealth mode to stabilize and dampen market competition while retaining the façade of a competitive environment. That tale is at the heart of this paper. We first raised algorithmic tacit collusion in 2015. Our recent book, Virtual Competition: The Promise and Perils of the Algorithm-Driven Economy, provides further context and analysis. We illustrate how online tacit collusion may emerge when products are generally homogeneous,  sellers do not benefit from brand recognition or loyalty, and markets are transparent and concentrated. Since our book elaborates on the four collusion scenarios, we begin here by outlining one model of tacit collusion and its manifestation online. Taking note of advancements in technology and emerging policies, we move the debate forward in reviewing the possible harm and means to address it. We illustrate with several case studies how the move to an online pricing environment, under certain market conditions, may harm the buyers’ welfare. We note how new technologies may undermine enforcers’ attempts to intervene - as stealth becomes a feature of future strategies. That tale, of course, is not immune from disruptive strategies. We consider the testing of counter-measures in an “algorithmic collusion incubator” to better understand what effectively destabilizes algorithmic tacit collusion. Further, we consider the effects and likelihood of secret dealings. We note how, somewhat counter-intuitively, secret deals in an online environment could reduce, at times, our welfare.},
	language = {en},
	number = {ID 2949434},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Ezrachi, Ariel and Stucke, Maurice E.},
	month = jul,
	year = {2017},
	doi = {10.2139/ssrn.2949434},
	keywords = {collusion, algorithms, behavioral discrimination, competition},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\XE3FIA3I\\Ezrachi and Stucke - 2017 - Two Artificial Neural Networks Meet in an Online H.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\GM4BLQ7U\\papers.html:text/html},
}

@misc{ezrachi_ezrachi_nodate,
	title = {Ezrachi, {A}: {Virtual} {Competition}: {The} {Promise} and {Perils} of the - {Ezrachi}, {Ariel}, {Stucke}, {Maurice} {E}. - {Amazon}.de: {Bücher}},
	url = {https://www.amazon.de/Virtual-Competition-Promise-Algorithm-Driven-Economy/dp/0674545478},
	urldate = {2021-01-25},
	author = {Ezrachi, Ariel and Stucke, Maurice E.},
	file = {Ezrachi, A\: Virtual Competition\: The Promise and Perils of the - Ezrachi, Ariel, Stucke, Maurice E. - Amazon.de\: Bücher:C\:\\Users\\psymo\\Zotero\\storage\\3S5QHUWB\\0674545478.html:text/html},
}

@techreport{ezrachi_two_2017-1,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Two {Artificial} {Neural} {Networks} {Meet} in an {Online} {Hub} and {Change} the {Future} ({Of} {Competition}, {Market} {Dynamics} and {Society})},
	url = {https://papers.ssrn.com/abstract=2949434},
	abstract = {In the future, one may imagine a new breed of antitrust humor. Jokes might start along the following lines: “Two Artificial Neural Network and one Nash equilibrium meet in an online (pub) hub. After a few milliseconds, a unique silent friendship is formed…” Back to the present; we are not sure how this joke might end. Nor can we estimate how funny future consumers would find it. We can, however, explain, at present, how technological advancements have changed, and will continue to change, the dynamics of competition and subsequently the distribution of wealth in society. How algorithms may be used in stealth mode to stabilize and dampen market competition while retaining the façade of a competitive environment. That tale is at the heart of this paper. We first raised algorithmic tacit collusion in 2015. Our recent book, Virtual Competition: The Promise and Perils of the Algorithm-Driven Economy, provides further context and analysis. We illustrate how online tacit collusion may emerge when products are generally homogeneous,  sellers do not benefit from brand recognition or loyalty, and markets are transparent and concentrated. Since our book elaborates on the four collusion scenarios, we begin here by outlining one model of tacit collusion and its manifestation online. Taking note of advancements in technology and emerging policies, we move the debate forward in reviewing the possible harm and means to address it. We illustrate with several case studies how the move to an online pricing environment, under certain market conditions, may harm the buyers’ welfare. We note how new technologies may undermine enforcers’ attempts to intervene - as stealth becomes a feature of future strategies. That tale, of course, is not immune from disruptive strategies. We consider the testing of counter-measures in an “algorithmic collusion incubator” to better understand what effectively destabilizes algorithmic tacit collusion. Further, we consider the effects and likelihood of secret dealings. We note how, somewhat counter-intuitively, secret deals in an online environment could reduce, at times, our welfare.},
	language = {en},
	number = {ID 2949434},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Ezrachi, Ariel and Stucke, Maurice E.},
	month = jul,
	year = {2017},
	doi = {10.2139/ssrn.2949434},
	keywords = {collusion, algorithms, behavioral discrimination, competition},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\QF3G87ZI\\Ezrachi and Stucke - 2017 - Two Artificial Neural Networks Meet in an Online H.pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\L98YGUWU\\papers.html:text/html},
}

@techreport{ezrachi_artificial_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Artificial {Intelligence} \& {Collusion}: {When} {Computers} {Inhibit} {Competition}},
	shorttitle = {Artificial {Intelligence} \& {Collusion}},
	url = {https://papers.ssrn.com/abstract=2591874},
	abstract = {The development of self-learning and independent computers has long captured our imagination.  The HAL 9000 computer, in the 1968 film, 2001: A Space Odyssey, for example, assured, “I am putting myself to the fullest possible use, which is all I think that any conscious entity can ever hope to do.” Machine learning raises many challenging legal and ethical questions as to the relationship between man and machine, humans’ control -- or lack of it -- over machines, and accountability for machine activities.  While these issues have long captivated our interest, few would envision the day when these developments (and the legal and ethical challenges raised by them) would become an antitrust issue.  Sophisticated computers are central to the competitiveness of present and future markets. With the accelerating development of AI, they are set to change the competitive landscape and the nature of competitive restraints. As pricing mechanisms shift to computer pricing algorithms, so too will the types of collusion. We are shifting from the world where executives expressly collude in smoke-filled hotel rooms to a world where pricing algorithms continually monitor and adjust to each other’s prices and market data. Our paper addresses these developments and considers the application of competition law to an advanced ‘computerised trade environment.’ After discussing the way in which computerised technology is changing the competitive landscape, we explore four scenarios where AI can foster anticompetitive collusion and the legal and ethical challenges each scenario raises.},
	language = {en},
	number = {ID 2591874},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Ezrachi, Ariel and Stucke, Maurice E.},
	month = apr,
	year = {2015},
	keywords = {Artificial Intelligence, Collusion, Antitrust, Cartels, Competition law, Computers},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\CZTGJAPY\\papers.html:text/html;Ezrachi and Stucke - 2015 - Artificial Intelligence & Collusion When Computer.pdf:C\:\\Users\\psymo\\Zotero\\storage\\NTCT9L2K\\Ezrachi and Stucke - 2015 - Artificial Intelligence & Collusion When Computer.pdf:application/pdf},
}

@misc{noauthor_sustainable_nodate,
	title = {Sustainable and {Unchallenged} {Algorithmic} {Tacit} {Collusion} by {Ariel} {Ezrachi}, {Maurice} {E}. {Stucke} :: {SSRN}},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3282235},
	urldate = {2021-01-25},
	file = {Sustainable and Unchallenged Algorithmic Tacit Collusion by Ariel Ezrachi, Maurice E. Stucke \:\: SSRN:C\:\\Users\\psymo\\Zotero\\storage\\UFMXKQFW\\papers.html:text/html;Sustainable and Unchallenged Algorithmic Tacit Col.pdf:C\:\\Users\\psymo\\Zotero\\storage\\4QK2HV8G\\Sustainable and Unchallenged Algorithmic Tacit Col.pdf:application/pdf},
}

@techreport{gal_algorithmic_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithmic {Consumers}},
	url = {https://papers.ssrn.com/abstract=2876201},
	abstract = {The next generation of e-commerce will be conducted by digital agents, based on algorithms that will not only make purchase recommendations, but will also predict what we want, make purchase decisions, negotiate and execute the transaction for the consumers, and even automatically form coalitions of buyers to enjoy better terms, thereby replacing human decision-making. Algorithmic consumers have the potential to change dramatically the way we conduct business, raising new conceptual and regulatory challenges.},
	language = {en},
	number = {ID 2876201},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Gal, Michal and Elkin-Koren, Niva},
	month = aug,
	year = {2016},
	keywords = {artificial intelligence, algorithms, antitrust, regulation},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\BIM7EZNM\\papers.html:text/html},
}

@techreport{gal_algorithms_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithms as {Illegal} {Agreements}},
	url = {https://papers.ssrn.com/abstract=3171977},
	abstract = {Despite the increased transparency, connectivity, and search abilities that characterize the digital marketplace, the digital revolution has not always yielded the bargain prices that many consumers expected. What is going on? Some researchers suggest that one factor may be coordination between the algorithms used by suppliers to determine trade terms. Simple coordination-facilitating algorithms are already available off the shelf, and such coordination is only likely to become more commonplace in the near future. This is not surprising.  If algorithms offer a legal way to overcome obstacles to profit-boosting coordination, and create a jointly profitable status quo in the market, why should suppliers not use them? In light of these developments, seeking solutions – both regulatory and market-driven – is timely and essential. While current research has largely focused on the concerns raised by algorithmic-facilitated coordination, this article takes the next step, asking to what extent current laws can be fitted to effectively deal with this phenomenon.},
	language = {en},
	number = {ID 3171977},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Gal, Michal},
	month = may,
	year = {2018},
	keywords = {Artificial Intelligence, Algorithms, Antitrust, Competition, Competition Law, Deep Learning, Facilitating Practices, Illegal Agreements},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\53TQA25Y\\papers.html:text/html},
}

@misc{gal_illegal_nodate,
	title = {Illegal {Pricing} {Algorithms}},
	url = {https://cacm.acm.org/magazines/2019/1/233516-illegal-pricing-algorithms/fulltext},
	abstract = {Examining the potential legal consequences of using pricing algorithms.},
	language = {en},
	urldate = {2021-01-25},
	author = {Gal, Michal S.},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\XG48L2LK\\fulltext.html:text/html},
}

@techreport{gal_algorithms_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithms and {Competition} {Law}},
	url = {https://papers.ssrn.com/abstract=3688324},
	abstract = {An interview of Prof. Michal Gal by Dr. Thibault Schrepel on the subject of algorithms and competition law. The interview covers subjects such as how algorithms affect competition; cases involving  algorithms; the potential benefits brought about by algorithms to consumers; the unique characteristics of algorithmic coordination; algorithmic liability for illegal cartels; personalized services; the creation of digital workforces in competition agencies.},
	language = {en},
	number = {ID 3688324},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Gal, Michal and Schrepel, Thibault},
	month = sep,
	year = {2020},
	keywords = {algorithms, competition, antitrust, algorithmic consumers, competition law},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\6DBQTIP4\\papers.html:text/html},
}

@inproceedings{waltman_theoretical_2007,
	title = {A {Theoretical} {Analysis} of {Cooperative} {Behavior} in {Multi}-agent {Q}-learning},
	doi = {10.1109/ADPRL.2007.368173},
	abstract = {A number of experimental studies have investigated whether cooperative behavior may emerge in multi-agent Q-learning. In some studies cooperative behavior did emerge, in others it did not. This paper provides a theoretical analysis of this issue. The analysis focuses on multi-agent Q-learning in iterated prisoner's dilemmas. It is shown that under certain assumptions cooperative behavior may emerge when multi-agent Q-learning is applied in an iterated prisoner's dilemma. An important consequence of the analysis is that multi-agent Q-learning may result in non-Nash behavior. It is found experimentally that the theoretical results presented in this paper are quite robust to violations of the underlying assumptions},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Approximate} {Dynamic} {Programming} and {Reinforcement} {Learning}},
	author = {Waltman, L. and Kaymak, U.},
	month = apr,
	year = {2007},
	note = {ISSN: 2325-1867},
	keywords = {Algorithm design and analysis, cooperative behavior, Dynamic programming, Environmental economics, Helium, Learning, learning (artificial intelligence), Microeconomics, multi-agent systems, multiagent Q-learning, Nash equilibrium, Oligopoly, Performance analysis, Robustness, theoretical analysis},
	pages = {84--91},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\LXRVUEGT\\Waltman and Kaymak - 2007 - A Theoretical Analysis of Cooperative Behavior in .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\psymo\\Zotero\\storage\\GT5EW3TN\\4220818.html:text/html},
}

@article{harrington_developing_2018,
	title = {{DEVELOPING} {COMPETITION} {LAW} {FOR} {COLLUSION} {BY} {AUTONOMOUS} {ARTIFICIAL} {AGENTS}†},
	volume = {14},
	issn = {1744-6414},
	url = {https://doi.org/10.1093/joclec/nhy016},
	doi = {10.1093/joclec/nhy016},
	abstract = {After arguing that collusion by software programs which choose pricing rules without any human intervention is not a violation of Section 1 of the Sherman Act, the paper offers a path toward making collusion by autonomous artificial agents unlawful.},
	number = {3},
	urldate = {2021-01-25},
	journal = {Journal of Competition Law \& Economics},
	author = {Harrington, Joseph E},
	month = sep,
	year = {2018},
	pages = {331--363},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\5UNUVI9N\\5292366.html:text/html},
}

@techreport{schwalbe_algorithms_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Algorithms, {Machine} {Learning}, and {Collusion}},
	url = {https://papers.ssrn.com/abstract=3232631},
	abstract = {This paper discusses the question whether self-learning price-setting algorithms are able to coordinate their pricing behaviour to achieve a collusive outcome that maximizes the joint profits of the firms using these algorithms. While the legal literature generally assumes that algorithmic collusion is indeed possible and in fact very easy, the computer science literature on cooperation between algorithms as well as the economics literature on collusion in experimental oligopolies indicate that a coordinated and in particular tacitly collusive behaviour is in general rather difficult to achieve. Many studies have shown that some form of communication is of vital importance for collusion if there are more than two firms in a market. Communication between algorithms is also a topic in artificial intelligence research and some recent contributions indicate that algorithms may learn to communicate, albeit in a rather limited way. This leads to the conclusion that algorithmic collusion is currently much more difficult to achieve than often assumed in the legal literature and is therefore currently not a particularly important competitive concern. In addition, there are also several legal problems associated with algorithmic collusion, for example, questions of liability, of auditing and monitoring algorithms as well as enforcement. The limited resources of competition authorities should rather be devoted to more pressing problems as, for example, the abuse of dominant positions by large online-platforms.},
	language = {en},
	number = {ID 3232631},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Schwalbe, Ulrich},
	month = jun,
	year = {2018},
	doi = {10.2139/ssrn.3232631},
	keywords = {Artificial Intelligence, Algorithms, Machine Learning, Competition Law, Tacit Collusion},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\YK35GPEQ\\papers.html:text/html},
}

@article{petit_antitrust_2017,
	title = {Antitrust and {Artificial} {Intelligence}: {A} {Research} {Agenda}},
	volume = {8},
	issn = {2041-7764},
	shorttitle = {Antitrust and {Artificial} {Intelligence}},
	url = {https://doi.org/10.1093/jeclap/lpx033},
	doi = {10.1093/jeclap/lpx033},
	number = {6},
	urldate = {2021-01-25},
	journal = {Journal of European Competition Law \& Practice},
	author = {Petit, Nicolas and Ittoo, Ashwin},
	month = jun,
	year = {2017},
	pages = {361--362},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\QF2H5DLP\\Petit - 2017 - Antitrust and Artificial Intelligence A Research .pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\5MS5LH59\\3812669.html:text/html},
}

@techreport{mehra_antitrust_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Antitrust and the {Robo}-{Seller}: {Competition} in the {Time} of {Algorithms}},
	shorttitle = {Antitrust and the {Robo}-{Seller}},
	url = {https://papers.ssrn.com/abstract=2576341},
	abstract = {Increasingly, firms are knitting together newly available mass data collection, Internet-driven interconnective power, and automated algorithmic selling with their traditional supply-chain and sales functions. Traditional sales functions such as competitive intelligence gathering and pricing are being delegated to software “robo-sellers.”  This Article offers the first descriptive and normative study of the implications of this shift away from humans to machines (the “robo-sellers”) for antitrust law.  This change is a critical challenge for antitrust law – both in how it is currently applied and in highlighting and exacerbating its existing weaknesses.},
	language = {en},
	number = {ID 2576341},
	urldate = {2021-01-25},
	institution = {Social Science Research Network},
	author = {Mehra, Salil K.},
	month = mar,
	year = {2015},
	keywords = {competition, antitrust, automation, computers, cyberlaw, oligopoly},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\LQDU7N79\\papers.html:text/html;Mehra - 2015 - Antitrust and the Robo-Seller Competition in the .pdf:C\:\\Users\\psymo\\Zotero\\storage\\79Z6ULA4\\Mehra - 2015 - Antitrust and the Robo-Seller Competition in the .pdf:application/pdf},
}

@misc{noauthor_implications_nodate,
	title = {The {Implications} of {Algorithmic} {Pricing} for {Coordinated} {Effects} {Analysis} and {Price} {Discrimination} {Markets} in {Antitrust} {Enforcement} - {Antitrust} {Writing} {Awards} 2018},
	url = {http://awa2018.concurrences.com/articles-awards/business-articles-awards/article/the-implications-of-algorithmic-pricing-for-coordinated-effects-analysis-and},
	urldate = {2021-01-25},
	file = {The Implications of Algorithmic Pricing for Coordinated Effects Analysis and Price Discrimination Markets in Antitrust Enforcement - Antitrust Writing Awards 2018:C\:\\Users\\psymo\\Zotero\\storage\\BKVA3YZZ\\the-implications-of-algorithmic-pricing-for-coordinated-effects-analysis-and.html:text/html;The Implications of Algorithmic Pricing for Coordi.pdf:C\:\\Users\\psymo\\Zotero\\storage\\2C43LUVC\\The Implications of Algorithmic Pricing for Coordi.pdf:application/pdf},
}

@misc{kane_few_2020,
	title = {A {Few} {Reflections} on the {Recent} {Case} {Law} on {Algorithmic} {Collusion}},
	url = {https://www.competitionpolicyinternational.com/a-few-reflections-on-the-recent-case-law-on-algorithmic-collusion/},
	abstract = {The article discusses the limited number of cases where the use of algorithms was found to be a factor that contributed to collusion.},
	language = {en-US},
	urldate = {2021-01-25},
	journal = {Competition Policy International},
	author = {Kane, Claudia Patricia O.},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\Q8I9RXG8\\a-few-reflections-on-the-recent-case-law-on-algorithmic-collusion.html:text/html;Kane - 2020 - A Few Reflections on the Recent Case Law on Algori.pdf:C\:\\Users\\psymo\\Zotero\\storage\\ZA74VWTU\\Kane - 2020 - A Few Reflections on the Recent Case Law on Algori.pdf:application/pdf},
}

@article{fischer_collusion_2019,
	title = {Collusion and bargaining in asymmetric {Cournot} duopoly—{An} experiment},
	volume = {111},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292118301806},
	doi = {10.1016/j.euroecorev.2018.10.005},
	abstract = {In asymmetric dilemma games without side payments, players face involved cooperation and bargaining problems. The maximization of joint profits is implausible, players disagree on the collusive action, and the outcome is often inefficient. For the example of a Cournot duopoly with asymmetric cost, we investigate experimentally how players cooperate (collude implicitly and explicitly), if at all, in such games. In our treatments without communication, players fail to cooperate and essentially play the static Nash equilibrium (consistent with previous results). With communication, inefficient firms gain at the expense of efficient ones. When the role of the efficient firm is earned in a contest, the efficient firm earns higher profits than when this role is randomly allocated. Bargaining solutions do not satisfactorily predict outcomes.},
	language = {en},
	urldate = {2021-01-25},
	journal = {European Economic Review},
	author = {Fischer, Christian and Normann, Hans-Theo},
	month = jan,
	year = {2019},
	keywords = {Cartels, Asymmetries, Bargaining, Communication, Cournot, Earned role, Experiments},
	pages = {360--379},
	file = {Full Text:C\:\\Users\\psymo\\Zotero\\storage\\LLKTF74A\\Fischer and Normann - 2019 - Collusion and bargaining in asymmetric Cournot duo.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\PMIJSAVN\\S0014292118301806.html:text/html},
}

@inproceedings{ghose_dynamic_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Dynamic} {Pricing} {Approach} in {E}-{Commerce} {Based} on {Multiple} {Purchase} {Attributes}},
	isbn = {978-3-642-13059-5},
	doi = {10.1007/978-3-642-13059-5_13},
	abstract = {In this paper, we propose an approach of dynamic pricing where buyers purchase decision is dependent on multiple preferred purchase attributes such as product price, product quality, after sales service, delivery time, sellers’ reputation. The approach requires the sellers, by considering the five attributes, to set an initial price of the product with the help of their prior knowledge about prices of the product offered by other competing sellers. Our approach adjusts the selling price of products automatically with the help of neural network in order to maximize seller revenue. The experimental results portray the effect of considering the five attributes in earning revenue by the sellers. Before concluding with directions for future works, we discuss the value of our approach in contrast with related work.},
	language = {en},
	booktitle = {Advances in {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Ghose, Tapu Kumar and Tran, Thomas T.},
	editor = {Farzindar, Atefeh and Kešelj, Vlado},
	year = {2010},
	keywords = {Dynamic Pricing, Electronic Commerce, Multiple Purchase Attributes},
	pages = {111--122},
	file = {Springer Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\I4NP72EF\\Ghose and Tran - 2010 - A Dynamic Pricing Approach in E-Commerce Based on .pdf:application/pdf},
}

@inproceedings{izquierdo_win-continue_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Economics} and {Mathematical} {Systems}},
	title = {The “{Win}-{Continue}, {Lose}-{Reverse}” {Rule} in {Cournot} {Oligopolies}: {Robustness} of {Collusive} {Outcomes}},
	isbn = {978-3-319-09578-3},
	shorttitle = {The “{Win}-{Continue}, {Lose}-{Reverse}” {Rule} in {Cournot} {Oligopolies}},
	doi = {10.1007/978-3-319-09578-3_3},
	abstract = {The so-called “Win-Continue, Lose-Reverse” (WCLR) rule is a simple procedure that can be used to choose a value for any numeric variable (e.g. setting a production level to maximise profit). The rule dictates that one should evaluate the consequences of the last adjustment made to the value (e.g. an increase or a decrease in production), and keep on changing the value in the same direction if the adjustment led to an improvement (e.g. if it led to greater profits), or reverse the direction of change otherwise. Somewhat surprisingly, this simple rule has been shown to lead to collusive outcomes in Cournot oligopolies, even though its application requires no information whatsoever about the choices made by any competing firms or about their results. Firms applying the WCLR rule need only know whether the last change in their own production turned out to be profitable or not; thus, there is no room for explicit coordination or collusion. In this paper we show that the convergence of the WCLR rule towards collusive outcomes can be very sensitive to small independent perturbations in the cost functions and in the income functions of the firms. These perturbations typically push the process towards the Cournot–Nash equilibrium of the one-shot game. Importantly, the destabilizing power of the independent perturbations is mainly due to the fact that they create miscoordination among the firms. In fact, if there is correlation between the perturbations, their impact on the dynamics of the model is not so dramatic.},
	language = {en},
	booktitle = {Advances in {Artificial} {Economics}},
	publisher = {Springer International Publishing},
	author = {Izquierdo, Segismundo S. and Izquierdo, Luis R.},
	editor = {Amblard, Frédéric and Miguel, Francisco J. and Blanchet, Adrien and Gaudou, Benoit},
	year = {2015},
	keywords = {Collusion, Oligopoly, Cournot, Duopoly, Simulation, Win-continue-Lose-reverse},
	pages = {33--44},
	file = {Submitted Version:C\:\\Users\\psymo\\Zotero\\storage\\8ZZY7VSN\\Izquierdo and Izquierdo - 2015 - The “Win-Continue, Lose-Reverse” Rule in Cournot O.pdf:application/pdf},
}

@techreport{deng_when_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {When {Machines} {Learn} to {Collude}: {Lessons} from a {Recent} {Research} {Study} on {Artificial} {Intelligence}},
	shorttitle = {When {Machines} {Learn} to {Collude}},
	url = {https://papers.ssrn.com/abstract=3029662},
	abstract = {From Professors Maurice Stucke and Ariel Ezrachi’s Virtual Competition published a year ago, to speeches by the Federal Trade Commission Commissioner Terrell McSweeny and Acting Chair Maureen K. Ohlhausen, to an entire issue of a recent CPI Antitrust Chronicles, and a conference hosted by Organisation for Economic Co-operation and Development (OECD) in June this year, there has been an active and ongoing discussion in the antitrust community about computer algorithms. In this note, I briefly summarize the current views and concerns in the antitrust and artificial intelligence (AAI) literature pertaining to algorithmic collusion and then discuss the insights and lessons we could learn from a recent AI research study. As I argue in the article, not all assumptions in the antitrust scholarship have empirical support at this point.},
	language = {en},
	number = {ID 3029662},
	urldate = {2021-01-31},
	institution = {Social Science Research Network},
	author = {Deng, Ai},
	month = aug,
	year = {2017},
	doi = {10.2139/ssrn.3029662},
	keywords = {Artificial Intelligence, Antitrust, Algorithmic Collusion},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\HT292ISC\\papers.html:text/html},
}

@article{besanko_logit_1998,
	title = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}: {An} {Equilibrium} {Framework}},
	volume = {44},
	issn = {0025-1909},
	shorttitle = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.44.11.1533},
	doi = {10.1287/mnsc.44.11.1533},
	abstract = {Discrete choice models of demand have typically been estimated assuming that prices are exogenous. Since unobservable (to the researcher) product attributes, such as coupon availability, may impact consumer utility as well as price setting by firms, we treat prices as endogenous. Specifically, prices are assumed to be the equilibrium outcomes of Nash competition among manufacturers and retailers. To empirically validate the assumptions, we estimate logit demand systems jointly with equilibrium pricing equations for two product categories using retail scanner data and cost data on factor prices. In each category, we find statistical evidence of price endogeneity. We also find that the estimates of the price response parameter and the brand-specific constants are generally biased downward when the endogeneity of prices is ignored. Our framework provides explicit estimates of the value created by a brand, i.e., the difference between consumers' willingness to pay for a brand and its cost of production. We develop theoretical propositions about the relationship between value creation and competitive advantage for logit demand systems and use our empirical results to illustrate how firms use alternative value creation strategies to accomplish competitive advantage.},
	number = {11-part-1},
	urldate = {2021-02-23},
	journal = {Management Science},
	author = {Besanko, David and Gupta, Sachin and Jain, Dipak},
	month = nov,
	year = {1998},
	note = {Publisher: INFORMS},
	pages = {1533--1547},
	file = {Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\YV3J5GHD\\mnsc.44.11.html:text/html;Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:C\:\\Users\\psymo\\Zotero\\storage\\BPZGSTF9\\Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:application/pdf},
}

@article{besanko_logit_1998-1,
	title = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}: {An} {Equilibrium} {Framework}},
	volume = {44},
	issn = {0025-1909, 1526-5501},
	shorttitle = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.44.11.1533},
	doi = {10.1287/mnsc.44.11.1533},
	language = {en},
	number = {11-part-1},
	urldate = {2021-02-23},
	journal = {Management Science},
	author = {Besanko, David and Gupta, Sachin and Jain, Dipak},
	month = nov,
	year = {1998},
	pages = {1533--1547},
	file = {Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:C\:\\Users\\psymo\\Zotero\\storage\\MSDP89XL\\Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:application/pdf},
}

@article{besanko_logit_1998-2,
	title = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}: {An} {Equilibrium} {Framework}},
	volume = {44},
	issn = {0025-1909, 1526-5501},
	shorttitle = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.44.11.1533},
	doi = {10.1287/mnsc.44.11.1533},
	language = {en},
	number = {11-part-1},
	urldate = {2021-02-23},
	journal = {Management Science},
	author = {Besanko, David and Gupta, Sachin and Jain, Dipak},
	month = nov,
	year = {1998},
	pages = {1533--1547},
	file = {Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:C\:\\Users\\psymo\\Zotero\\storage\\RRHFD7I8\\Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:application/pdf},
}

@article{besanko_logit_1998-3,
	title = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}: {An} {Equilibrium} {Framework}},
	volume = {44},
	issn = {0025-1909, 1526-5501},
	shorttitle = {Logit {Demand} {Estimation} {Under} {Competitive} {Pricing} {Behavior}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.44.11.1533},
	doi = {10.1287/mnsc.44.11.1533},
	language = {en},
	number = {11-part-1},
	urldate = {2021-02-23},
	journal = {Management Science},
	author = {Besanko, David and Gupta, Sachin and Jain, Dipak},
	month = nov,
	year = {1998},
	pages = {1533--1547},
	file = {Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:C\:\\Users\\psymo\\Zotero\\storage\\86E5FMX5\\Besanko et al. - 1998 - Logit Demand Estimation Under Competitive Pricing .pdf:application/pdf},
}

@article{anderson_logit_1992,
	title = {The {Logit} as a {Model} of {Product} {Differentiation}},
	volume = {44},
	issn = {0030-7653},
	url = {https://www.jstor.org/stable/2663424},
	number = {1},
	urldate = {2021-02-23},
	journal = {Oxford Economic Papers},
	author = {Anderson, Simon P. and de Palma, Andre},
	year = {1992},
	note = {Publisher: Oxford University Press},
	pages = {51--67},
	file = {JSTOR Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\NHVEAHJQ\\Anderson and de Palma - 1992 - The Logit as a Model of Product Differentiation.pdf:application/pdf},
}

@article{van_seijen_true_2016,
	title = {True {Online} {Temporal}-{Difference} {Learning}},
	url = {http://arxiv.org/abs/1512.04087},
	abstract = {The temporal-difference methods TD(\${\textbackslash}lambda\$) and Sarsa(\${\textbackslash}lambda\$) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD(\${\textbackslash}lambda\$) and true online Sarsa(\${\textbackslash}lambda\$), respectively (van Seijen \& Sutton, 2014). These new versions maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD(\${\textbackslash}lambda\$)/Sarsa(\${\textbackslash}lambda\$) with regular TD(\${\textbackslash}lambda\$)/Sarsa(\${\textbackslash}lambda\$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-depth analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations.},
	urldate = {2021-03-12},
	journal = {arXiv:1512.04087 [cs]},
	author = {van Seijen, Harm and Mahmood, A. Rupam and Pilarski, Patrick M. and Machado, Marlos C. and Sutton, Richard S.},
	month = sep,
	year = {2016},
	note = {arXiv: 1512.04087},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: This is the published JMLR version. It is a much improved version. The main changes are: 1) re-structuring of the article; 2) additional analysis on the forward view; 3) empirical comparison of traditional and new forward view; 4) added discussion of other true online papers; 5) updated discussion for non-linear function approximation},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\6B7CDZL9\\van Seijen et al. - 2016 - True Online Temporal-Difference Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\YKFQYXMG\\1512.html:text/html},
}

@inproceedings{seijen_true_2014,
	title = {True {Online} {TD}(lambda)},
	url = {http://proceedings.mlr.press/v32/seijen14.html},
	abstract = {TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented onlin...},
	language = {en},
	urldate = {2021-03-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Seijen, Harm and Sutton, Rich},
	month = jan,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {692--700},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\Y5N8TJFN\\Seijen and Sutton - 2014 - True Online TD(lambda).pdf:application/pdf;Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\NTLD6NWB\\seijen14.html:text/html},
}

@inproceedings{van_seijen_true_2014,
	title = {True {Online} {TD}(λ)},
	doi = {10.13140/2.1.1456.2568},
	abstract = {TD(λ) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(λ) and the for-ward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(λ) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(λ) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and re-placing traces. The overall computational complexity is the same as TD(λ), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(λ) in all of its variations. It seems, by adhering more truly to the original goal of TD(λ)—matching an intuitively clear forward view even in the online case—that we have found a new algorithm that simply improves on classical TD(λ).},
	author = {van Seijen, Harm and Sutton, Richard},
	month = jun,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\TGDYMCL8\\van Seijen and Sutton - 2014 - True Online TD(λ).pdf:application/pdf},
}

@article{doya_reinforcement_2000,
	title = {Reinforcement learning in continuous time and space},
	volume = {12},
	issn = {0899-7667},
	doi = {10.1162/089976600300015961},
	abstract = {This article presents a reinforcement learning framework for continuous-time dynamical systems without a priori discretization of time, state, and action. Based on the Hamilton-Jacobi-Bellman (HJB) equation for infinite-horizon, discounted reward problems, we derive algorithms for estimating value functions and improving policies with the use of function approximators. The process of value function estimation is formulated as the minimization of a continuous-time form of the temporal difference (TD) error. Update methods based on backward Euler approximation and exponential eligibility traces are derived, and their correspondences with the conventional residual gradient, TD(0), and TD(lambda) algorithms are shown. For policy improvement, two methods-a continuous actor-critic method and a value-gradient-based greedy policy-are formulated. As a special case of the latter, a nonlinear feedback control law using the value gradient and the model of the input gain is derived. The advantage updating, a model-free algorithm derived previously, is also formulated in the HJB-based framework. The performance of the proposed algorithms is first tested in a nonlinear control task of swinging a pendulum up with limited torque. It is shown in the simulations that (1) the task is accomplished by the continuous actor-critic method in a number of trials several times fewer than by the conventional discrete actor-critic method; (2) among the continuous policy update methods, the value-gradient-based policy with a known or learned dynamic model performs several times better than the actor-critic method; and (3) a value function update using exponential eligibility traces is more efficient and stable than that based on Euler approximation. The algorithms are then tested in a higher-dimensional task: cart-pole swing-up. This task is accomplished in several hundred trials using the value-gradient-based policy with a learned dynamic model.},
	language = {eng},
	number = {1},
	journal = {Neural Computation},
	author = {Doya, K.},
	month = jan,
	year = {2000},
	pmid = {10636940},
	keywords = {Artificial Intelligence, Algorithms, Learning, Models, Psychological, Models, Statistical, Reinforcement, Psychology, Reward, Time Factors},
	pages = {219--245},
}

@misc{noauthor_convergence_nodate,
	title = {On the {Convergence} of {Stochastic} {Iterative} {Dynamic} {Programming} {Algorithms} {\textbar} {Neural} {Computation} {\textbar} {MIT} {Press}},
	url = {https://direct.mit.edu/neco/article/6/6/1185/5826/On-the-Convergence-of-Stochastic-Iterative-Dynamic},
	urldate = {2021-03-14},
	file = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms | Neural Computation | MIT Press:C\:\\Users\\psymo\\Zotero\\storage\\LZDDJXSV\\On-the-Convergence-of-Stochastic-Iterative-Dynamic.html:text/html},
}

@article{jaakkola_convergence_1994,
	title = {On the {Convergence} of {Stochastic} {Iterative} {Dynamic} {Programming} {Algorithms}},
	volume = {6},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1994.6.6.1185},
	doi = {10.1162/neco.1994.6.6.1185},
	abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.},
	number = {6},
	urldate = {2021-03-14},
	journal = {Neural Computation},
	author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
	month = nov,
	year = {1994},
	pages = {1185--1201},
	file = {Full Text PDF:C\:\\Users\\psymo\\Zotero\\storage\\44HU6L3W\\Jaakkola et al. - 1994 - On the Convergence of Stochastic Iterative Dynamic.pdf:application/pdf},
}

@article{naik_discounted_2019,
	title = {Discounted {Reinforcement} {Learning} {Is} {Not} an {Optimization} {Problem}},
	url = {http://arxiv.org/abs/1910.02140},
	abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
	urldate = {2021-03-14},
	journal = {arXiv:1910.02140 [cs]},
	author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.02140},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted for presentation at the Optimization Foundations of Reinforcement Learning Workshop at NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\psymo\\Zotero\\storage\\8YB4C936\\Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\psymo\\Zotero\\storage\\DW3BUZMZ\\1910.html:text/html},
}
